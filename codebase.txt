.\get_clean_message_list.py
```python
from typing import List, Dict, Any

def get_clean_message_list(messages: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    """Clean and format a list of messages for the model.
    
    Args:
        messages: List of message dictionaries with 'role' and 'content' keys
        
    Returns:
        List of cleaned message dictionaries
    """
    cleaned_messages = []
    for message in messages:
        if not isinstance(message, dict):
            continue
            
        role = message.get('role', '').strip().lower()
        content = message.get('content', '')
        
        if not role or not content:
            continue
            
        cleaned_messages.append({
            'role': role,
            'content': content
        })
        
    return cleaned_messages 
```

.\init_neo4j.py
```python
import os
from neo4j import GraphDatabase
from dotenv import load_dotenv
from loguru import logger

# Load environment variables
load_dotenv()

# Neo4j connection settings
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USERNAME = os.getenv("NEO4J_USERNAME", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password")

def init_neo4j():
    """Initialize Neo4j with indexes and constraints."""
    try:
        # Connect to Neo4j
        driver = GraphDatabase.driver(
            NEO4J_URI,
            auth=(NEO4J_USERNAME, NEO4J_PASSWORD)
        )
        
        with driver.session() as session:
            # Create constraints
            logger.info("Creating constraints...")
            session.run("""
                CREATE CONSTRAINT document_id IF NOT EXISTS
                FOR (d:Document) REQUIRE d.id IS UNIQUE
            """)
            
            # Create indexes
            logger.info("Creating indexes...")
            session.run("""
                CREATE INDEX document_content IF NOT EXISTS
                FOR (d:Document)
                ON (d.content)
            """)
            
            session.run("""
                CREATE INDEX document_embedding IF NOT EXISTS
                FOR (d:Document)
                ON (d.embedding)
            """)
            
            logger.info("Neo4j initialization completed successfully")
            
    except Exception as e:
        logger.error(f"Error initializing Neo4j: {str(e)}")
        raise
    finally:
        driver.close()

if __name__ == "__main__":
    init_neo4j() 
```

.\inspect_graph.py
```python
from neo4j import GraphDatabase
from rich.console import Console
from rich.table import Table
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize Rich console
console = Console()

# Neo4j connection details
uri = "bolt://localhost:7687"
username = "neo4j"
password = "password123"

def inspect_graph():
    with GraphDatabase.driver(uri, auth=(username, password)) as driver:
        with driver.session() as session:
            # Get node counts by label
            node_counts = session.run("""
                MATCH (n)
                RETURN labels(n) as labels, count(n) as count
            """)
            
            # Create table for node counts
            table = Table(title="Neo4j Graph Statistics")
            table.add_column("Label", style="cyan")
            table.add_column("Count", style="green")
            
            for record in node_counts:
                table.add_row(str(record["labels"]), str(record["count"]))
            
            console.print(table)
            
            # Get sample documents
            console.print("\n[bold]Sample Documents:[/bold]")
            documents = session.run("""
                MATCH (d:Document)
                RETURN d.content as content, d.metadata_str as metadata
                LIMIT 5
            """)
            
            for doc in documents:
                console.print(f"\n[cyan]Content:[/cyan] {doc['content']}")
                console.print(f"[cyan]Metadata:[/cyan] {doc['metadata']}")
            
            # Check indexes
            console.print("\n[bold]Indexes:[/bold]")
            indexes = session.run("SHOW INDEXES")
            
            index_table = Table(title="Neo4j Indexes")
            index_table.add_column("Name", style="cyan")
            index_table.add_column("Type", style="green")
            index_table.add_column("Properties", style="yellow")
            
            for index in indexes:
                index_table.add_row(
                    str(index.get("name", "N/A")),
                    str(index.get("type", "N/A")),
                    str(index.get("properties", []))
                )
            
            console.print(index_table)

if __name__ == "__main__":
    try:
        inspect_graph()
    except Exception as e:
        console.print(f"[red]Error:[/red] {str(e)}") 
```

.\kg_test.py
```python
import networkx as nx

# Create a directed graph
G = nx.DiGraph()

# Add nodes (Topics, Herbs, Mechanisms, Approaches) - Cardiovascular Health Section

nodes_cardio = [
    "Cardiovascular Health", "Hawthorn", "Garlic", "Omega-3s", "CoQ10", "Yoga", "Meditation", "Acupuncture", "Qi Gong",
    "Cardiometabolic Network Modulation", "Gut-Heart Axis Optimization",
    "Cardiovascular Support", "Advanced Phytochemistry & Energetic Integration",
    "Sulfur Bioavailability", "Microbiome Modulation", "TCM Syndromes",
    "Specialized Pro-resolving Mediators (SPMs)", "Lipid Raft Dynamics", "Personalized Dosing",
    "Redox Biology", "Mitochondrial Dynamics", "Bioenergetic Profiling",
    "Neurocardiology", "Interoception", "Vagal Tone Variability",
    "Default Mode Network Modulation", "Epigenetic Regulation", "Heart-Brain Synchronization",
    "Neurovascular Coupling", "Microcirculatory Dynamics", "Meridian Theory Refinement",
    "Fascial Network Dynamics", "Biofield Interactions", "Microcirculation Enhancement at Cellular Level",
    "Systems Biology", "Multi-Omics Integration", "Personalized Interventions",
    "Microbial Metabolite Signaling", "Vagal Nerve Stimulation", "Barrier Function Integrity",
    "Holistic Approaches", "Ayurvedic Perspective", "TCM Perspective",
    "Transcriptomics", "Proteomics", "Metabolomics", "Network Pharmacology",
    "Personalized Lifestyle Modifications", "Dietary Modifications", "Stress Reduction Techniques",
    "Emotional Heart-Mind Connection", "Cardiovascular Resilience",
    "Sulfur Chemistry", "Allicin", "Ajoene", "Vinyldithiins", "Hydrogen Sulfide (H2S)",
    "Gut Dysbiosis", "Gut-Heart Axis", "Microbial Metabolites", "Short-Chain Fatty Acids (SCFAs)", "Trimethylamine N-Oxide (TMAO) Pathways",
    "Blood Stasis", "Phlegm Dampness", "Sulfur Compounds", "Garlic Cultivar", "Growing Conditions", "Processing Methods",
    "Metabolites in vivo", "Cardiovascular Risk Markers", "Pungent Nature", "Ayurvedic Dietary Recommendations", "TCM Dietary Recommendations", "Individual Constitutions",
    "Omega-3 Fatty Acids (EPA & DHA)", "Anti-inflammatory Effects", "Triglyceride-Lowering Effects", "Resolvins", "Protectins", "Maresins", "Inflammation Resolution", "Tissue Repair",
    "Cell Membrane Lipid Rafts", "Membrane Fluidity", "Receptor Signaling", "Cellular Function", "Cardiomyocytes", "Endothelial Cells",
    "Genetic Variations", "Fatty Acid Desaturases (FADS Genes)", "Dietary Intake", "Cardiovascular Risk Profiles", "Advanced Lipidomics", "Omega-3 Status",
    "Dietary Framework", "Whole Foods", "Balanced Macronutrients", "Inflammatory Triggers", "Lifestyle Factors", "Environmental Factors",
    "Coenzyme Q10 (CoQ10)", "Mitochondrial Support", "Antioxidant Activity", "Redox Signaling", "Mitochondria", "Cytoplasm", "Cellular Processes", "ATP Production",
    "Mitochondrial Dynamics", "Fusion", "Fission", "Mitochondrial Biogenesis", "Mitophagy",
    "Bioenergetic Profiling Techniques", "Seahorse XF Analyzer", "Mitochondrial Respiration", "Energy Production", "Physiological Conditions", "Pathological Conditions",
    "Mitochondrial Nutrients", "L-Carnitine", "Riboflavin", "Magnesium", "Synergistic Approach", "Mitochondrial Health", "Exercise", "Stress Reduction", "Mitochondrial Toxins",
    "Neurocardiology of Yoga", "Brain-Heart Axis", "Neural Pathways", "Central Nervous System", "Cardiovascular System",
    "Internal Bodily States", "Cardiovascular Autonomic Control", "HRV (Heart Rate Variability) Analysis Techniques", "Nonlinear HRV Analysis", "Frequency Domain Measures",
    "Vagal Tone Dynamics", "Parasympathetic Nervous System Function", "Yoga Styles", "Restorative Yoga", "Yin Yoga", "Pranayama Techniques", "Alternate Nostril Breathing", "Ujjayi Breath",
    "Mindfulness", "Emotional Regulation", "Sense of Purpose", "Connection", "Heart-Mind Coherence",
    "Default Mode Network (DMN)", "Mind-Wandering", "Rumination", "Cardiovascular Outcomes", "Epigenetic Mechanisms", "DNA Methylation", "Histone Modifications",
    "Inflammation", "Stress Response", "Endothelial Function", "Heart Rhythms", "Brain Rhythms", "Neuroimaging Techniques", "fMRI", "EEG", "Brain Activity Changes", "Cardiovascular Parameters",
    "Mindfulness-Based Stress Reduction (MBSR) Programs", "Contemplative Practices", "Emotional Intelligence", "Sustained Heart Health Benefits",
    "Neurovascular Coupling Mechanisms", "Acupuncture Points", "PC6", "HT7", "Vascular Tone", "Blood Flow", "Microcirculation",
    "Advanced Imaging Techniques", "Laser Doppler Flowmetry", "Intravital Microscopy", "Tissue Perfusion", "TCM Meridian Theory", "Bioelectrical Properties", "Bioenergetic Properties", "Acupoints", "Meridians", "Heart Qi Deficiency", "Heart Blood Stasis", "Cardiovascular Risk Factors", "TCM Dietary Therapy", "Herbal Medicine", "Lifestyle Recommendations", "Individual Constitutions", "Cardiovascular Imbalances",
    "Fascial Network Dynamics", "Body-Wide Communication System", "Qi Gong Movements", "Fascial Tension", "Fluid Dynamics", "Mechanotransduction",
    "Biofield Interactions", "Subtle Energetic Effects", "Cellular Communication", "Physiological Processes", "Advanced Microscopy Techniques", "Atomic Force Microscopy", "Super-Resolution Microscopy",
    "Capillary Density", "Red Blood Cell Deformability", "Endothelial Cell Function", "Jing", "Qi", "Shen", "Essence", "Energy", "Spirit",
    "Systems Biology Approach", "Cardiometabolic Health", "Computational Models", "Interconnected Networks", "Cardiovascular Diseases", "Metabolic Diseases", "Multi-Omics Data",
    "Genomics", "Transcriptomics", "Proteomics", "Metabolomics", "Microbiomics", "Dynamic Interplay", "Genes", "Proteins", "Metabolites", "Microbial Communities",
    "Key Network Nodes", "Pathways", "Targeted Interventions", "Pharmacological Approaches", "Non-Pharmacological Approaches", "Precision Medicine Strategies", "Root Causes", "Cardiometabolic Dysfunction", "Global Health Optimization",
    "Gut-Heart Connection", "Intricate Signaling Pathways", "Microbial Metabolite Signaling", "Vagal Nerve Stimulation", "Barrier Function Integrity",
    "Microbial Metabolites", "SCFAs", "Bile Acids", "Tryptophan Metabolites", "Blood Pressure Regulation", "Endothelial Function", "Bidirectional Communication", "Vagal Nerve", "Gut Microbiome Modulation",
    "Vagal Tone", "Autonomic Balance", "Leaky Gut", "Systemic Translocation", "Microbial Products", "LPS", "Peptidoglycans", "Chronic Inflammation",
    "Gut Microbial Profiles", "Microbial Species", "Prebiotic Strategies", "Probiotic Strategies", "Dietary Fiber Optimization", "Fermented Foods", "Gut-Healing Protocols", "Healthy Gut Microbiome", "Cardiovascular Well-being"
]
G.add_nodes_from(nodes_cardio)


# Add edges (Relationships) - Cardiovascular Health Section
edges_cardio = [
    ("Cardiovascular Health", "Hawthorn", {'relation': 'USES'}),
    ("Cardiovascular Health", "Garlic", {'relation': 'USES'}),
    ("Cardiovascular Health", "Omega-3s", {'relation': 'USES'}),
    ("Cardiovascular Health", "CoQ10", {'relation': 'USES'}),
    ("Cardiovascular Health", "Yoga", {'relation': 'USES'}),
    ("Cardiovascular Health", "Meditation", {'relation': 'USES'}),
    ("Cardiovascular Health", "Acupuncture", {'relation': 'USES'}),
    ("Cardiovascular Health", "Qi Gong", {'relation': 'USES'}),
    ("Cardiovascular Health", "Cardiometabolic Network Modulation", {'relation': 'USES'}),
    ("Cardiovascular Health", "Gut-Heart Axis Optimization", {'relation': 'USES'}),

    ("Hawthorn", "Cardiovascular Support", {'relation': 'HAS_TOPIC'}),
    ("Hawthorn", "Advanced Phytochemistry & Energetic Integration", {'relation': 'HAS_TOPIC'}),
    ("Garlic", "Cardiovascular Health", {'relation': 'HAS_TOPIC'}),
    ("Garlic", "Sulfur Bioavailability", {'relation': 'HAS_TOPIC'}),
    ("Garlic", "Microbiome Modulation", {'relation': 'HAS_TOPIC'}),
    ("Garlic", "TCM Syndromes", {'relation': 'HAS_TOPIC'}),
    ("Omega-3s", "Cardiovascular Protection", {'relation': 'HAS_TOPIC'}),
    ("Omega-3s", "Specialized Pro-resolving Mediators (SPMs)", {'relation': 'HAS_TOPIC'}),
    ("Omega-3s", "Lipid Raft Dynamics", {'relation': 'HAS_TOPIC'}),
    ("Omega-3s", "Personalized Dosing", {'relation': 'HAS_TOPIC'}),
    ("CoQ10", "Heart Function", {'relation': 'HAS_TOPIC'}),
    ("CoQ10", "Redox Biology", {'relation': 'HAS_TOPIC'}),
    ("CoQ10", "Mitochondrial Dynamics", {'relation': 'HAS_TOPIC'}),
    ("CoQ10", "Bioenergetic Profiling", {'relation': 'HAS_TOPIC'}),
    ("Yoga", "Cardiovascular Resilience", {'relation': 'HAS_TOPIC'}),
    ("Yoga", "Neurocardiology", {'relation': 'HAS_TOPIC'}),
    ("Yoga", "Interoception", {'relation': 'HAS_TOPIC'}),
    ("Yoga", "Vagal Tone Variability", {'relation': 'HAS_TOPIC'}),
    ("Meditation", "Heart Health", {'relation': 'HAS_TOPIC'}),
    ("Meditation", "Default Mode Network Modulation", {'relation': 'HAS_TOPIC'}),
    ("Meditation", "Epigenetic Regulation", {'relation': 'HAS_TOPIC'}),
    ("Meditation", "Heart-Brain Synchronization", {'relation': 'HAS_TOPIC'}),
    ("Acupuncture", "Cardiovascular Regulation", {'relation': 'HAS_TOPIC'}),
    ("Acupuncture", "Neurovascular Coupling", {'relation': 'HAS_TOPIC'}),
    ("Acupuncture", "Microcirculatory Dynamics", {'relation': 'HAS_TOPIC'}),
    ("Acupuncture", "Meridian Theory Refinement", {'relation': 'HAS_TOPIC'}),
    ("Qi Gong", "Cardiovascular Fitness", {'relation': 'HAS_TOPIC'}),
    ("Qi Gong", "Fascial Network Dynamics", {'relation': 'HAS_TOPIC'}),
    ("Qi Gong", "Biofield Interactions", {'relation': 'HAS_TOPIC'}),
    ("Qi Gong", "Microcirculation Enhancement at Cellular Level", {'relation': 'HAS_TOPIC'}),
    ("Cardiometabolic Network Modulation", "Cardiometabolic Network Modulation", {'relation': 'HAS_TOPIC'}),
    ("Cardiometabolic Network Modulation", "Systems Biology", {'relation': 'HAS_TOPIC'}),
    ("Cardiometabolic Network Modulation", "Multi-Omics Integration", {'relation': 'HAS_TOPIC'}),
    ("Cardiometabolic Network Modulation", "Personalized Interventions", {'relation': 'HAS_TOPIC'}),
    ("Gut-Heart Axis Optimization", "Gut-Heart Axis Optimization", {'relation': 'HAS_TOPIC'}),
    ("Gut-Heart Axis Optimization", "Microbial Metabolite Signaling", {'relation': 'HAS_TOPIC'}),
    ("Gut-Heart Axis Optimization", "Vagal Nerve Stimulation", {'relation': 'HAS_TOPIC'}),
    ("Gut-Heart Axis Optimization", "Barrier Function Integrity", {'relation': 'HAS_TOPIC'}),

    ("Hawthorn", "Ayurvedic Perspective", {'relation': 'APPROACH'}),
    ("Hawthorn", "TCM Perspective", {'relation': 'APPROACH'}),
    ("Hawthorn", "Phytochemistry", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Flavonoids", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Triterpenes", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Amines", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Transcriptomics", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Proteomics", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Metabolomics", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Network Pharmacology", {'relation': 'MECHANISM'}),
    ("Hawthorn", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Hawthorn", "Personalized Lifestyle Modifications", {'relation': 'APPROACH'}),
    ("Hawthorn", "Dietary Modifications", {'relation': 'APPROACH'}),
    ("Hawthorn", "Stress Reduction Techniques", {'relation': 'APPROACH'}),
    ("Hawthorn", "Yoga", {'relation': 'APPROACH'}),
    ("Hawthorn", "Meditation", {'relation': 'APPROACH'}),
    ("Hawthorn", "Emotional Heart-Mind Connection", {'relation': 'APPROACH'}),

    ("Garlic", "Sulfur Bioavailability", {'relation': 'MECHANISM'}),
    ("Garlic", "Microbiome Modulation", {'relation': 'MECHANISM'}),
    ("Garlic", "TCM Syndromes", {'relation': 'MECHANISM'}),
    ("Garlic", "Sulfur Chemistry", {'relation': 'MECHANISM'}),
    ("Garlic", "Allicin", {'relation': 'MECHANISM'}),
    ("Garlic", "Ajoene", {'relation': 'MECHANISM'}),
    ("Garlic", "Vinyldithiins", {'relation': 'MECHANISM'}),
    ("Garlic", "Hydrogen Sulfide (H2S)", {'relation': 'MECHANISM'}),
    ("Garlic", "Gut Dysbiosis", {'relation': 'MECHANISM'}),
    ("Garlic", "Gut-Heart Axis", {'relation': 'MECHANISM'}),
    ("Garlic", "Microbial Metabolites", {'relation': 'MECHANISM'}),
    ("Garlic", "Short-Chain Fatty Acids (SCFAs)", {'relation': 'MECHANISM'}),
    ("Garlic", "Trimethylamine N-Oxide (TMAO) Pathways", {'relation': 'MECHANISM'}),
    ("Garlic", "Blood Stasis", {'relation': 'MECHANISM'}),
    ("Garlic", "Phlegm Dampness", {'relation': 'MECHANISM'}),
    ("Garlic", "Metabolomics", {'relation': 'MECHANISM'}),
    ("Garlic", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Garlic", "Ayurvedic Dietary Recommendations", {'relation': 'APPROACH'}),
    ("Garlic", "TCM Dietary Recommendations", {'relation': 'APPROACH'}),

    ("Omega-3s", "Specialized Pro-resolving Mediators (SPMs)", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Lipid Raft Dynamics", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Personalized Dosing", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Omega-3 Fatty Acids (EPA & DHA)", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Anti-inflammatory Effects", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Triglyceride-Lowering Effects", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Resolvins", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Protectins", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Maresins", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Inflammation Resolution", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Tissue Repair", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Cell Membrane Lipid Rafts", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Membrane Fluidity", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Receptor Signaling", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Cellular Function", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Cardiomyocytes", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Endothelial Cells", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Personalized Dosing Strategies", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Genetic Variations", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Fatty Acid Desaturases (FADS Genes)", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Dietary Intake", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Cardiovascular Risk Profiles", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Advanced Lipidomics", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Omega-3 Status", {'relation': 'MECHANISM'}),
    ("Omega-3s", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Omega-3s", "Dietary Framework", {'relation': 'APPROACH'}),
    ("Omega-3s", "Whole Foods", {'relation': 'APPROACH'}),
    ("Omega-3s", "Balanced Macronutrients", {'relation': 'APPROACH'}),
    ("Omega-3s", "Inflammatory Triggers", {'relation': 'APPROACH'}),
    ("Omega-3s", "Lifestyle Factors", {'relation': 'APPROACH'}),
    ("Omega-3s", "Environmental Factors", {'relation': 'APPROACH'}),

    ("CoQ10", "Redox Biology", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondrial Dynamics", {'relation': 'MECHANISM'}),
    ("CoQ10", "Bioenergetic Profiling", {'relation': 'MECHANISM'}),
    ("CoQ10", "Coenzyme Q10 (CoQ10)", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondrial Support", {'relation': 'MECHANISM'}),
    ("CoQ10", "Antioxidant Activity", {'relation': 'MECHANISM'}),
    ("CoQ10", "Redox Signaling", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondria", {'relation': 'MECHANISM'}),
    ("CoQ10", "Cytoplasm", {'relation': 'MECHANISM'}),
    ("CoQ10", "Cellular Processes", {'relation': 'MECHANISM'}),
    ("CoQ10", "ATP Production", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondrial Dynamics", {'relation': 'MECHANISM'}),
    ("CoQ10", "Fusion", {'relation': 'MECHANISM'}),
    ("CoQ10", "Fission", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondrial Biogenesis", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitophagy", {'relation': 'MECHANISM'}),
    ("CoQ10", "Cardiomyocytes", {'relation': 'MECHANISM'}),
    ("CoQ10", "Bioenergetic Profiling Techniques", {'relation': 'MECHANISM'}),
    ("CoQ10", "Seahorse XF Analyzer", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondrial Respiration", {'relation': 'MECHANISM'}),
    ("CoQ10", "Energy Production", {'relation': 'MECHANISM'}),
    ("CoQ10", "Physiological Conditions", {'relation': 'MECHANISM'}),
    ("CoQ10", "Pathological Conditions", {'relation': 'MECHANISM'}),
    ("CoQ10", "Mitochondrial Nutrients", {'relation': 'MECHANISM'}),
    ("CoQ10", "L-Carnitine", {'relation': 'MECHANISM'}),
    ("CoQ10", "Riboflavin", {'relation': 'MECHANISM'}),
    ("CoQ10", "Magnesium", {'relation': 'MECHANISM'}),
    ("CoQ10", "Synergistic Approach", {'relation': 'MECHANISM'}),
    ("CoQ10", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("CoQ10", "Lifestyle Modifications", {'relation': 'APPROACH'}),
    ("CoQ10", "Mitochondrial Health", {'relation': 'APPROACH'}),
    ("CoQ10", "Exercise", {'relation': 'APPROACH'}),
    ("CoQ10", "Stress Reduction", {'relation': 'APPROACH'}),
    ("CoQ10", "Mitochondrial Toxins", {'relation': 'APPROACH'}),

    ("Yoga", "Neurocardiology", {'relation': 'MECHANISM'}),
    ("Yoga", "Interoception", {'relation': 'MECHANISM'}),
    ("Yoga", "Vagal Tone Variability", {'relation': 'MECHANISM'}),
    ("Yoga", "Neurocardiology of Yoga", {'relation': 'MECHANISM'}),
    ("Yoga", "Brain-Heart Axis", {'relation': 'MECHANISM'}),
    ("Yoga", "Neural Pathways", {'relation': 'MECHANISM'}),
    ("Yoga", "Central Nervous System", {'relation': 'MECHANISM'}),
    ("Yoga", "Cardiovascular System", {'relation': 'MECHANISM'}),
    ("Yoga", "Interoception", {'relation': 'MECHANISM'}),
    ("Yoga", "Internal Bodily States", {'relation': 'MECHANISM'}),
    ("Yoga", "Cardiovascular Autonomic Control", {'relation': 'MECHANISM'}),
    ("Yoga", "HRV (Heart Rate Variability) Analysis Techniques", {'relation': 'MECHANISM'}),
    ("Yoga", "Nonlinear HRV Analysis", {'relation': 'MECHANISM'}),
    ("Yoga", "Frequency Domain Measures", {'relation': 'MECHANISM'}),
    ("Yoga", "Vagal Tone Dynamics", {'relation': 'MECHANISM'}),
    ("Yoga", "Parasympathetic Nervous System Function", {'relation': 'MECHANISM'}),
    ("Yoga", "Yoga Styles", {'relation': 'MECHANISM'}),
    ("Yoga", "Restorative Yoga", {'relation': 'MECHANISM'}),
    ("Yoga", "Yin Yoga", {'relation': 'MECHANISM'}),
    ("Yoga", "Pranayama Techniques", {'relation': 'MECHANISM'}),
    ("Yoga", "Alternate Nostril Breathing", {'relation': 'MECHANISM'}),
    ("Yoga", "Ujjayi Breath", {'relation': 'MECHANISM'}),
    ("Yoga", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Yoga", "Mindfulness", {'relation': 'APPROACH'}),
    ("Yoga", "Emotional Regulation", {'relation': 'APPROACH'}),
    ("Yoga", "Sense of Purpose", {'relation': 'APPROACH'}),
    ("Yoga", "Connection", {'relation': 'APPROACH'}),
    ("Yoga", "Heart-Mind Coherence", {'relation': 'APPROACH'}),

    ("Meditation", "Default Mode Network Modulation", {'relation': 'MECHANISM'}),
    ("Meditation", "Epigenetic Regulation", {'relation': 'MECHANISM'}),
    ("Meditation", "Heart-Brain Synchronization", {'relation': 'MECHANISM'}),
    ("Meditation", "Default Mode Network (DMN)", {'relation': 'MECHANISM'}),
    ("Meditation", "Mind-Wandering", {'relation': 'MECHANISM'}),
    ("Meditation", "Rumination", {'relation': 'MECHANISM'}),
    ("Meditation", "Stress Reduction", {'relation': 'MECHANISM'}),
    ("Meditation", "Cardiovascular Outcomes", {'relation': 'MECHANISM'}),
    ("Meditation", "Epigenetic Mechanisms", {'relation': 'MECHANISM'}),
    ("Meditation", "DNA Methylation", {'relation': 'MECHANISM'}),
    ("Meditation", "Histone Modifications", {'relation': 'MECHANISM'}),
    ("Meditation", "Inflammation", {'relation': 'MECHANISM'}),
    ("Meditation", "Stress Response", {'relation': 'MECHANISM'}),
    ("Meditation", "Endothelial Function", {'relation': 'MECHANISM'}),
    ("Meditation", "Heart-Brain Synchronization", {'relation': 'MECHANISM'}),
    ("Meditation", "Heart Rhythms", {'relation': 'MECHANISM'}),
    ("Meditation", "Brain Rhythms", {'relation': 'MECHANISM'}),
    ("Meditation", "Neuroimaging Techniques", {'relation': 'MECHANISM'}),
    ("Meditation", "fMRI", {'relation': 'MECHANISM'}),
    ("Meditation", "EEG", {'relation': 'MECHANISM'}),
    ("Meditation", "Brain Activity Changes", {'relation': 'MECHANISM'}),
    ("Meditation", "Cardiovascular Parameters", {'relation': 'MECHANISM'}),
    ("Meditation", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Meditation", "Mindfulness-Based Stress Reduction (MBSR) Programs", {'relation': 'APPROACH'}),
    ("Meditation", "Contemplative Practices", {'relation': 'APPROACH'}),
    ("Meditation", "Emotional Intelligence", {'relation': 'APPROACH'}),
    ("Meditation", "Sustained Heart Health Benefits", {'relation': 'APPROACH'}),

    ("Acupuncture", "Neurovascular Coupling", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Microcirculatory Dynamics", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Meridian Theory Refinement", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Neurovascular Coupling Mechanisms", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Acupuncture Points", {'relation': 'MECHANISM'}),
    ("Acupuncture", "PC6", {'relation': 'MECHANISM'}),
    ("Acupuncture", "HT7", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Neural Pathways", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Vascular Tone", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Blood Flow", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Microcirculation", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Cardiovascular System", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Advanced Imaging Techniques", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Laser Doppler Flowmetry", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Intravital Microscopy", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Microcirculatory Dynamics", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Tissue Perfusion", {'relation': 'MECHANISM'}),
    ("Acupuncture", "TCM Meridian Theory", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Bioelectrical Properties", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Bioenergetic Properties", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Acupoints", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Meridians", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Heart Health", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Personalized Acupuncture Protocols", {'relation': 'MECHANISM'}),
    ("Acupuncture", "TCM Pattern Differentiation", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Heart Qi Deficiency", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Heart Blood Stasis", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Cardiovascular Risk Factors", {'relation': 'MECHANISM'}),
    ("Acupuncture", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Acupuncture", "TCM Dietary Therapy", {'relation': 'APPROACH'}),
    ("Acupuncture", "Herbal Medicine", {'relation': 'APPROACH'}),
    ("Acupuncture", "Lifestyle Recommendations", {'relation': 'APPROACH'}),
    ("Acupuncture", "Individual Constitutions", {'relation': 'APPROACH'}),
    ("Acupuncture", "Cardiovascular Imbalances", {'relation': 'APPROACH'}),

    ("Qi Gong", "Fascial Network Dynamics", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Biofield Interactions", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Microcirculation Enhancement at Cellular Level", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Fascial Network Dynamics", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Body-Wide Communication System", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Qi Gong Movements", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Fascial Tension", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Fluid Dynamics", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Mechanotransduction", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Cardiovascular Function", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Biofield Interactions", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Subtle Energetic Effects", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Cellular Communication", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Physiological Processes", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Advanced Microscopy Techniques", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Atomic Force Microscopy", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Super-Resolution Microscopy", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Microcirculation", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Cellular Level", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Subcellular Level", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Capillary Density", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Red Blood Cell Deformability", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Endothelial Cell Function", {'relation': 'MECHANISM'}),
    ("Qi Gong", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Qi Gong", "Mindful Movement Practices", {'relation': 'APPROACH'}),
    ("Qi Gong", "Breathing Exercises", {'relation': 'APPROACH'}),
    ("Qi Gong", "Jing", {'relation': 'APPROACH'}),
    ("Qi Gong", "Qi", {'relation': 'APPROACH'}),
    ("Qi Gong", "Shen", {'relation': 'APPROACH'}),
    ("Qi Gong", "Essence", {'relation': 'APPROACH'}),
    ("Qi Gong", "Energy", {'relation': 'APPROACH'}),
    ("Qi Gong", "Spirit", {'relation': 'APPROACH'}),

    ("Cardiometabolic Network Modulation", "Systems Biology", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Multi-Omics Integration", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Personalized Interventions", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Systems Biology Approach", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Cardiometabolic Health", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Computational Models", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Interconnected Networks", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Cardiovascular Diseases", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Metabolic Diseases", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Multi-Omics Data", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Genomics", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Transcriptomics", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Proteomics", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Metabolomics", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Microbiomics", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Dynamic Interplay", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Genes", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Proteins", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Metabolites", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Microbial Communities", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Key Network Nodes", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Pathways", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Targeted Interventions", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Pharmacological Approaches", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Non-Pharmacological Approaches", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Personalized Interventions", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Cardiometabolic Risk Profiles", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Network Signatures", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Precision Medicine Strategies", {'relation': 'MECHANISM'}),
    ("Cardiometabolic Network Modulation", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Cardiometabolic Network Modulation", "Lifestyle Interventions", {'relation': 'APPROACH'}),
    ("Cardiometabolic Network Modulation", "Dietary Interventions", {'relation': 'APPROACH'}),
    ("Cardiometabolic Network Modulation", "Mind-Body Interventions", {'relation': 'APPROACH'}),
    ("Cardiometabolic Network Modulation", "Root Causes", {'relation': 'APPROACH'}),
    ("Cardiometabolic Network Modulation", "Cardiometabolic Dysfunction", {'relation': 'APPROACH'}),
    ("Cardiometabolic Network Modulation", "Global Health Optimization", {'relation': 'APPROACH'}),

    ("Gut-Heart Axis Optimization", "Gut-Heart Connection", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Intricate Signaling Pathways", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Microbial Metabolite Signaling", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Vagal Nerve Stimulation", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Barrier Function Integrity", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Gut-Heart Connection", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Intricate Signaling Pathways", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Microbial Metabolites", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "SCFAs", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Bile Acids", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Tryptophan Metabolites", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Cardiovascular Function", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Blood Pressure Regulation", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Inflammation", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Endothelial Function", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Bidirectional Communication", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Vagal Nerve", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Gut Microbiome Modulation", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Vagal Tone", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Autonomic Balance", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Gut Barrier Function Integrity", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Leaky Gut", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Systemic Translocation", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Microbial Products", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "LPS", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Peptidoglycans", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Chronic Inflammation", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Cardiovascular Disease", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Advanced Microbiome Sequencing", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Metabolomics Techniques", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Gut Microbial Profiles", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Microbial Species", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Metabolites", {'relation': 'MECHANISM'}),
    ("Gut-Heart Axis Optimization", "Holistic Approaches", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Prebiotic Strategies", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Probiotic Strategies", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Dietary Fiber Optimization", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Fermented Foods", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Gut-Healing Protocols", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Healthy Gut Microbiome", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Gut-Heart Axis", {'relation': 'APPROACH'}),
    ("Gut-Heart Axis Optimization", "Cardiovascular Well-being", {'relation': 'APPROACH'}),

]

G.add_edges_from(edges_cardio)


print("Knowledge Graph - Cardiovascular Health Section (Nodes and Edges):")
print("\nNodes:")
print(list(G.nodes))
print("\nEdges:")
for u, v, data in G.edges(data=True):
    print(f"({u}) -[:{data['relation']}]-> ({v})")
```

.\run_agents.py
```python
import asyncio
import aiofiles
import json
import os
import sys
from typing import List, Dict, Optional, Any, cast, TypedDict, Tuple, Protocol, Awaitable, Callable, Union, TypeVar, Coroutine, Sequence
from pydantic import BaseModel, Field, SecretStr, validator, field_validator
from langchain_neo4j import Neo4jGraph
from langchain_core.documents import Document
from langchain_ollama import ChatOllama, OllamaEmbeddings
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich import box
from dotenv import load_dotenv
from scripts.logging_config import (
    log_error_with_traceback,
    log_warning_with_context,
    log_info_with_context,
    setup_logging,
    create_progress,
    cleanup_progress,
    log_extraction_results,
    console
)
from langchain_core.globals import set_debug
from langchain.output_parsers import PydanticOutputParser
from prompts.compiler.compiler_prompts import (
    Task, Plan, TaskResult, JoinDecision, CompilerState as BaseCompilerState, get_join_decision_prompt
)
from pathlib import Path
from prompts.knowledge_acquisition.extraction import get_key_terms_prompt, KeyTermsResponse, SourceMetadata
from scripts.text_web_browser_fixed import SimpleTextBrowser, web_search
from datetime import datetime
from scripts.chat_langchain import ChatLangChain
from scripts.llm_compiler import LLMCompiler
from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory
from google.auth.credentials import AnonymousCredentials
import uuid
from scripts.models import KnowledgeAcquisitionConfig, ExtractedKnowledge, SourceMetadata, Relationship, DomainConfig, ConfidenceEvaluation, ConfidenceFactors
from datasets import Dataset
from scripts.synthetic_knowledge import SyntheticKnowledgeGenerator
from scripts.lora_training import LoRATrainer, LoRATrainingConfig, TrainingExample
from scripts.example_generator import ExampleGenerator
from rich.progress import Progress
import shutil
import logging
from dataclasses import dataclass, field, asdict

logger = logging.getLogger(__name__)

set_debug(False)
# Load environment variables
load_dotenv(override=True)

# Initialize logging
setup_logging()

# Add project root to Python path
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.append(project_root)

# Ensure required environment variables
if not os.getenv("OLLAMA_HOST"):
    raise EnvironmentError("OLLAMA_HOST environment variable must be set")

# Set up Google API key
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise EnvironmentError("GOOGLE_API_KEY environment variable must be set")
os.environ["GOOGLE_API_KEY"] = google_api_key

from scripts.knowledge_acquisition import KnowledgeAcquisitionSystem
from scripts.qa_system import QASystem
from scripts.synthetic_knowledge import SyntheticKnowledgeGenerator
from scripts.lora_training import LoRATrainer, LoRATrainingConfig, TrainingExample

console = Console()

class SystemState(BaseModel):
    """Overall system state"""
    domain_name: str = Field(description="Current domain name")
    knowledge_sources: List[Document] = Field(default_factory=list)
    generated_questions: List[Dict] = Field(default_factory=list)
    synthetic_knowledge: List[Dict] = Field(default_factory=list)
    training_examples: List[TrainingExample] = Field(default_factory=list)
    model_metrics: Dict = Field(default_factory=dict)

def print_state_summary(state: SystemState):
    """Print a summary of the current system state"""
    table = Table(title="System State Summary")
    table.add_column("Component", style="cyan")
    table.add_column("Count", style="green")
    table.add_column("Details", style="yellow")
    
    table.add_row(
        "Knowledge Sources",
        str(len(state.knowledge_sources)),
        f"Documents loaded and processed"
    )
    table.add_row(
        "Generated Questions", 
        str(len(state.generated_questions)),
        f"Questions generated from sources"
    )
    table.add_row(
        "Synthetic Knowledge",
        str(len(state.synthetic_knowledge)),
        f"Pieces of synthesized knowledge"
    )
    table.add_row(
        "Training Examples",
        str(len(state.training_examples)),
        f"Examples prepared for training"
    )
    
    if state.model_metrics:
        metrics_str = "\n".join(f"{k}: {v}" for k, v in state.model_metrics.items())
        table.add_row("Model Metrics", "✓", metrics_str)
    else:
        table.add_row("Model Metrics", "✗", "No metrics available")
        
    console.print(table)

class CompilerError(Exception):
    """Base class for compiler errors"""
    pass

class PlanningError(CompilerError):
    """Error during plan generation"""
    pass

class ExecutionError(CompilerError):
    """Error during task execution"""
    pass

class DecisionError(CompilerError):
    """Error during join decision"""
    pass

class CompilerStateDict(TypedDict):
    """State for compiler workflow."""
    # Required fields
    content: str
    domain_name: str
    results: List[TaskResult]
    knowledge_sources: List[Union[Document, Dict[str, Any]]]
    synthetic_knowledge: List[Dict[str, Any]]
    training_examples: List[Dict[str, Any]]
    model_metrics: Dict[str, Any]
    
    # Optional fields
    error: Optional[str]
    feedback: Optional[str]
    plan: Optional[Plan]
    join_decision: Optional[JoinDecision]

class ResearchAgent(LLMCompiler):
    """Research agent that uses LLM compiler for execution."""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize research agent."""
        # Initialize LLMs
        planning_llm = ChatLangChain(
            model="gemini-1.5-flash",
            temperature=0.7,
            api_key=SecretStr(os.getenv("GOOGLE_API_KEY", "")),
            format="json",
            pydantic_schema=Plan
        )
        
        # Initialize compiler with planning LLM
        super().__init__(planning_llm)
        
        # Store configuration
        self.config = config
        
        # Initialize Neo4j graph
        self.graph = Neo4jGraph(
            url=config["neo4j"]["url"],
            username=config["neo4j"]["username"],
            password=config["neo4j"]["password"]
        )
        config["graph"] = self.graph
        
        # Initialize state
        initial_state: CompilerStateDict = {
            "content": "",
            "domain_name": config.get("domain_name", ""),
            "plan": None,
            "results": [],
            "join_decision": None,
            "knowledge_sources": [],
            "synthetic_knowledge": [],
            "training_examples": [],
            "model_metrics": {},
            "error": None,
            "feedback": None
        }
        self.state = cast(BaseCompilerState, initial_state)
        
        # Initialize knowledge system
        self.knowledge_system = KnowledgeAcquisitionSystem(
            KnowledgeAcquisitionConfig(**config["knowledge_acquisition"])
        )
        
        # Initialize QA system
        self.qa_system = QASystem(self.graph, self.llm)
        
        # Initialize knowledge generator
        self.knowledge_generator = SyntheticKnowledgeGenerator(self.graph, self.llm)
        
        # Initialize example generator
        self.example_generator = ExampleGenerator(config)
        
        # Initialize LoRA trainer
        self.lora_trainer = LoRATrainer(
            LoRATrainingConfig(**config["lora_training"])
        )
        
        # Initialize web browser for search
        self.browser = SimpleTextBrowser()
        
        # Register tools
        self.register_tool("research_topics", self.research_topics)
        self.register_tool("synthesize_knowledge", self.synthesize_knowledge)
        self.register_tool("generate_examples", self.generate_examples)
        self.register_tool("train_model", self.train_model)
        
        log_info_with_context("Research agent initialized", "Research")
        console.print(Panel("[bold green]Research Agent Initialized[/bold green]"))

    async def research_topics(self, domain: str) -> Dict[str, Any]:
        """Research topics in a domain."""
        return await self._research_topics(domain)

    async def synthesize_knowledge(self, sources: Dict[str, Any]) -> Dict[str, Any]:
        """Synthesize knowledge from sources."""
        return await self._synthesize_knowledge(sources)

    async def generate_examples(self, knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Generate examples from knowledge."""
        return await self._generate_examples(knowledge)

    async def train_model(self, examples: Dict[str, Any]) -> Dict[str, Any]:
        """Train model on examples."""
        return await self._train_model(examples)

    async def _research_topics(self, domain: str) -> Dict[str, Any]:
        """Research topics in a domain."""
        try:
            # Initialize knowledge system with tracking
            log_info_with_context("Initializing knowledge system", "Research")
            await self.knowledge_system.initialize()
            
            # Track embeddings and tokens
            total_tokens = 0
            total_embeddings = 0
            
            # Get sources with rate limiting and tracking
            max_retries = 3
            base_delay = 2.0
            
            log_info_with_context(f"Starting source collection for domain: {domain}", "Research")
            progress = await create_progress()
            source_task = progress.add_task("[cyan]Collecting sources...", total=max_retries)
            
            for attempt in range(max_retries):
                try:
                    sources = await self._get_sources(domain)
                    if sources:
                        break
                    await asyncio.sleep(base_delay * (2 ** attempt))
                    progress.update(source_task, advance=1)
                except Exception as e:
                    if "429" in str(e) or "Resource exhausted" in str(e):
                        if attempt < max_retries - 1:
                            await asyncio.sleep(base_delay * (2 ** attempt))
                            continue
                    log_error_with_traceback(e, "Error getting sources")
                    return {
                        "knowledge_sources": [],
                        "thought": "Error occurred during research",
                        "metrics": {
                            "total_tokens": total_tokens,
                            "total_embeddings": total_embeddings,
                            "sources_processed": 0,
                            "entities_found": 0,
                            "relationships_created": 0
                        }
                    }
            
            if not sources:
                return {
                    "knowledge_sources": [],
                    "thought": "No sources found",
                    "metrics": {
                        "total_tokens": total_tokens,
                        "total_embeddings": total_embeddings,
                        "sources_processed": 0,
                        "entities_found": 0,
                        "relationships_created": 0
                    }
                }
            
            # Process sources with tracking
            knowledge_sources = []
            total_entities = 0
            total_relationships = 0
            
            process_task = progress.add_task("[cyan]Processing sources...", total=len(sources))
            
            for i, source in enumerate(sources):
                try:
                    # Add delay between processing sources
                    await asyncio.sleep(1.0)
                    
                    # Extract knowledge with tracking
                    log_info_with_context(f"Processing source {i+1}/{len(sources)}", "Research")
                    knowledge = await self._extract_knowledge(source)
                    
                    if knowledge:
                        # Track metrics
                        total_entities += len(knowledge.entities)
                        total_relationships += len(knowledge.relationships)
                        total_embeddings += 1  # Count embeddings generated
                        
                        # Store knowledge with ID
                        knowledge_dict = knowledge.model_dump()
                        knowledge_dict["id"] = i
                        knowledge_sources.append(knowledge_dict)
                        
                        # Update state with knowledge sources
                        self.state["knowledge_sources"] = knowledge_sources
                        
                        # Log progress
                        console.print(f"[green]✓ Processed source {i+1}[/green]")
                        console.print(f"  Entities found: {len(knowledge.entities)}")
                        console.print(f"  Relationships created: {len(knowledge.relationships)}")
                        console.print(f"  Confidence: {knowledge.confidence:.2f}")
                    
                    progress.update(process_task, advance=1)
                    
                except Exception as e:
                    source_metadata = source.get("metadata", {})
                    query = source_metadata.get("query", "unknown")
                    log_error_with_traceback(e, f"Error processing source: {query}")
                    progress.update(process_task, advance=1)
                    continue
            
            # Generate final metrics
            metrics = {
                "total_tokens": total_tokens,
                "total_embeddings": total_embeddings,
                "sources_processed": len(knowledge_sources),
                "entities_found": total_entities,
                "relationships_created": total_relationships
            }
            
            return {
                "knowledge_sources": knowledge_sources,
                "thought": f"Researched {len(knowledge_sources)} sources about {domain}",
                "metrics": metrics
            }
                
        except Exception as e:
            log_error_with_traceback(e, "Error in research topics")
            return {
                "knowledge_sources": [],
                "thought": "Error occurred during research",
                "metrics": {
                    "total_tokens": 0,
                    "total_embeddings": 0,
                    "sources_processed": 0,
                    "entities_found": 0,
                    "relationships_created": 0
                }
            }

    async def _synthesize_knowledge(self, sources: Dict[str, Any]) -> Dict[str, Any]:
        """Synthesize knowledge from sources."""
        try:
            # Initialize progress tracking
            progress = await create_progress()
            synthesis_task = progress.add_task("[cyan]Synthesizing knowledge...", total=len(sources.get("knowledge_sources", [])))
            
            # Track synthesized knowledge
            synthetic_knowledge = []
            
            # Get knowledge sources from input
            knowledge_sources = sources.get("knowledge_sources", [])
            if not knowledge_sources:
                log_warning_with_context("No knowledge sources found in input", "Knowledge Synthesis")
                return {
                    "synthetic_knowledge": [],
                    "thought": "No knowledge sources available for synthesis"
                }
            
            # Process each source
            for source in knowledge_sources:
                try:
                    if isinstance(source, str):
                        # Skip if source is a string
                        log_warning_with_context("Invalid source format (string), skipping", "Knowledge Synthesis")
                        progress.update(synthesis_task, advance=1)
                        continue
                        
                    # Extract content and metadata
                    content = source.get("content", "").strip()
                    metadata = source.get("metadata", {})
                    
                    if not content:
                        log_warning_with_context("Empty content, skipping source", "Knowledge Synthesis")
                        progress.update(synthesis_task, advance=1)
                        continue
                        
                    # Process source with knowledge system
                    try:
                        knowledge = await self.knowledge_system.process_source(content)
                        if not knowledge:
                            log_warning_with_context("No knowledge extracted, skipping source", "Knowledge Synthesis")
                            progress.update(synthesis_task, advance=1)
                            continue
                            
                        # Add source metadata and ID
                        if isinstance(knowledge, dict):
                            # Get existing metadata
                            knowledge_metadata = knowledge.get("metadata", {})
                            # Convert metadata to dict if it's a Pydantic model
                            if hasattr(knowledge_metadata, "model_dump"):
                                knowledge_metadata = knowledge_metadata.model_dump()
                            elif hasattr(knowledge_metadata, "dict"):
                                knowledge_metadata = knowledge_metadata.dict()
                                
                            # Create new combined metadata
                            combined_metadata = {**knowledge_metadata, **metadata}
                            knowledge["metadata"] = combined_metadata
                            
                            # Add source ID to track relationships
                            knowledge["source_id"] = source.get("id")
                        
                        # Convert relationships to dictionaries
                        if "relationships" in knowledge:
                            knowledge["relationships"] = [
                                {
                                    "source": rel.source,
                                    "relation": rel.relation,
                                    "target": rel.target,
                                    "domain": rel.domain,
                                    "confidence": rel.confidence
                                }
                                for rel in knowledge["relationships"]
                            ]
                        
                        # Add to synthetic knowledge
                        synthetic_knowledge.append(knowledge)
                        
                        # Update progress and log
                        progress.update(synthesis_task, advance=1)
                        console.print(f"[green]✓ Synthesized knowledge from source {metadata.get('source_file', 'unknown')}[/green]")
                        console.print(f"  Title: {metadata.get('title', 'Unknown')}")
                        console.print(f"  Entities found: {len(knowledge.get('entities', []))}")
                        console.print(f"  Relationships found: {len(knowledge.get('relationships', []))}")
                        console.print(f"  Confidence: {knowledge.get('confidence', 0.0):.2f}")
                        
                    except Exception as e:
                        log_error_with_traceback(e, "Error in knowledge extraction")
                        progress.update(synthesis_task, advance=1)
                        continue
                        
                except Exception as e:
                    log_error_with_traceback(e, f"Error synthesizing knowledge from source {source.get('metadata', {}).get('source_file', 'unknown')}")
                    progress.update(synthesis_task, advance=1)
                    continue
            
            # Save synthetic knowledge to disk
            results_dir = os.path.join("results", self.config.get("domain_name", "test_domain"))
            os.makedirs(results_dir, exist_ok=True)
            
            knowledge_path = os.path.join(results_dir, "synthetic_knowledge.json")
            with open(knowledge_path, "w", encoding="utf-8") as f:
                # Convert any remaining Pydantic models to dicts before saving
                serializable_knowledge = []
                for k in synthetic_knowledge:
                    if isinstance(k, dict):
                        # If it's already a dict, just append it
                        serializable_knowledge.append(k)
                    elif hasattr(k, "model_dump"):
                        # If it's a Pydantic v2 model
                        serializable_knowledge.append(k.model_dump())
                    elif hasattr(k, "dict"):
                        # If it's a Pydantic v1 model
                        serializable_knowledge.append(k.dict())
                    else:
                        # If it's something else, try to convert to dict
                        try:
                            serializable_knowledge.append(dict(k))
                        except:
                            # If conversion fails, just append as is
                            serializable_knowledge.append(k)
                json.dump(serializable_knowledge, f, indent=2, ensure_ascii=False)
            
            # Update state with synthetic knowledge
            self.state["synthetic_knowledge"] = synthetic_knowledge
            
            # Store serializable knowledge in system state for example generation
            self.state["synthetic_knowledge"] = serializable_knowledge
            
            # Count total entities and relationships
            total_entities = sum(len(k.get('entities', [])) for k in synthetic_knowledge)
            total_relationships = sum(len(k.get('relationships', [])) for k in synthetic_knowledge)
            
            return {
                "synthetic_knowledge": synthetic_knowledge,
                "thought": f"Successfully synthesized knowledge from {len(sources.get('knowledge_sources', []))} sources with {total_entities} entities and {total_relationships} relationships"
            }
                
        except Exception as e:
            log_error_with_traceback(e, "Error in knowledge synthesis")
            return {
                "synthetic_knowledge": [],
                "thought": f"Error synthesizing knowledge: {str(e)}"
            }

    async def _generate_examples(self, synthetic_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """Generate training examples from synthetic knowledge."""
        try:
            examples = []
            knowledge_list = synthetic_knowledge.get("synthetic_knowledge", [])
            
            if not knowledge_list:
                return {
                    "training_examples": [],
                    "thought": "No synthetic knowledge available for example generation"
                }
            
            for knowledge in knowledge_list:
                if not isinstance(knowledge, dict):
                    continue
                
                # Generate examples using the knowledge
                example = {
                    "input_text": knowledge.get("content", ""),
                    "output_text": json.dumps({
                        "entities": knowledge.get("entities", []),
                        "relationships": knowledge.get("relationships", [])
                    }),
                    "metadata": knowledge.get("metadata", {})
                }
                examples.append(example)
            
            # Save examples to file
            results_dir = os.path.join("results", self.state["domain_name"])
            os.makedirs(results_dir, exist_ok=True)
            examples_path = os.path.join(results_dir, "training_examples.json")
            with open(examples_path, "w", encoding="utf-8") as f:
                json.dump(examples, f, indent=2, ensure_ascii=False)
            
            return {
                "training_examples": examples,
                "thought": f"Generated {len(examples)} training examples"
            }
        except Exception as e:
            logger.error(f"Error generating examples: {str(e)}")
            return {"training_examples": [], "thought": f"Error generating examples: {str(e)}"}

    async def _train_model(self, examples: Dict[str, Any]) -> Dict[str, Any]:
        """Train model on examples."""
        try:
            if not examples:
                return {
                    "metrics": {
                        "error": "No training examples provided"
                    },
                    "thought": "Failed to train model: no examples available"
                }
            
            # Initialize progress tracking
            progress = await create_progress()
            
            # Split data
            train_data, eval_data = self._split_data(examples)
            
            # 1. Prepare training data
            prep_task = progress.add_task("[cyan]Preparing training data...", total=100)
            
            # Format examples
            formatted_examples = []
            for i, example in enumerate(train_data):
                try:
                    formatted = await self._format_example(example)
                    if formatted:
                        formatted_examples.append(formatted)
                    progress.update(prep_task, completed=(i * 100) // len(train_data))
                except Exception as e:
                    log_error_with_traceback(e, f"Error formatting example {i}")
                    continue
                
            progress.update(prep_task, completed=100)
            console.print("[green]✓ Training data prepared[/green]")
            
            if not formatted_examples:
                return {
                    "metrics": {
                        "error": "No valid formatted examples"
                    },
                    "thought": "Failed to train model: no valid examples after formatting"
                }
            
            # 2. Configure training
            config_task = progress.add_task("[cyan]Configuring training...", total=100)
            
            # Get LoRA config
            lora_config = await self._get_lora_config(formatted_examples)
            
            progress.update(config_task, completed=100)
            console.print("[green]✓ Training configuration prepared[/green]")
            
            # 3. Train model
            train_task = progress.add_task("[cyan]Training model...", total=100)
            
            # Initialize metrics tracking
            metrics = {
                "loss": [],
                "eval_loss": [],
                "train_samples": len(train_data),
                "eval_samples": len(eval_data),
                "epochs": lora_config.get("num_epochs", 3),
                "learning_rate": lora_config.get("learning_rate", 3e-4),
                "batch_size": lora_config.get("batch_size", 4)
            }
            
            # Train model using LoRA trainer
            try:
                # Convert examples to Datasets
                train_dataset = Dataset.from_dict({
                    "text": [ex["text"] for ex in formatted_examples],
                    "metadata": [ex["metadata"] for ex in formatted_examples]
                })
                eval_dataset = Dataset.from_dict({
                    "text": [ex["text"] for ex in eval_data],
                    "metadata": [ex["metadata"] for ex in eval_data]
                }) if eval_data else None
                
                # Train model
                train_results = self.lora_trainer.train(
                    train_dataset=train_dataset,
                    eval_dataset=eval_dataset,
                    num_train_epochs=metrics["epochs"],
                    per_device_train_batch_size=metrics["batch_size"],
                    learning_rate=metrics["learning_rate"],
                    weight_decay=0.01,
                    max_grad_norm=1.0
                )
                metrics.update(train_results.model_dump())
                progress.update(train_task, completed=100)
                console.print("[green]✓ Model training completed[/green]")
                
                # Save metrics to disk
                results_dir = os.path.join("results", self.config.get("domain_name", "test_domain"))
                os.makedirs(results_dir, exist_ok=True)
                
                metrics_path = os.path.join(results_dir, "model_metrics.json")
                with open(metrics_path, "w", encoding="utf-8") as f:
                    json.dump(metrics, f, indent=2)
                
                # Save LoRA adapter
                adapter_dir = os.path.join(results_dir, "lora_adapter")
                os.makedirs(adapter_dir, exist_ok=True)
                
                # Save adapter config
                config_path = os.path.join(adapter_dir, "adapter_config.json")
                with open(config_path, "w", encoding="utf-8") as f:
                    json.dump(lora_config, f, indent=2)
                
                # Copy adapter files
                adapter_files = [
                    "adapter_model.bin",
                    "adapter_config.json",
                    "special_tokens_map.json",
                    "tokenizer_config.json",
                    "tokenizer.json",
                    "vocab.json",
                    "merges.txt"
                ]
                
                for file in adapter_files:
                    src = os.path.join(lora_config["output_dir"], file)
                    if os.path.exists(src):
                        dst = os.path.join(adapter_dir, file)
                        shutil.copy2(src, dst)
                
            except Exception as e:
                log_error_with_traceback(e, "Error during model training")
                metrics["error"] = str(e)
                progress.update(train_task, completed=100)
                console.print("[red]✗ Model training failed[/red]")
            
            await cleanup_progress()
            return {
                "metrics": metrics,
                "thought": f"Successfully trained model on {len(train_data)} examples with {len(eval_data)} validation examples"
            }
                
        except Exception as e:
            log_error_with_traceback(e, "Error in model training")
            await cleanup_progress()
            return {
                "metrics": {
                    "error": str(e)
                },
                "thought": f"Error training model: {str(e)}"
            }
        finally:
            # Cleanup
            if hasattr(self, 'knowledge_system'):
                try:
                    # Get session attribute if it exists
                    session = getattr(self.knowledge_system, '_session', None) or getattr(self.knowledge_system, 'session', None)
                    if session and hasattr(session, 'close'):
                        await session.close()
                except Exception as e:
                    logger.error(f"Error closing knowledge system session: {str(e)}")
            if hasattr(self, 'qa_system'):
                try:
                    # Get session attribute if it exists
                    session = getattr(self.qa_system, '_session', None) or getattr(self.qa_system, 'session', None)
                    if session and hasattr(session, 'close'):
                        await session.close()
                except Exception as e:
                    logger.error(f"Error closing QA system session: {str(e)}")
            if hasattr(self, 'browser'):
                try:
                    # Get session attribute if it exists
                    session = getattr(self.browser, 'session', None)
                    if session and hasattr(session, 'close'):
                        await session.close()
                except Exception as e:
                    logger.error(f"Error closing browser session: {str(e)}")
            await cleanup_progress()

    async def run(self, initial_state: Dict[str, Any]) -> CompilerStateDict:
        """Run the research agent with the given initial state."""
        try:
            # Initialize state with required fields
            state_dict: CompilerStateDict = {
                "content": initial_state.get("content", ""),
                "domain_name": initial_state.get("domain_name", ""),
                "results": initial_state.get("results", []),
                "knowledge_sources": initial_state.get("knowledge_sources", []),
                "synthetic_knowledge": initial_state.get("synthetic_knowledge", []),
                "training_examples": initial_state.get("training_examples", []),
                "model_metrics": initial_state.get("model_metrics", {}),
                "error": initial_state.get("error"),
                "feedback": initial_state.get("feedback"),
                "plan": initial_state.get("plan"),
                "join_decision": initial_state.get("join_decision")
            }
            
            self.state = cast(BaseCompilerState, state_dict)
            
            # Create plan
            plan = await self.generate_plan(self.state)
            state_dict["plan"] = plan
            self.state = cast(BaseCompilerState, state_dict)
            
            # Execute tasks
            results = await self.execute_tasks(plan.tasks, self.state)
            state_dict["results"] = results
            self.state = cast(BaseCompilerState, state_dict)
            
            # Generate final result
            final_result = await self._generate_final_result(self.state)
            state_dict["join_decision"] = final_result
            self.state = cast(BaseCompilerState, state_dict)
            
            return state_dict
        except Exception as e:
            logger.error(f"Error in research agent: {str(e)}")
            error_state: CompilerStateDict = {
                    "content": "",
                "domain_name": "",
                    "results": [],
                "knowledge_sources": [],
                "synthetic_knowledge": [],
                "training_examples": [],
                "model_metrics": {},
                "error": str(e),
                    "feedback": None,
                "plan": None,
                "join_decision": None
            }
            return error_state
        finally:
            # Cleanup
            if hasattr(self, 'knowledge_system'):
                try:
                    # Get session attribute if it exists
                    session = getattr(self.knowledge_system, '_session', None) or getattr(self.knowledge_system, 'session', None)
                    if session and hasattr(session, 'close'):
                        await session.close()
                except Exception as e:
                    logger.error(f"Error closing knowledge system session: {str(e)}")
            if hasattr(self, 'qa_system'):
                try:
                    # Get session attribute if it exists
                    session = getattr(self.qa_system, '_session', None) or getattr(self.qa_system, 'session', None)
                    if session and hasattr(session, 'close'):
                        await session.close()
                except Exception as e:
                    logger.error(f"Error closing QA system session: {str(e)}")
            if hasattr(self, 'browser'):
                try:
                    # Get session attribute if it exists
                    session = getattr(self.browser, 'session', None)
                    if session and hasattr(session, 'close'):
                        await session.close()
                except Exception as e:
                    logger.error(f"Error closing browser session: {str(e)}")
            await cleanup_progress()

    async def _extract_knowledge(self, source: Dict[str, Any]) -> Optional[ExtractedKnowledge]:
        """Extract knowledge from a source."""
        try:
            # Get content and metadata
            content = source.get("content", "").strip()
            source_metadata = source.get("metadata", {})
            
            if not content:
                log_warning_with_context("Empty content, skipping source", "Knowledge Extraction")
                return None
            
            # Generate QA pairs for content
            log_info_with_context("Generating QA pairs", "Knowledge Extraction")
            questions = await self.qa_system.generate_questions(content, num_questions=5)
            
            # Track QA metrics
            qa_metrics = {
                "questions_generated": len(questions),
                "questions_answered": 0,
                "average_confidence": 0.0
            }
            
            # Process each question
            answers = []
            total_confidence = 0.0
            
            for question in questions:
                try:
                    response = await self.qa_system.process_qa_chain(question.question)
                    if response and response.answer:
                        answers.append({
                            "question": question.question,
                            "answer": response.answer,
                            "confidence": response.confidence,
                            "sources": response.sources
                        })
                        total_confidence += response.confidence
                        qa_metrics["questions_answered"] += 1
                except Exception as e:
                    log_error_with_traceback(e, f"Error processing question: {question.question}")
                    continue
            
            if qa_metrics["questions_answered"] > 0:
                qa_metrics["average_confidence"] = total_confidence / qa_metrics["questions_answered"]
            
            # Process source with knowledge system
            log_info_with_context("Extracting knowledge", "Knowledge Extraction")
            result = await self.knowledge_system.process_source(content)
            if not result or not isinstance(result, dict):
                log_warning_with_context("No knowledge extracted, skipping source", "Knowledge Extraction")
                return None
            
            # Create metadata from dict
            metadata_dict = result.get("metadata", {})
            if isinstance(metadata_dict, dict):
                # Create new metadata object
                metadata = SourceMetadata(
                    source_type=metadata_dict.get("source_type", "text"),
                    confidence_score=metadata_dict.get("confidence_score", 0.8),
                    domain_relevance=metadata_dict.get("domain_relevance", 0.8),
                    timestamp=metadata_dict.get("timestamp", datetime.now().isoformat()),
                    validation_status=metadata_dict.get("validation_status", "pending"),
                    domain=metadata_dict.get("domain", "medical"),
                    qa_metrics=qa_metrics  # Add QA metrics to metadata
                )
                
                # Update with source metadata
                if isinstance(source_metadata, dict):
                    metadata = SourceMetadata(
                        source_type=source_metadata.get("source_type", metadata.source_type),
                        confidence_score=source_metadata.get("confidence_score", metadata.confidence_score),
                        domain_relevance=source_metadata.get("domain_relevance", metadata.domain_relevance),
                        timestamp=source_metadata.get("timestamp", metadata.timestamp),
                        validation_status=source_metadata.get("validation_status", metadata.validation_status),
                        domain=source_metadata.get("domain", metadata.domain),
                        qa_metrics=qa_metrics
                    )
            else:
                metadata = metadata_dict
            
            # Create ExtractedKnowledge
            extracted = ExtractedKnowledge(
                content=result.get("content", ""),
                entities=result.get("entities", []),
                relationships=result.get("relationships", []),
                metadata=metadata,
                confidence=result.get("confidence", 0.5),
                domain=metadata.domain if hasattr(metadata, "domain") else "medical",
                qa_pairs=answers  # Add QA pairs to extracted knowledge
            )
            return extracted
            
        except Exception as e:
            log_error_with_traceback(e, "Error in knowledge extraction")
            return None

    def _split_data(self, examples: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Split examples into training and evaluation sets."""
        try:
            # Get examples list from input
            example_list = examples.get("training_examples", [])
            
            # If no examples, return empty lists
            if not example_list:
                return [], []
            
            # Split into train/eval sets (80/20)
            split_idx = int(len(example_list) * 0.8)
            train_data = example_list[:split_idx]
            eval_data = example_list[split_idx:]
            
            return train_data, eval_data
            
        except Exception as e:
            log_error_with_traceback(e, "Error splitting data")
            return [], []

    async def _format_example(self, example: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Format an example for training."""
        try:
            # Validate example has required fields
            if "input_text" in example and "output_text" in example:
                return {
                    "text": f"Input: {example['input_text']}\nOutput: {example['output_text']}",
                    "metadata": {
                        **example.get("metadata", {}),
                        "id": example.get("id", str(uuid.uuid4())),
                        "quality_score": example.get("quality_score", 0.5)
                    }
                }
            return None
            
        except Exception as e:
            log_error_with_traceback(e, "Error formatting example")
            return None

    async def _get_lora_config(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Get LoRA configuration for training."""
        try:
            # Get LoRA config from system state
            lora_config = self.config.get("lora_training", {})
            return {
                "model_name": lora_config.get("model_name", "mistral"),
                "r": lora_config.get("r", 8),
                "bias": lora_config.get("bias", "none"),
                "task_type": lora_config.get("task_type", "CAUSAL_LM"),
                "target_modules": lora_config.get("target_modules", ["q_proj", "v_proj"]),
                "inference_mode": lora_config.get("inference_mode", False),
                "lora_alpha": lora_config.get("lora_alpha", 32),
                "lora_dropout": lora_config.get("lora_dropout", 0.1),
                "num_epochs": lora_config.get("num_epochs", 3),
                "learning_rate": lora_config.get("learning_rate", 3e-4),
                "batch_size": lora_config.get("batch_size", 4)
            }
            
        except Exception as e:
            log_error_with_traceback(e, "Error getting LoRA config")
            return {}

    async def make_join_decision(self, state: BaseCompilerState) -> JoinDecision:
        """Make join decision based on task results."""
        try:
            log_info_with_context("Making join decision", "Decision")
            console.print("\n[bold green]Making Join Decision...[/bold green]")
            
            # Convert state to dictionary for access
            state_dict = cast(CompilerStateDict, dict(state))
            
            # Get task results
            results = state_dict["results"]
            if not results:
                return JoinDecision(
                    complete=False,
                    thought="No task results available",
                    replan=True,
                    feedback="Need to execute tasks first"
                )
                
            # Check each task result
            failed_tasks = []
            successful_tasks = []
            for result in results:
                if result.error:
                    failed_tasks.append(result.task_id)
                elif result.result is not None:
                    successful_tasks.append(result.task_id)
                    
            # Analyze task dependencies
            plan = state_dict["plan"]
            if plan and plan.tasks:
                tasks_by_id = {task.idx: task for task in plan.tasks}
                
                # Check if any failed task blocks others
                blocking_failures = []
                for task_id in failed_tasks:
                    # Find tasks that depend on this failed task
                    blocked_tasks = [
                        task.idx for task in plan.tasks 
                        if task_id in task.dependencies
                    ]
                    if blocked_tasks:
                        blocking_failures.append((task_id, blocked_tasks))
                
                # If there are blocking failures, we need to replan
                if blocking_failures:
                    feedback = []
                    for failed_id, blocked_ids in blocking_failures:
                        task = tasks_by_id.get(failed_id)
                        if task:
                            feedback.append(f"Task {failed_id} ({task.tool}) failed and blocks tasks {blocked_ids}")
                    return JoinDecision(
                        complete=False,
                        thought=f"Critical task failures blocking dependent tasks",
                        replan=True,
                        feedback="; ".join(feedback)
                    )
                
                # Check if all tasks completed successfully
                all_tasks = set(task.idx for task in plan.tasks)
                if all_tasks.issubset(set(successful_tasks)):
                    return JoinDecision(
                        complete=True,
                        thought="All tasks completed successfully",
                        replan=False,
                        feedback=None
                    )
                
                # Check if remaining tasks can proceed
                remaining_tasks = all_tasks - set(successful_tasks)
                blocked_tasks = set()
                for task_id in remaining_tasks:
                    task = tasks_by_id.get(task_id)
                    if task and any(dep in failed_tasks for dep in task.dependencies):
                        blocked_tasks.add(task_id)
                
                if blocked_tasks:
                    return JoinDecision(
                        complete=False,
                        thought=f"Tasks {blocked_tasks} are blocked by failed dependencies",
                        replan=True,
                        feedback=f"Need to retry failed tasks: {failed_tasks}"
                    )
                
                # If we have remaining tasks but they're not blocked, continue execution
                return JoinDecision(
                    complete=False,
                    thought=f"Tasks {remaining_tasks} still need to be executed",
                    replan=False,
                    feedback=None
                )
            
            # If we have no plan but have results, something went wrong
            return JoinDecision(
                complete=False,
                thought="No execution plan available",
                replan=True,
                feedback="Need to generate execution plan"
            )
                
        except Exception as e:
            log_error_with_traceback(e, "Error in join decision")
            return JoinDecision(
                complete=False,
                thought=f"Error in join decision: {str(e)}",
                replan=True,
                feedback="Error occurred during join decision"
            )

    async def _get_sources(self, domain: str) -> List[Dict[str, Any]]:
        """Get sources for a domain."""
        try:
            # Get actual domain name from state if needed
            if domain == "{state}":
                state_dict = self._get_state_dict()
                domain = state_dict["domain_name"]
            
            log_info_with_context(f"Starting topic research for domain: {domain}", "Research")
            
            # Perform searches
            sources = []
            search_queries = [
                f"{domain} overview",
                f"{domain} key concepts",
                f"{domain} latest developments"
            ]
            
            for query in search_queries:
                log_info_with_context(f"Searching for: {query}", "Search")
                results = await web_search(query, self.config)
                if results and "No results found" not in results:
                    sources.append({
                        "content": results,
                        "metadata": {
                            "query": query,
                            "timestamp": datetime.now().isoformat(),
                            "source_type": "web_search"
                        }
                    })
            
            return sources
            
        except Exception as e:
            log_error_with_traceback(e, "Error getting sources")
            return []

    def _get_state_dict(self) -> CompilerStateDict:
        """Get current state as a dictionary."""
        return cast(CompilerStateDict, dict(self.state))

async def main():
    """Main entry point."""
    try:
        # Parse arguments
        parser = argparse.ArgumentParser(description='Run research agent')
        parser.add_argument('--config', type=str, default='config.json', help='Path to config file')
        args = parser.parse_args()
        
        # Load config
        async with aiofiles.open(args.config, 'r') as f:
            config = json.loads(await f.read())
        
        log_info_with_context("Initializing research agent", "Main")
        agent = ResearchAgent(config)
        
        log_info_with_context("Starting research agent", "Main")
        
        # Create initial state
        initial_state = {
            "content": "",
            "domain_name": config.get("domain_name", ""),
            "plan": None,
            "results": [],
            "join_decision": None,
            "final_result": None,
            "error": None,
            "feedback": None,
            "knowledge_sources": [],
            "synthetic_knowledge": [],
            "training_examples": [],
            "model_metrics": {}
        }
        
        # Run agent
        await agent.run(initial_state)
        
    except Exception as e:
        log_error_with_traceback(e, "Fatal error in research agent")
        raise

if __name__ == "__main__":
    # Set up argparse at module level
    import argparse
    
    # Run main with asyncio
    asyncio.run(main())

```

.\run_tests.py
```python
import os
import sys
import pytest
from loguru import logger

def setup_logging():
    """Configure logging for tests"""
    log_file = "test_results.log"
    
    # Remove default handler
    logger.remove()
    
    # Add handlers for both file and console
    logger.add(log_file, rotation="1 day", retention="7 days", level="DEBUG")
    logger.add(sys.stderr, level="INFO")
    
    return log_file

def main():
    """Run the integration tests"""
    log_file = setup_logging()
    logger.info("Starting integration tests...")
    
    # Add the project root to Python path
    project_root = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, project_root)
    
    # Configure pytest arguments
    pytest_args = [
        "integration_tests/test_visual_qa_system.py",
        "-v",                    # Verbose output
        "--capture=no",         # Show print statements
        "-s",                   # Show output
        "--tb=short",          # Shorter traceback format
        f"--log-file={log_file}",  # Log to file
        "--log-level=DEBUG",    # Log level
        "-n", "auto"           # Parallel execution
    ]
    
    try:
        # Run tests
        exit_code = pytest.main(pytest_args)
        
        if exit_code == 0:
            logger.info("All tests passed successfully!")
        else:
            logger.error(f"Tests failed with exit code: {exit_code}")
            
        logger.info(f"Test results have been saved to {log_file}")
        return exit_code
        
    except Exception as e:
        logger.error(f"Error running tests: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 
```

.\setup.py
```python
from setuptools import setup, find_packages

setup(
    name="agents",
 
```

.\test_imports.py
```python
 
```

.\test_model_behavior.py
```python
from typing import List, Dict, Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_ollama import ChatOllama
from loguru import logger

# Configure logging
logger.add("model_behavior.log", rotation="100 MB")

class SimpleResponse(BaseModel):
    """Test simple response format"""
    answer: str = Field(description="The answer to the question")
    confidence: float = Field(description="Confidence score between 0 and 1")

def test_model_behavior():
    """Test different prompting strategies and response formats"""
    
    # Initialize LLM with json format
    llm = ChatOllama(
        model="deepscaler",
        temperature=0.2,
        format="json",
        mirostat=2,
        mirostat_eta=0.1,
        mirostat_tau=5.0
    )

    
    # Initialize parser
    parser = PydanticOutputParser(pydantic_object=SimpleResponse)
    
    logger.info("Starting model behavior tests")
    
    # Test 1: Using parser format instructions
    logger.info("Test 1: Using parser format instructions")
    prompt = PromptTemplate(
        template="Answer the user question.\n{format_instructions}\n\nQuestion: {question}",
        input_variables=["question"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    chain = prompt | llm | parser
    
    try:
        result = chain.invoke({"question": "What is the capital of France?"})
        logger.info(f"Parsed response: {result}")
    except Exception as e:
        logger.error(f"Error in test 1: {e}")
    
    # Test 2: Using explicit schema
    logger.info("\nTest 2: Using explicit schema")
    prompt = PromptTemplate(
        template="""Answer the user question. Your response must be a valid JSON object with these fields:
- answer (string): Your answer to the question
- confidence (number): Your confidence between 0 and 1

Question: {question}""",
        input_variables=["question"]
    )
    
    chain = prompt | llm | parser
    
    try:
        result = chain.invoke({"question": "What is the capital of France?"})
        logger.info(f"Parsed response: {result}")
    except Exception as e:
        logger.error(f"Error in test 2: {e}")

if __name__ == "__main__":
    test_model_behavior() 
```

.\test_visual_qa.py
```python
import os
import shutil
from pathlib import Path
from scripts.visual_qa import visualizer, encode_image, resize_image
from loguru import logger

def test_visual_qa():
    """Test the visual QA functionality"""
    
    # Create test directory if it doesn't exist
    test_dir = Path("test_data")
    test_dir.mkdir(exist_ok=True)
    
    # Create a simple test image
    from PIL import Image, ImageDraw
    
    # Create a test image
    img = Image.new('RGB', (200, 200), color='white')
    draw = ImageDraw.Draw(img)
    draw.rectangle([50, 50, 150, 150], fill='blue')
    draw.ellipse([75, 75, 125, 125], fill='red')
    
    # Save test image
    test_image_path = test_dir / "test_image.png"
    img.save(test_image_path)
    
    logger.info(f"Created test image at {test_image_path}")
    
    resized_path = None
    try:
        # Test image encoding
        logger.info("Testing image encoding...")
        encoded = encode_image(str(test_image_path))
        assert encoded, "Image encoding failed"
        logger.info("Image encoding successful")
        
        # Test image resizing
        logger.info("Testing image resizing...")
        resized_path = resize_image(str(test_image_path))
        assert os.path.exists(resized_path), "Image resizing failed"
        logger.info("Image resizing successful")
        
        # Test visual QA
        logger.info("Testing visual QA...")
        question = "What shapes can you see in this image?"
        result = visualizer.invoke({"image_path": str(test_image_path), "question": question})
        assert result, "Visual QA failed"
        logger.info(f"Visual QA result: {result}")
        
        logger.info("All tests completed successfully!")
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        raise
    finally:
        # Cleanup
        try:
            if resized_path:
                resized_dir = Path(resized_path).parent
                if resized_dir.exists():
                    shutil.rmtree(resized_dir)
            if test_dir.exists():
                shutil.rmtree(test_dir)
            logger.info("Cleanup completed")
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")

if __name__ == "__main__":
    test_visual_qa() 
```

.\agents\co_scientist\base_agent.py
```python
"""Base agent class for AI co-scientist system."""

from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableSerializable

class AgentState(BaseModel):
    """Base state for all agents."""
    agent_id: str = Field(description="Unique identifier for this agent")
    agent_type: str = Field(description="Type of agent (e.g. generation, reflection, etc)")
    memory: Dict[str, Any] = Field(default_factory=dict, description="Agent's memory/context")
    tools: Dict[str, Any] = Field(default_factory=dict, description="Tools available to this agent")
    metrics: Dict[str, Any] = Field(default_factory=dict, description="Agent performance metrics")

class BaseAgent:
    """Base agent class with common functionality."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str,
        agent_type: str,
        system_prompt: str,
        output_parser: Optional[PydanticOutputParser] = None
    ):
        """Initialize the base agent.
        
        Args:
            llm: The language model to use
            agent_id: Unique identifier for this agent
            agent_type: Type of agent (e.g. generation, reflection, etc)
            system_prompt: System prompt for this agent
            output_parser: Optional output parser for structured outputs
        """
        self.llm = llm
        self.state = AgentState(
            agent_id=agent_id,
            agent_type=agent_type,
            memory={},
            tools={},
            metrics={}
        )
        self.system_prompt = system_prompt
        self.output_parser = output_parser
        
        # Create base prompt template
        if output_parser:
            format_instructions = output_parser.get_format_instructions()
            messages = [
                SystemMessage(content=system_prompt.format(format_instructions=format_instructions)),
                HumanMessage(content="{input}")
            ]
        else:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content="{input}")
            ]
        self.prompt = ChatPromptTemplate.from_messages(messages)
        
        # Create chain
        self.chain = self.prompt | self.llm
        if output_parser:
            self.chain = self.chain | output_parser
            
    def register_tool(self, name: str, tool: Any) -> None:
        """Register a tool with this agent."""
        self.state.tools[name] = tool
        
    def update_memory(self, key: str, value: Any) -> None:
        """Update agent's memory."""
        self.state.memory[key] = value
        
    def get_memory(self, key: str) -> Optional[Any]:
        """Get value from agent's memory."""
        return self.state.memory.get(key)
        
    def update_metrics(self, metrics: Dict[str, Any]) -> None:
        """Update agent's performance metrics."""
        self.state.metrics.update(metrics)
        
    def get_metrics(self) -> Dict[str, Any]:
        """Get agent's performance metrics."""
        return self.state.metrics
        
    async def arun(self, input_data: Dict[str, Any]) -> Any:
        """Run the agent's chain asynchronously."""
        try:
            from rich.console import Console
            console = Console()
            with console.status(f"[bold cyan]{self.state.agent_type}[/bold cyan] agent thinking..."):
                result = await self.chain.ainvoke(input_data)
            return result
        except Exception as e:
            raise RuntimeError(f"Agent {self.state.agent_id} failed: {str(e)}")
            
    def get_state(self) -> AgentState:
        """Get the current agent state."""
        return self.state 
```

.\agents\co_scientist\main.py
```python
"""Main entry point for the AI co-scientist system."""

from typing import Any, Dict, List, Optional, Literal, cast
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
import networkx as nx
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich import box
import asyncio
from scripts.text_web_browser_fixed import web_search, WebContent
from rich.progress import Progress, track

from .supervisor.supervisor_agent import SupervisorAgent, ResearchGoal
from .generation.generation_agent import GenerationAgent, Hypothesis
from .reflection.reflection_agent import ReflectionAgent, Review
from .ranking.ranking_agent import RankingAgent, TournamentMatch
from .evolution.evolution_agent import EvolutionAgent, RefinementResult
from .proximity.proximity_agent import ProximityAgent, HypothesisCluster
from .meta_review.meta_review_agent import MetaReviewAgent, ResearchOverview

# Type definitions
ReviewType = Literal["initial", "full", "deep_verification", "observation", "simulation", "tournament"]

class AICoScientist:
    """Main AI co-scientist system."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        console: Optional[Console] = None
    ):
        """Initialize the AI co-scientist system."""
        self.llm = llm
        self.console = console or Console()
        
        # Initialize agents
        self.supervisor = SupervisorAgent(llm)
        self.generation = GenerationAgent(llm)
        self.reflection = ReflectionAgent(llm)
        self.ranking = RankingAgent(llm)
        self.evolution = EvolutionAgent(llm)
        self.proximity = ProximityAgent(llm)
        self.meta_review = MetaReviewAgent(llm)
        
        # Initialize system graph
        self.system_graph = nx.DiGraph()
        self._initialize_system_graph()
        
        # Print initialization status
        self._print_initialization_status()
        
    def _initialize_system_graph(self) -> None:
        """Initialize the system graph structure."""
        # Add agent nodes
        for agent in [
            self.supervisor,
            self.generation,
            self.reflection,
            self.ranking,
            self.evolution,
            self.proximity,
            self.meta_review
        ]:
            self.system_graph.add_node(
                agent.state.agent_id,
                type=agent.state.agent_type,
                state=agent.state.dict()
            )
            
        # Add relationships
        self.system_graph.add_edge(
            self.supervisor.state.agent_id,
            self.generation.state.agent_id,
            relationship="coordinates"
        )
        self.system_graph.add_edge(
            self.supervisor.state.agent_id,
            self.reflection.state.agent_id,
            relationship="coordinates"
        )
        self.system_graph.add_edge(
            self.supervisor.state.agent_id,
            self.ranking.state.agent_id,
            relationship="coordinates"
        )
        self.system_graph.add_edge(
            self.supervisor.state.agent_id,
            self.evolution.state.agent_id,
            relationship="coordinates"
        )
        self.system_graph.add_edge(
            self.supervisor.state.agent_id,
            self.proximity.state.agent_id,
            relationship="coordinates"
        )
        self.system_graph.add_edge(
            self.supervisor.state.agent_id,
            self.meta_review.state.agent_id,
            relationship="coordinates"
        )
        
        # Add data flow relationships
        self.system_graph.add_edge(
            self.generation.state.agent_id,
            self.reflection.state.agent_id,
            relationship="sends_hypotheses"
        )
        self.system_graph.add_edge(
            self.reflection.state.agent_id,
            self.ranking.state.agent_id,
            relationship="sends_reviews"
        )
        self.system_graph.add_edge(
            self.ranking.state.agent_id,
            self.evolution.state.agent_id,
            relationship="sends_rankings"
        )
        self.system_graph.add_edge(
            self.evolution.state.agent_id,
            self.proximity.state.agent_id,
            relationship="sends_refinements"
        )
        self.system_graph.add_edge(
            self.proximity.state.agent_id,
            self.meta_review.state.agent_id,
            relationship="sends_clusters"
        )
        
    def _print_initialization_status(self) -> None:
        """Print system initialization status."""
        table = Table(title="AI Co-scientist System Status", box=box.ROUNDED)
        table.add_column("Agent", style="cyan")
        table.add_column("Status", style="green")
        table.add_column("State", style="yellow")
        
        for agent in [
            self.supervisor,
            self.generation,
            self.reflection,
            self.ranking,
            self.evolution,
            self.proximity,
            self.meta_review
        ]:
            table.add_row(
                agent.state.agent_type,
                "Initialized",
                f"Memory: {len(agent.state.memory)} items"
            )
            
        self.console.print(table)
        self.console.print(Panel(
            "[bold green]AI Co-scientist System Initialized[/bold green]\n"
            "Ready to begin research exploration",
            title="System Status"
        ))
        
    async def set_research_goal(self, goal: Dict[str, Any]) -> None:
        """Set the research goal for the system."""
        research_goal = ResearchGoal(**goal)
        self.supervisor.state.research_goal = research_goal
        
        self.console.print(Panel(
            f"[bold blue]Research Goal Set[/bold blue]\n"
            f"Goal: {research_goal.goal}\n"
            f"Domain: {research_goal.domain}",
            title="Research Configuration"
        ))
        
    async def run_research_cycle(self) -> Dict[str, Any]:
        """Run a complete research cycle."""
        if not self.supervisor.state.research_goal:
            raise ValueError("Research goal must be set before running a cycle")
            
        self.console.print("\n[bold cyan]Starting Research Cycle[/bold cyan]")
        
        # Gather web knowledge
        self.console.print("\n[bold yellow]1. Gathering Research Knowledge[/bold yellow]")
        config = {
            "domain_name": self.supervisor.state.research_goal.domain,
            "search_depth": 2,
            "max_results": 10,
            "disable_progress": True  # Disable tqdm progress bars
        }
        search_query = f"{self.supervisor.state.research_goal.goal} {self.supervisor.state.research_goal.domain}"
        self.console.print(f"Searching for: {search_query}")
        
        self.console.print("[cyan]Searching web...[/cyan]")
        web_results = await web_search(search_query, config)
            
        # Parse web results
        web_contents = []
        for result in web_results.split("---"):
            if result.strip():
                try:
                    content = eval(result)  # Convert string representation to dict
                    if isinstance(content, dict):
                        web_contents.append(content)
                except:
                    continue
                    
        self.console.print(f"Found {len(web_contents)} relevant sources")
        
        # Create research plan with web knowledge
        self.console.print("\n[bold yellow]2. Creating Research Plan[/bold yellow]")
        plan = await self.supervisor.create_research_plan(context={"web_knowledge": web_contents})
        self.console.print(Panel(
            f"[bold]Research Plan Created[/bold]\n"
            f"Number of tasks: {len(plan.tasks)}\n"
            f"Focus areas: {', '.join(task.get('name', '') for task in plan.tasks)}"
        ))
            
        # Generate hypotheses
        self.console.print("\n[bold yellow]3. Generating Hypotheses[/bold yellow]")
        hypotheses = []
        with Progress() as progress:
            hypothesis_task = progress.add_task("[cyan]Generating hypotheses...", total=plan.tasks[0].get("num_hypotheses", 3))
            
            for i in range(plan.tasks[0].get("num_hypotheses", 3)):
                hypothesis = await self.generation.generate_hypothesis(
                    self.supervisor.state.research_goal.dict(),
                    {
                        "plan": plan.dict(),
                        "web_knowledge": web_contents
                    }
                )
                self.console.print(Panel(
                    f"[bold]Hypothesis {i+1}[/bold]\n"
                    f"Statement: {hypothesis.statement}\n"
                    f"Novelty: {hypothesis.novelty_score:.2f}\n"
                    f"Feasibility: {hypothesis.feasibility_score:.2f}\n"
                    f"Evidence: {', '.join(hypothesis.evidence[:2])}\n"
                    f"Key assumptions: {', '.join(hypothesis.assumptions[:2])}"
                ))
                hypotheses.append(hypothesis)
                progress.update(hypothesis_task, advance=1)
            
        # Review hypotheses
        self.console.print("\n[bold yellow]4. Reviewing Hypotheses[/bold yellow]")
        reviews = []
        review_types: List[ReviewType] = ["initial", "full", "deep_verification"]
        
        with Progress() as progress:
            review_task = progress.add_task(
                "[cyan]Reviewing hypotheses...", 
                total=len(hypotheses) * len(review_types)
            )
            
            for hypothesis in hypotheses:
                self.console.print(f"\n[bold]Reviewing: {hypothesis.statement[:100]}...[/bold]")
                for review_type in review_types:
                    review = await self.reflection.review_hypothesis(
                        hypothesis,
                        review_type,
                        {
                            "plan": plan.dict(),
                            "web_knowledge": web_contents
                        }
                    )
                    self.console.print(
                        f"- {review_type.replace('_', ' ').title()} Review:\n"
                        f"  Score: {review.score:.2f}\n"
                        f"  Confidence: {review.confidence:.2f}\n"
                        f"  Key points: {review.key_points[:2] if review.key_points else 'None'}"
                    )
                    reviews.append(review)
                    progress.update(review_task, advance=1)
                
        # Conduct tournament
        self.console.print("\n[bold yellow]5. Conducting Tournament[/bold yellow]")
        matches = []
        with Progress() as progress:
            match_task = progress.add_task(
                "[cyan]Conducting matches...", 
                total=len(hypotheses) * (len(hypotheses) - 1) // 2
            )
            
            for i, h1 in enumerate(hypotheses):
                for h2 in hypotheses[i+1:]:
                    match = await self.ranking.conduct_match(
                        h1, h2,
                        {
                            "reviews": [r.dict() for r in reviews],
                            "web_knowledge": web_contents
                        }
                    )
                    self.console.print(
                        f"\nMatch: {h1.id} vs {h2.id}\n"
                        f"Winner: {match.winner}\n"
                        f"Scores: {match.score_a:.2f} vs {match.score_b:.2f}"
                    )
                    matches.append(match)
                    progress.update(match_task, advance=1)
                
        # Get rankings
        rankings = self.ranking.get_rankings()
        self.console.print("\n[bold green]Current Rankings:[/bold green]")
        for rank, (h_id, score) in enumerate(rankings, 1):
            hypothesis = next(h for h in hypotheses if h.id == h_id)
            self.console.print(f"{rank}. {hypothesis.statement[:100]}... (Score: {score:.2f})")
                
        # Refine top hypotheses
        self.console.print("\n[bold yellow]6. Refining Top Hypotheses[/bold yellow]")
        refinements = []
        with Progress() as progress:
            refine_task = progress.add_task("[cyan]Refining hypotheses...", total=3)
            
            for h_id, _ in rankings[:3]:  # Refine top 3
                hypothesis = next(h for h in hypotheses if h.id == h_id)
                refinement = await self.evolution.refine_hypothesis(
                    hypothesis,
                    strategy_id="literature_enhancement",
                    context={"web_knowledge": web_contents}
                )
                self.console.print(Panel(
                    f"[bold]Refinement for {hypothesis.id}[/bold]\n"
                    f"Original: {hypothesis.statement}\n"
                    f"Refined: {refinement.refined_hypothesis.statement}\n"
                    f"Improvements:\n" + "\n".join(f"- {imp}" for imp in refinement.improvements)
                ))
                refinements.append(refinement)
                progress.update(refine_task, advance=1)
            
        # Cluster hypotheses
        self.console.print("\n[bold yellow]7. Clustering Hypotheses[/bold yellow]")
        all_hypotheses = hypotheses + [r.refined_hypothesis for r in refinements]
        clusters = await self.proximity.cluster_hypotheses(all_hypotheses)
        
        self.console.print(Panel(
            f"[bold]Clustering Results[/bold]\n"
            f"Number of clusters: {len(clusters)}\n\n" +
            "\n".join(
                f"Cluster {i+1}: {cluster.theme}\n"
                f"Size: {len(cluster.hypotheses)} hypotheses\n"
                f"Key features: {', '.join(cluster.key_features)}\n"
                for i, cluster in enumerate(clusters)
            )
        ))
        
        # Generate overview
        self.console.print("\n[bold yellow]8. Generating Research Overview[/bold yellow]")
        overview = await self.meta_review.generate_overview(
            all_hypotheses,
            reviews,
            {
                "plan": plan.dict(),
                "rankings": rankings,
                "clusters": [c.dict() for c in clusters],
                "web_knowledge": web_contents
            }
        )
        
        self.console.print(Panel(
            "[bold]Research Cycle Summary[/bold]\n\n"
            "[bold cyan]Key Findings:[/bold cyan]\n" +
            "\n".join(f"- {finding}" for finding in overview.key_findings) +
            "\n\n[bold cyan]Promising Directions:[/bold cyan]\n" +
            "\n".join(f"- {direction['title']}" for direction in overview.promising_directions) +
            "\n\n[bold cyan]Next Steps:[/bold cyan]\n" +
            "\n".join(f"- {step}" for step in overview.next_steps)
        ))
            
        return {
            "hypotheses": [h.dict() for h in hypotheses],
            "reviews": [r.dict() for r in reviews],
            "rankings": rankings,
            "refinements": [r.dict() for r in refinements],
            "clusters": [c.dict() for c in clusters],
            "overview": overview.dict(),
            "web_knowledge": web_contents
        }
        
    def get_system_status(self) -> Dict[str, Any]:
        """Get current system status."""
        return {
            "research_goal": self.supervisor.state.research_goal.dict() if self.supervisor.state.research_goal else None,
            "generation_stats": {
                "total_hypotheses": len(self.generation.state.hypotheses),
                "generation_history": len(self.generation.state.generation_history)
            },
            "reflection_stats": {
                "total_reviews": sum(len(reviews) for reviews in self.reflection.state.reviews.values()),
                "review_history": len(self.reflection.state.review_history)
            },
            "ranking_stats": {
                "total_matches": len(self.ranking.state.matches),
                "match_history": len(self.ranking.state.match_history)
            },
            "evolution_stats": {
                "total_refinements": len(self.evolution.state.refinement_history),
                "active_strategies": len(self.evolution.state.strategies)
            },
            "proximity_stats": {
                "total_clusters": len(self.proximity.state.clusters),
                "similarity_cache_size": len(self.proximity.state.similarity_cache)
            },
            "meta_review_stats": {
                "total_overviews": len(self.meta_review.state.research_overviews),
                "synthesis_history": len(self.meta_review.state.synthesis_history)
            }
        }
        
    def print_system_status(self) -> None:
        """Print current system status."""
        status = self.get_system_status()
        
        table = Table(title="System Status Overview", box=box.ROUNDED)
        table.add_column("Component", style="cyan")
        table.add_column("Metric", style="yellow")
        table.add_column("Value", style="green")
        
        if status["research_goal"]:
            table.add_row(
                "Research Goal",
                "Domain",
                status["research_goal"]["domain"]
            )
            
        for agent, stats in [
            ("Generation", status["generation_stats"]),
            ("Reflection", status["reflection_stats"]),
            ("Ranking", status["ranking_stats"]),
            ("Evolution", status["evolution_stats"]),
            ("Proximity", status["proximity_stats"]),
            ("Meta-review", status["meta_review_stats"])
        ]:
            for metric, value in stats.items():
                table.add_row(agent, metric, str(value))
                
        self.console.print(table)
        
    async def run(self, research_goal: Dict[str, Any]) -> Dict[str, Any]:
        """Run the complete research process."""
        # Set research goal
        await self.set_research_goal(research_goal)
        
        # Run research cycles
        results = []
        try:
            while True:
                cycle_result = await self.run_research_cycle()
                results.append(cycle_result)
                
                # Check if we should continue
                overview = cycle_result["overview"]
                if len(overview["key_findings"]) >= 10:  # Example stopping condition
                    break
                    
        except Exception as e:
            self.console.print(f"[bold red]Error in research cycle:[/bold red] {str(e)}")
            
        finally:
            # Print final status
            self.print_system_status()
            
        return {
            "research_goal": research_goal,
            "cycles": results,
            "final_status": self.get_system_status()
        } 
```

.\agents\co_scientist\evolution\evolution_agent.py
```python
"""Evolution agent for hypothesis refinement and improvement."""

from typing import Any, Dict, List, Optional, Literal
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser

from ..base_agent import BaseAgent, AgentState
from ..generation.generation_agent import Hypothesis

class RefinementStrategy(BaseModel):
    """Strategy for hypothesis refinement."""
    strategy_id: str = Field(description="Unique identifier for this strategy")
    name: str = Field(description="Name of the strategy")
    description: str = Field(description="Description of how the strategy works")
    target_aspects: List[str] = Field(description="Aspects of hypotheses this strategy targets")
    success_criteria: List[str] = Field(description="Criteria for successful application")

class RefinementResult(BaseModel):
    """Result of applying a refinement strategy."""
    result_id: str = Field(description="Unique identifier for this result")
    original_hypothesis: str = Field(description="ID of original hypothesis")
    refined_hypothesis: Hypothesis = Field(description="The refined hypothesis")
    strategy_used: str = Field(description="ID of strategy used")
    improvements: List[str] = Field(description="List of improvements made")
    rationale: str = Field(description="Reasoning behind refinements")
    metrics: Dict[str, float] = Field(description="Improvement metrics")

class EvolutionState(AgentState):
    """Evolution agent state."""
    strategies: Dict[str, RefinementStrategy] = Field(default_factory=dict)
    refinement_history: List[RefinementResult] = Field(default_factory=list)
    evolution_metrics: Dict[str, Any] = Field(default_factory=dict)
    current_strategy: Optional[str] = None

class EvolutionAgent(BaseAgent):
    """Agent responsible for refining and improving hypotheses."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "evolution",
        system_prompt: Optional[str] = None
    ):
        """Initialize the evolution agent."""
        if system_prompt is None:
            system_prompt = """You are the evolution agent responsible for refining research hypotheses.
Your role is to:
1. Apply various refinement strategies
2. Improve hypothesis quality and testability
3. Address identified weaknesses
4. Enhance scientific rigor
5. Maintain hypothesis novelty
6. Track improvement metrics

Refinement Strategies:
- Enhancement through Literature
- Coherence Improvement
- Feasibility Enhancement
- Inspiration from Success
- Combination of Strengths
- Simplification
- Out-of-box Thinking

Follow these guidelines:
- Preserve valuable aspects
- Address specific weaknesses
- Maintain scientific validity
- Track all modifications
- Justify refinements
- Consider multiple approaches
- Balance improvement goals"""

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="evolution",
            system_prompt=system_prompt,
            output_parser=PydanticOutputParser(pydantic_object=RefinementResult)
        )
        
        # Initialize evolution-specific state
        self.state = EvolutionState(
            agent_id=agent_id,
            agent_type="evolution",
            strategies={},
            refinement_history=[],
            evolution_metrics={},
            current_strategy=None
        )
        
        # Initialize default strategies
        self._initialize_default_strategies()
        
    def _initialize_default_strategies(self) -> None:
        """Initialize default refinement strategies."""
        default_strategies = [
            RefinementStrategy(
                strategy_id="literature_enhancement",
                name="Enhancement through Literature",
                description="Strengthen hypothesis by incorporating additional literature evidence",
                target_aspects=["evidence", "grounding", "support"],
                success_criteria=["increased_citations", "stronger_evidence", "better_grounding"]
            ),
            RefinementStrategy(
                strategy_id="coherence_improvement",
                name="Coherence Improvement",
                description="Enhance logical flow and internal consistency",
                target_aspects=["logic", "consistency", "clarity"],
                success_criteria=["reduced_contradictions", "clearer_logic", "better_structure"]
            ),
            RefinementStrategy(
                strategy_id="feasibility_enhancement",
                name="Feasibility Enhancement",
                description="Improve practical testability and implementation",
                target_aspects=["testability", "practicality", "resources"],
                success_criteria=["more_testable", "resource_efficient", "practical_methods"]
            ),
            RefinementStrategy(
                strategy_id="success_inspiration",
                name="Inspiration from Success",
                description="Learn from successful hypotheses and adapt their strengths",
                target_aspects=["methodology", "structure", "approach"],
                success_criteria=["adapted_strengths", "novel_combination", "improved_approach"]
            ),
            RefinementStrategy(
                strategy_id="strength_combination",
                name="Combination of Strengths",
                description="Combine strong elements from multiple hypotheses",
                target_aspects=["integration", "synthesis", "complementarity"],
                success_criteria=["successful_integration", "preserved_strengths", "added_value"]
            )
        ]
        
        for strategy in default_strategies:
            self.register_strategy(strategy)
            
    def register_strategy(self, strategy: RefinementStrategy) -> None:
        """Register a new refinement strategy."""
        self.state.strategies[strategy.strategy_id] = strategy
        
    async def refine_hypothesis(
        self,
        hypothesis: Hypothesis,
        strategy_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> RefinementResult:
        """Refine a hypothesis using specified strategy."""
        # Use specified or current strategy
        strategy_id = strategy_id or self.state.current_strategy
        if not strategy_id or strategy_id not in self.state.strategies:
            raise ValueError("Valid strategy_id required")
            
        strategy = self.state.strategies[strategy_id]
        context = context or {}
        
        # Generate refinement using LLM
        result = await self.arun({
            "hypothesis": hypothesis.dict(),
            "strategy": strategy.dict(),
            "context": context,
            "previous_refinements": [
                r.dict() for r in self.state.refinement_history 
                if r.original_hypothesis == hypothesis.id
            ]
        })
        
        # Create refinement result
        refinement = RefinementResult(**result)
        
        # Update state
        self.state.refinement_history.append(refinement)
        self._update_evolution_metrics(refinement)
        
        return refinement
        
    def set_strategy(self, strategy_id: str) -> None:
        """Set the current refinement strategy."""
        if strategy_id not in self.state.strategies:
            raise ValueError(f"Unknown strategy: {strategy_id}")
        self.state.current_strategy = strategy_id
        
    def get_strategies(self) -> List[RefinementStrategy]:
        """Get all available refinement strategies."""
        return list(self.state.strategies.values())
        
    def get_refinement_history(
        self,
        hypothesis_id: Optional[str] = None,
        strategy_id: Optional[str] = None
    ) -> List[RefinementResult]:
        """Get refinement history, optionally filtered."""
        history = self.state.refinement_history
        
        if hypothesis_id:
            history = [r for r in history if r.original_hypothesis == hypothesis_id]
            
        if strategy_id:
            history = [r for r in history if r.strategy_used == strategy_id]
            
        return history
        
    def _update_evolution_metrics(self, result: RefinementResult) -> None:
        """Update evolution metrics with new refinement result."""
        metrics = self.state.evolution_metrics
        
        # Update strategy usage
        strategy_usage = metrics.get("strategy_usage", {})
        strategy_id = result.strategy_used
        strategy_usage[strategy_id] = strategy_usage.get(strategy_id, 0) + 1
        metrics["strategy_usage"] = strategy_usage
        
        # Update improvement tracking
        improvements = metrics.get("improvements", {})
        for improvement in result.improvements:
            improvements[improvement] = improvements.get(improvement, 0) + 1
        metrics["improvements"] = improvements
        
        # Update metric averages
        for metric, value in result.metrics.items():
            if "metric_averages" not in metrics:
                metrics["metric_averages"] = {}
            if metric not in metrics["metric_averages"]:
                metrics["metric_averages"][metric] = {"sum": 0.0, "count": 0}
            metrics["metric_averages"][metric]["sum"] += value
            metrics["metric_averages"][metric]["count"] += 1
            
        self.state.evolution_metrics = metrics
        
    def analyze_evolution_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in hypothesis evolution."""
        metrics = self.state.evolution_metrics
        
        # Calculate metric averages
        metric_averages = {}
        for metric, data in metrics.get("metric_averages", {}).items():
            if data["count"] > 0:
                metric_averages[metric] = data["sum"] / data["count"]
                
        # Get most successful strategies
        strategy_success = {}
        for result in self.state.refinement_history:
            strategy_id = result.strategy_used
            if strategy_id not in strategy_success:
                strategy_success[strategy_id] = {
                    "total_improvements": 0,
                    "total_metrics": 0.0,
                    "count": 0
                }
            stats = strategy_success[strategy_id]
            stats["total_improvements"] += len(result.improvements)
            stats["total_metrics"] += sum(result.metrics.values())
            stats["count"] += 1
            
        top_strategies = sorted(
            [
                (s_id, stats)
                for s_id, stats in strategy_success.items()
                if stats["count"] >= 3  # Minimum applications threshold
            ],
            key=lambda x: (x[1]["total_improvements"] / x[1]["count"]),
            reverse=True
        )[:5]
        
        return {
            "total_refinements": len(self.state.refinement_history),
            "strategy_usage": metrics.get("strategy_usage", {}),
            "metric_averages": metric_averages,
            "top_strategies": [
                {
                    "strategy_id": s_id,
                    "name": self.state.strategies[s_id].name,
                    "avg_improvements": stats["total_improvements"] / stats["count"],
                    "avg_metrics": stats["total_metrics"] / stats["count"],
                    "times_used": stats["count"]
                }
                for s_id, stats in top_strategies
            ],
            "common_improvements": sorted(
                metrics.get("improvements", {}).items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
        } 
```

.\agents\co_scientist\generation\generation_agent.py
```python
"""Generation agent for hypothesis generation."""

from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableSerializable

from ..base_agent import BaseAgent, AgentState

class Hypothesis(BaseModel):
    """Model for research hypotheses."""
    id: str = Field(description="Unique identifier for this hypothesis")
    statement: str = Field(description="Clear hypothesis statement")
    rationale: str = Field(description="Detailed reasoning behind the hypothesis")
    evidence: List[str] = Field(description="Supporting evidence points")
    novelty_score: float = Field(description="Novelty score (0-1)", ge=0.0, le=1.0)
    feasibility_score: float = Field(description="Feasibility score (0-1)", ge=0.0, le=1.0)
    assumptions: List[str] = Field(description="Key assumptions")
    testability: Dict[str, Any] = Field(description="Testability information")
    references: List[str] = Field(description="Literature references")

class GenerationState(AgentState):
    """Generation agent state."""
    hypotheses: List[Hypothesis] = Field(default_factory=list)
    literature_context: Dict[str, Any] = Field(default_factory=dict)
    generation_history: List[Dict[str, Any]] = Field(default_factory=list)
    current_strategy: Optional[str] = None

class GenerationAgent(BaseAgent):
    """Agent responsible for generating research hypotheses."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "generation",
        system_prompt: Optional[str] = None
    ):
        """Initialize the generation agent."""
        # Create output parser
        parser = PydanticOutputParser(pydantic_object=Hypothesis)
        format_instructions = parser.get_format_instructions()
        
        if system_prompt is None:
            system_prompt = """You are the generation agent responsible for creating novel research hypotheses.
Your role is to:
1. Generate original and testable research hypotheses
2. Ground hypotheses in existing scientific literature
3. Ensure hypotheses are aligned with research goals
4. Provide clear rationale and evidence
5. Assess novelty and feasibility
6. Break down complex hypotheses into testable components

Follow these guidelines:
- Focus on generating truly novel ideas
- Ensure all hypotheses are scientifically sound
- Provide detailed supporting evidence
- Consider multiple research directions
- Break down complex ideas into testable parts
- Maintain clear documentation of your reasoning
- Assess practical feasibility of testing

{format_instructions}"""

        # Create prompt template
        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt),
            HumanMessagePromptTemplate.from_template("""Please generate a research hypothesis based on the following:

Research Goal: {goal}
Domain: {domain}
Web Knowledge: {web_knowledge}
Previous Hypotheses: {previous_hypotheses}
Current Strategy: {strategy}

Use the web knowledge to inform your hypothesis generation. Each web source contains:
- title: Title of the source
- url: URL of the source
- content: Main content
- summary: Brief summary
- metadata: Additional metadata

Your response MUST include ALL of the following fields in the exact format specified:
1. id: A unique identifier string for the hypothesis
2. statement: A clear hypothesis statement string
3. rationale: Detailed reasoning string behind the hypothesis
4. evidence: Array of strings, each string being a complete evidence point (e.g., ["Evidence 1: description", "Evidence 2: description"])
5. novelty_score: A number between 0 and 1 (e.g., 0.85)
6. feasibility_score: A number between 0 and 1 (e.g., 0.75)
7. assumptions: Array of strings, each string being a complete assumption (e.g., ["Assumption 1: description", "Assumption 2: description"])
8. testability: Object with these exact fields:
   - methods: Array of strings
   - required_resources: Array of strings
   - estimated_duration: String
9. references: Array of strings, each string being a complete reference (e.g., ["Author et al. (2023) Title, Journal", "Author et al. (2022) Title, Journal"])

Example evidence format:
"Evidence 1: Studies have shown that drug X affects pathway Y in AML cells (Author et al., 2023)"

Example assumption format:
"Assumption 1: The identified molecular pathways are conserved across different AML subtypes"

Generate a single, well-formed hypothesis that follows the required format exactly.
Focus on drug repurposing opportunities for treating the specified condition.
Ensure the hypothesis is novel, testable, and grounded in the available literature.
Do not omit any required fields or deviate from the specified formats.""")
        ])

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="generation",
            system_prompt=system_prompt,
            output_parser=parser
        )
        
        # Initialize generation-specific state
        self.state = GenerationState(
            agent_id=agent_id,
            agent_type="generation",
            hypotheses=[],
            literature_context={},
            generation_history=[],
            current_strategy=None
        )
        
        # Create chain with format instructions
        self.chain = prompt | self.llm | parser
        
    async def generate_hypothesis(self, research_goal: Dict[str, Any], context: Dict[str, Any]) -> Hypothesis:
        """Generate a new research hypothesis."""
        # Update context
        self.state.literature_context.update(context.get("literature", {}))
        
        # Format web knowledge for prompt
        web_knowledge_summary = []
        for source in context.get("web_knowledge", []):
            if isinstance(source, dict):
                summary = {
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "summary": source.get("summary", ""),
                    "key_points": source.get("content", "")[:500] + "..."  # First 500 chars
                }
                web_knowledge_summary.append(summary)
        
        # Generate hypothesis using LLM
        result = await self.chain.ainvoke({
            "format_instructions": PydanticOutputParser(pydantic_object=Hypothesis).get_format_instructions(),
            "goal": research_goal.get("goal", ""),
            "domain": research_goal.get("domain", ""),
            "web_knowledge": web_knowledge_summary if web_knowledge_summary else "No web knowledge available",
            "previous_hypotheses": [h.dict() for h in self.state.hypotheses],
            "strategy": self.state.current_strategy or "Generate novel hypotheses based on available literature"
        })
        
        # Create hypothesis object
        if isinstance(result, dict):
            hypothesis = Hypothesis(**result)
        else:
            # If result is already a Hypothesis (from output parser), use it directly
            hypothesis = result
        
        # Update state
        self.state.hypotheses.append(hypothesis)
        self.state.generation_history.append({
            "hypothesis_id": hypothesis.id,
            "goal": research_goal,
            "context": context,
            "strategy": self.state.current_strategy,
            "timestamp": "TODO: Add timestamp"
        })
        
        return hypothesis
        
    def set_generation_strategy(self, strategy: str) -> None:
        """Set the current hypothesis generation strategy."""
        self.state.current_strategy = strategy
        self.update_memory("current_strategy", strategy)
        
    def get_hypotheses(self, filters: Optional[Dict[str, Any]] = None) -> List[Hypothesis]:
        """Get generated hypotheses, optionally filtered."""
        if not filters:
            return self.state.hypotheses
            
        filtered = self.state.hypotheses
        
        # Apply filters
        if "min_novelty" in filters:
            filtered = [h for h in filtered if h.novelty_score >= filters["min_novelty"]]
            
        if "min_feasibility" in filters:
            filtered = [h for h in filtered if h.feasibility_score >= filters["min_feasibility"]]
            
        if "keywords" in filters:
            keywords = filters["keywords"]
            filtered = [
                h for h in filtered 
                if any(k.lower() in h.statement.lower() for k in keywords)
            ]
            
        return filtered
        
    def get_generation_history(self) -> List[Dict[str, Any]]:
        """Get the history of hypothesis generation."""
        return self.state.generation_history
        
    def analyze_hypothesis_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in generated hypotheses."""
        if not self.state.hypotheses:
            return {}
            
        # Calculate basic statistics
        avg_novelty = sum(h.novelty_score for h in self.state.hypotheses) / len(self.state.hypotheses)
        avg_feasibility = sum(h.feasibility_score for h in self.state.hypotheses) / len(self.state.hypotheses)
        
        # Collect common themes
        all_assumptions = [a for h in self.state.hypotheses for a in h.assumptions]
        assumption_counts = {}
        for assumption in all_assumptions:
            assumption_counts[assumption] = assumption_counts.get(assumption, 0) + 1
            
        return {
            "total_hypotheses": len(self.state.hypotheses),
            "average_novelty": avg_novelty,
            "average_feasibility": avg_feasibility,
            "common_assumptions": sorted(
                assumption_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5],
            "strategies_used": list(set(h["strategy"] for h in self.state.generation_history if h["strategy"]))
        } 
```

.\agents\co_scientist\meta_review\meta_review_agent.py
```python
"""Meta-review agent for system-wide analysis and synthesis."""

from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser

from ..base_agent import BaseAgent, AgentState
from ..generation.generation_agent import Hypothesis
from ..reflection.reflection_agent import Review

class ResearchOverview(BaseModel):
    """Model for research cycle overview."""
    key_findings: List[str] = Field(description="Key findings from the research cycle")
    promising_directions: List[Dict[str, str]] = Field(description="Promising research directions")
    next_steps: List[str] = Field(default_factory=list, description="Recommended next steps")
    insights: List[str] = Field(description="Important insights gained")
    challenges: List[str] = Field(description="Identified challenges")
    recommendations: List[str] = Field(description="Recommendations for future work")

class MetaReviewState(AgentState):
    """Meta-review agent state."""
    research_overviews: List[ResearchOverview] = Field(default_factory=list)
    synthesis_history: List[Dict[str, Any]] = Field(default_factory=list)
    meta_review_metrics: Dict[str, Any] = Field(default_factory=dict)
    current_overview: Optional[ResearchOverview] = None

class MetaReviewAgent(BaseAgent):
    """Agent responsible for system-wide analysis and synthesis."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "meta_review",
        system_prompt: Optional[str] = None
    ):
        """Initialize the meta-review agent."""
        if system_prompt is None:
            system_prompt = """You are the meta-review agent responsible for system-wide analysis.
Your role is to:
1. Synthesize insights across the system
2. Identify emerging patterns and themes
3. Track research progress and quality
4. Provide strategic recommendations
5. Generate research overviews
6. Guide system improvement

Follow these guidelines:
- Analyze patterns across agents
- Identify emerging themes
- Track quality metrics
- Provide actionable insights
- Generate clear summaries
- Guide strategic decisions
- Maintain research focus"""

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="meta_review",
            system_prompt=system_prompt,
            output_parser=PydanticOutputParser(pydantic_object=ResearchOverview)
        )
        
        # Initialize meta-review-specific state
        self.state = MetaReviewState(
            agent_id=agent_id,
            agent_type="meta_review",
            research_overviews=[],
            synthesis_history=[],
            meta_review_metrics={},
            current_overview=None
        )
        
    async def generate_overview(
        self,
        hypotheses: List[Hypothesis],
        reviews: List[Review],
        context: Dict[str, Any]
    ) -> ResearchOverview:
        """Generate a research overview."""
        # Generate overview using LLM
        result = await self.arun({
            "hypotheses": [h.dict() for h in hypotheses],
            "reviews": [r.dict() for r in reviews],
            "context": context,
            "previous_overviews": [o.dict() for o in self.state.research_overviews]
        })
        
        # Create overview object
        overview = ResearchOverview(**result)
        
        # Update state
        self.state.research_overviews.append(overview)
        self.state.current_overview = overview
        
        self.state.synthesis_history.append({
            "timestamp": "TODO: Add timestamp",
            "num_hypotheses": len(hypotheses),
            "num_reviews": len(reviews),
            "key_findings": len(overview.key_findings),
            "emerging_themes": len(overview.promising_directions)
        })
        
        # Update metrics
        self._update_meta_review_metrics(overview)
        
        return overview
        
    def _update_meta_review_metrics(self, overview: ResearchOverview) -> None:
        """Update meta-review metrics with new overview."""
        metrics = self.state.meta_review_metrics
        
        # Track theme evolution
        theme_counts = metrics.get("theme_counts", {})
        for theme in overview.promising_directions:
            theme_name = theme["theme"]
            theme_counts[theme_name] = theme_counts.get(theme_name, 0) + 1
        metrics["theme_counts"] = theme_counts
        
        # Track challenge types
        challenge_types = metrics.get("challenge_types", {})
        for challenge in overview.challenges:
            challenge_type = challenge
            challenge_types[challenge_type] = challenge_types.get(challenge_type, 0) + 1
        metrics["challenge_types"] = challenge_types
        
        # Track recommendation categories
        recommendation_categories = metrics.get("recommendation_categories", {})
        for rec in overview.recommendations:
            category = rec
            recommendation_categories[category] = recommendation_categories.get(category, 0) + 1
        metrics["recommendation_categories"] = recommendation_categories
        
        self.state.meta_review_metrics = metrics
        
    def get_research_roadmap(self) -> Dict[str, Any]:
        """Generate a research roadmap based on current overview."""
        if not self.state.current_overview:
            return {}
            
        overview = self.state.current_overview
        
        # Organize recommendations by priority
        prioritized_recommendations = {}
        for rec in overview.next_steps:
            priority = rec.split(":")[0]
            if priority not in prioritized_recommendations:
                prioritized_recommendations[priority] = []
            prioritized_recommendations[priority].append(rec)
            
        # Organize directions by theme
        themed_directions = {}
        for direction in overview.promising_directions:
            theme = direction["theme"]
            if theme not in themed_directions:
                themed_directions[theme] = []
            themed_directions[theme].append(direction)
            
        return {
            "current_status": {
                "key_findings": overview.key_findings,
                "main_themes": [t["theme"] for t in overview.promising_directions],
                "critical_challenges": [c for c in overview.challenges if c.startswith("high")]
            },
            "next_steps": {
                "immediate_priorities": prioritized_recommendations.get("high", []),
                "medium_term": prioritized_recommendations.get("medium", []),
                "long_term": prioritized_recommendations.get("low", [])
            },
            "research_directions": {
                theme: [d["theme"] for d in directions]
                for theme, directions in themed_directions.items()
            }
        }
        
    def get_quality_metrics(self) -> Dict[str, Any]:
        """Get research quality metrics."""
        if not self.state.research_overviews:
            return {}
            
        # Track metric evolution
        metric_evolution = {
            "findings_per_overview": [],
            "themes_per_overview": [],
            "challenges_per_overview": [],
            "recommendations_per_overview": []
        }
        
        for overview in self.state.research_overviews:
            metric_evolution["findings_per_overview"].append(len(overview.key_findings))
            metric_evolution["themes_per_overview"].append(len(overview.promising_directions))
            metric_evolution["challenges_per_overview"].append(len(overview.challenges))
            metric_evolution["recommendations_per_overview"].append(len(overview.recommendations))
            
        return {
            "total_overviews": len(self.state.research_overviews),
            "metric_evolution": metric_evolution,
            "theme_distribution": self.state.meta_review_metrics.get("theme_counts", {}),
            "challenge_distribution": self.state.meta_review_metrics.get("challenge_types", {}),
            "recommendation_distribution": self.state.meta_review_metrics.get("recommendation_categories", {})
        }
        
    def analyze_meta_review_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in meta-review insights."""
        if not self.state.research_overviews:
            return {}
            
        # Analyze theme evolution
        theme_evolution = {}
        for i, overview in enumerate(self.state.research_overviews):
            for theme in overview.promising_directions:
                theme_name = theme["theme"]
                if theme_name not in theme_evolution:
                    theme_evolution[theme_name] = {"first_seen": i, "occurrences": 0}
                theme_evolution[theme_name]["occurrences"] += 1
                
        # Find persistent themes
        persistent_themes = [
            name for name, stats in theme_evolution.items()
            if stats["occurrences"] >= len(self.state.research_overviews) * 0.7  # Present in 70% of overviews
        ]
        
        # Analyze challenge patterns
        challenge_patterns = {}
        for overview in self.state.research_overviews:
            for challenge in overview.challenges:
                challenge_type = challenge
                if challenge_type not in challenge_patterns:
                    challenge_patterns[challenge_type] = {
                        "count": 0,
                        "severity_distribution": {"low": 0, "medium": 0, "high": 0}
                    }
                stats = challenge_patterns[challenge_type]
                stats["count"] += 1
                severity = "high" if challenge.startswith("high") else "medium"
                stats["severity_distribution"][severity] += 1
                
        return {
            "theme_evolution": {
                name: stats for name, stats in theme_evolution.items()
                if stats["occurrences"] > 1  # Show only recurring themes
            },
            "persistent_themes": persistent_themes,
            "challenge_patterns": challenge_patterns,
            "synthesis_metrics": {
                "total_findings": sum(len(o.key_findings) for o in self.state.research_overviews),
                "total_themes": len(theme_evolution),
                "total_challenges": sum(stats["count"] for stats in challenge_patterns.values()),
                "synthesis_history": self.state.synthesis_history
            }
        } 
```

.\agents\co_scientist\proximity\proximity_agent.py
```python
"""Proximity agent for hypothesis clustering and similarity analysis."""

from typing import Any, Dict, List, Optional, Tuple
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity

from ..base_agent import BaseAgent, AgentState
from ..generation.generation_agent import Hypothesis

class SimilarityScore(BaseModel):
    """Similarity assessment between two hypotheses."""
    score: float = Field(description="Overall similarity score (0-1)", ge=0.0, le=1.0)
    aspects: Dict[str, float] = Field(description="Similarity scores by aspect")
    shared_elements: List[str] = Field(description="Common elements between hypotheses")
    key_differences: List[str] = Field(description="Important distinguishing features")

class HypothesisCluster(BaseModel):
    """Cluster of related hypotheses."""
    cluster_id: str = Field(description="Unique identifier for this cluster")
    hypotheses: List[str] = Field(description="IDs of hypotheses in this cluster")
    centroid: Optional[str] = Field(description="ID of central/representative hypothesis")
    theme: str = Field(description="Main theme of the cluster")
    key_features: List[str] = Field(description="Defining features of the cluster")
    intra_cluster_similarity: float = Field(description="Average similarity within cluster", ge=0.0, le=1.0)

class ProximityState(AgentState):
    """Proximity agent state."""
    similarity_cache: Dict[str, SimilarityScore] = Field(default_factory=dict)
    clusters: Dict[str, HypothesisCluster] = Field(default_factory=dict)
    embeddings: Dict[str, List[float]] = Field(default_factory=dict)
    proximity_metrics: Dict[str, Any] = Field(default_factory=dict)

class ProximityAgent(BaseAgent):
    """Agent responsible for hypothesis clustering and similarity analysis."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "proximity",
        system_prompt: Optional[str] = None,
        embedding_dim: int = 768,
        min_cluster_size: int = 3,
        similarity_threshold: float = 0.7
    ):
        """Initialize the proximity agent."""
        if system_prompt is None:
            system_prompt = """You are the proximity agent responsible for analyzing hypothesis relationships.
Your role is to:
1. Assess hypothesis similarities
2. Identify related hypotheses
3. Form meaningful clusters
4. Find representative examples
5. Track relationship patterns
6. Guide hypothesis organization

Follow these guidelines:
- Consider multiple similarity aspects
- Identify meaningful relationships
- Form coherent clusters
- Find representative examples
- Track emerging patterns
- Maintain clear organization
- Support hypothesis navigation"""

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="proximity",
            system_prompt=system_prompt,
            output_parser=PydanticOutputParser(pydantic_object=SimilarityScore)
        )
        
        # Initialize proximity-specific state
        self.state = ProximityState(
            agent_id=agent_id,
            agent_type="proximity",
            similarity_cache={},
            clusters={},
            embeddings={},
            proximity_metrics={}
        )
        
        self.embedding_dim = embedding_dim
        self.min_cluster_size = min_cluster_size
        self.similarity_threshold = similarity_threshold
        
    async def compute_similarity(
        self,
        hypothesis_a: Hypothesis,
        hypothesis_b: Hypothesis,
        aspects: Optional[List[str]] = None
    ) -> SimilarityScore:
        """Compute similarity between two hypotheses."""
        # Check cache
        cache_key = f"{hypothesis_a.id}_{hypothesis_b.id}"
        if cache_key in self.state.similarity_cache:
            return self.state.similarity_cache[cache_key]
            
        # Generate similarity assessment using LLM
        result = await self.arun({
            "hypothesis_a": hypothesis_a.dict(),
            "hypothesis_b": hypothesis_b.dict(),
            "aspects": aspects or ["methodology", "concepts", "evidence", "implications"],
            "previous_scores": [
                score.dict() for score in self.state.similarity_cache.values()
                if hypothesis_a.id in [score.hypothesis_a, score.hypothesis_b]
                or hypothesis_b.id in [score.hypothesis_a, score.hypothesis_b]
            ]
        })
        
        # Create similarity score
        similarity = SimilarityScore(**result)
        
        # Update cache
        self.state.similarity_cache[cache_key] = similarity
        self.state.similarity_cache[f"{hypothesis_b.id}_{hypothesis_a.id}"] = similarity
        
        return similarity
        
    async def update_embeddings(self, hypotheses: List[Hypothesis]) -> None:
        """Update hypothesis embeddings."""
        for hypothesis in hypotheses:
            if hypothesis.id not in self.state.embeddings:
                # This would use a real embedding model in practice
                # For now, we'll generate random embeddings
                self.state.embeddings[hypothesis.id] = list(np.random.rand(self.embedding_dim))
                
    async def cluster_hypotheses(self, hypotheses: List[Hypothesis]) -> List[HypothesisCluster]:
        """Cluster hypotheses based on similarity."""
        # Ensure we have embeddings
        await self.update_embeddings(hypotheses)
        
        # Create embedding matrix
        embedding_matrix = np.array([
            self.state.embeddings[h.id] for h in hypotheses
        ])
        
        # Compute similarity matrix
        similarity_matrix = cosine_similarity(embedding_matrix)
        
        # Perform clustering
        clustering = DBSCAN(
            eps=1 - self.similarity_threshold,
            min_samples=self.min_cluster_size,
            metric="precomputed"
        ).fit(1 - similarity_matrix)
        
        # Create clusters
        clusters = {}
        for i, label in enumerate(clustering.labels_):
            if label == -1:  # Noise points
                continue
                
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(hypotheses[i].id)
            
        # Create cluster objects
        cluster_objects = []
        for label, hypothesis_ids in clusters.items():
            # Find centroid (most central hypothesis)
            centroid = None
            if hypothesis_ids:
                # Use hypothesis with highest average similarity to others
                similarities = []
                for h_id in hypothesis_ids:
                    avg_sim = np.mean([
                        self.get_cached_similarity(h_id, other_id).score
                        for other_id in hypothesis_ids
                        if other_id != h_id
                    ])
                    similarities.append((h_id, avg_sim))
                centroid = max(similarities, key=lambda x: x[1])[0]
            
            cluster = HypothesisCluster(
                cluster_id=f"cluster_{label}",
                hypotheses=hypothesis_ids,
                centroid=centroid,
                theme=f"Theme for cluster {label}",  # This would be generated by LLM
                key_features=[],  # This would be generated by LLM
                intra_cluster_similarity=np.mean([
                    self.get_cached_similarity(h1, h2).score
                    for i, h1 in enumerate(hypothesis_ids)
                    for h2 in hypothesis_ids[i+1:]
                ])
            )
            cluster_objects.append(cluster)
            self.state.clusters[cluster.cluster_id] = cluster
            
        return cluster_objects
        
    def get_cached_similarity(self, hypothesis_a_id: str, hypothesis_b_id: str) -> SimilarityScore:
        """Get cached similarity score."""
        cache_key = f"{hypothesis_a_id}_{hypothesis_b_id}"
        reverse_key = f"{hypothesis_b_id}_{hypothesis_a_id}"
        
        if cache_key in self.state.similarity_cache:
            return self.state.similarity_cache[cache_key]
        if reverse_key in self.state.similarity_cache:
            return self.state.similarity_cache[reverse_key]
            
        # Return default score if not found
        return SimilarityScore(
            score=0.0,
            aspects={},
            shared_elements=[],
            key_differences=[]
        )
        
    def get_similar_hypotheses(
        self,
        hypothesis: Hypothesis,
        min_similarity: float = 0.7,
        max_results: int = 5
    ) -> List[Tuple[str, float]]:
        """Get most similar hypotheses to the given one."""
        similarities = []
        for other_id in self.state.embeddings.keys():
            if other_id != hypothesis.id:
                score = self.get_cached_similarity(hypothesis.id, other_id)
                similarities.append((other_id, score.score))
                
        return sorted(
            [s for s in similarities if s[1] >= min_similarity],
            key=lambda x: x[1],
            reverse=True
        )[:max_results]
        
    def get_cluster_stats(self) -> Dict[str, Any]:
        """Get statistics about hypothesis clusters."""
        if not self.state.clusters:
            return {}
            
        stats = {
            "total_clusters": len(self.state.clusters),
            "average_cluster_size": np.mean([
                len(c.hypotheses) for c in self.state.clusters.values()
            ]),
            "largest_cluster": max(
                len(c.hypotheses) for c in self.state.clusters.values()
            ),
            "average_intra_cluster_similarity": np.mean([
                c.intra_cluster_similarity for c in self.state.clusters.values()
            ]),
            "cluster_sizes": {
                c.cluster_id: len(c.hypotheses)
                for c in self.state.clusters.values()
            }
        }
        
        return stats
        
    def analyze_proximity_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in hypothesis relationships."""
        if not self.state.similarity_cache:
            return {}
            
        # Calculate overall similarity statistics
        similarities = [s.score for s in self.state.similarity_cache.values()]
        
        # Analyze aspect-specific patterns
        aspect_stats = {}
        for score in self.state.similarity_cache.values():
            for aspect, value in score.aspects.items():
                if aspect not in aspect_stats:
                    aspect_stats[aspect] = []
                aspect_stats[aspect].append(value)
                
        return {
            "total_comparisons": len(self.state.similarity_cache) // 2,  # Divide by 2 due to symmetry
            "average_similarity": np.mean(similarities),
            "similarity_distribution": {
                "min": min(similarities),
                "max": max(similarities),
                "std": np.std(similarities)
            },
            "aspect_averages": {
                aspect: np.mean(values)
                for aspect, values in aspect_stats.items()
            },
            "cluster_stats": self.get_cluster_stats()
        } 
```

.\agents\co_scientist\ranking\ranking_agent.py
```python
"""Ranking agent for tournament-based hypothesis evaluation."""

from typing import Any, Dict, List, Optional, Tuple
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
import math

from ..base_agent import BaseAgent, AgentState
from ..generation.generation_agent import Hypothesis

class TournamentMatch(BaseModel):
    """Tournament match between two hypotheses."""
    match_id: str = Field(description="Unique identifier for this match")
    hypothesis_a: str = Field(description="ID of first hypothesis")
    hypothesis_b: str = Field(description="ID of second hypothesis")
    winner: Optional[str] = Field(description="ID of winning hypothesis")
    score_a: float = Field(description="Score for hypothesis A (0-1)", ge=0.0, le=1.0)
    score_b: float = Field(description="Score for hypothesis B (0-1)", ge=0.0, le=1.0)
    reasoning: str = Field(description="Reasoning for the decision")
    criteria_scores: Dict[str, Dict[str, float]] = Field(description="Detailed scoring by criteria")

class RankingState(AgentState):
    """Ranking agent state."""
    matches: List[TournamentMatch] = Field(default_factory=list)
    elo_ratings: Dict[str, float] = Field(default_factory=dict)  # hypothesis_id -> rating
    match_history: List[Dict[str, Any]] = Field(default_factory=list)
    tournament_metrics: Dict[str, Any] = Field(default_factory=dict)
    current_tournament: Optional[str] = None

class RankingAgent(BaseAgent):
    """Agent responsible for tournament-based hypothesis ranking."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "ranking",
        system_prompt: Optional[str] = None,
        k_factor: float = 32.0,
        initial_rating: float = 1500.0
    ):
        """Initialize the ranking agent."""
        if system_prompt is None:
            system_prompt = """You are the ranking agent responsible for tournament-based hypothesis evaluation.
Your role is to:
1. Conduct fair and objective hypothesis comparisons
2. Maintain an Elo-based ranking system
3. Identify strongest hypotheses through tournaments
4. Provide detailed comparison reasoning
5. Track hypothesis performance over time
6. Ensure diverse hypothesis evaluation

Follow these guidelines:
- Compare hypotheses systematically
- Use consistent evaluation criteria
- Provide detailed reasoning for decisions
- Consider multiple aspects in scoring
- Track and analyze tournament patterns
- Maintain fairness in comparisons
- Focus on scientific merit"""

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="ranking",
            system_prompt=system_prompt,
            output_parser=PydanticOutputParser(pydantic_object=TournamentMatch)
        )
        
        # Initialize ranking-specific state
        self.state = RankingState(
            agent_id=agent_id,
            agent_type="ranking",
            matches=[],
            elo_ratings={},
            match_history=[],
            tournament_metrics={},
            current_tournament=None
        )
        
        self.k_factor = k_factor
        self.initial_rating = initial_rating
        
    async def conduct_match(
        self,
        hypothesis_a: Hypothesis,
        hypothesis_b: Hypothesis,
        context: Dict[str, Any]
    ) -> TournamentMatch:
        """Conduct a tournament match between two hypotheses."""
        # Generate match result using LLM
        result = await self.arun({
            "hypothesis_a": hypothesis_a.dict(),
            "hypothesis_b": hypothesis_b.dict(),
            "context": context,
            "previous_matches": [
                m for m in self.state.matches 
                if m.hypothesis_a in [hypothesis_a.id, hypothesis_b.id] 
                or m.hypothesis_b in [hypothesis_a.id, hypothesis_b.id]
            ]
        })
        
        # Create match object
        match = TournamentMatch(**result)
        
        # Update state
        self.state.matches.append(match)
        self.state.match_history.append({
            "match_id": match.match_id,
            "hypothesis_a": hypothesis_a.id,
            "hypothesis_b": hypothesis_b.id,
            "winner": match.winner,
            "tournament": self.state.current_tournament,
            "timestamp": "TODO: Add timestamp"
        })
        
        # Update Elo ratings
        self._update_elo_ratings(match)
        
        # Update metrics
        self._update_tournament_metrics(match)
        
        return match
        
    def _update_elo_ratings(self, match: TournamentMatch) -> None:
        """Update Elo ratings based on match result."""
        # Initialize ratings if needed
        if match.hypothesis_a not in self.state.elo_ratings:
            self.state.elo_ratings[match.hypothesis_a] = self.initial_rating
        if match.hypothesis_b not in self.state.elo_ratings:
            self.state.elo_ratings[match.hypothesis_b] = self.initial_rating
            
        # Get current ratings
        rating_a = self.state.elo_ratings[match.hypothesis_a]
        rating_b = self.state.elo_ratings[match.hypothesis_b]
        
        # Calculate expected scores
        expected_a = 1.0 / (1.0 + math.pow(10, (rating_b - rating_a) / 400.0))
        expected_b = 1.0 / (1.0 + math.pow(10, (rating_a - rating_b) / 400.0))
        
        # Calculate actual scores
        actual_a = match.score_a
        actual_b = match.score_b
        
        # Update ratings
        self.state.elo_ratings[match.hypothesis_a] = rating_a + self.k_factor * (actual_a - expected_a)
        self.state.elo_ratings[match.hypothesis_b] = rating_b + self.k_factor * (actual_b - expected_b)
        
    def _update_tournament_metrics(self, match: TournamentMatch) -> None:
        """Update tournament metrics with new match."""
        metrics = self.state.tournament_metrics
        
        # Update match counts
        metrics["total_matches"] = metrics.get("total_matches", 0) + 1
        
        # Track hypothesis performance
        for hypothesis_id in [match.hypothesis_a, match.hypothesis_b]:
            if hypothesis_id not in metrics.get("hypothesis_stats", {}):
                metrics.setdefault("hypothesis_stats", {})[hypothesis_id] = {
                    "matches": 0,
                    "wins": 0,
                    "total_score": 0.0
                }
            
            stats = metrics["hypothesis_stats"][hypothesis_id]
            stats["matches"] += 1
            if match.winner == hypothesis_id:
                stats["wins"] += 1
            stats["total_score"] += match.score_a if hypothesis_id == match.hypothesis_a else match.score_b
            
        # Track criteria statistics
        for criteria, scores in match.criteria_scores.items():
            if criteria not in metrics.get("criteria_stats", {}):
                metrics.setdefault("criteria_stats", {})[criteria] = {
                    "total_score": 0.0,
                    "count": 0
                }
            
            stats = metrics["criteria_stats"][criteria]
            stats["total_score"] += sum(scores.values())
            stats["count"] += len(scores)
            
        self.state.tournament_metrics = metrics
        
    def get_rankings(self) -> List[Tuple[str, float]]:
        """Get current hypothesis rankings."""
        return sorted(
            self.state.elo_ratings.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
    def get_hypothesis_stats(self, hypothesis_id: str) -> Dict[str, Any]:
        """Get statistics for a specific hypothesis."""
        stats = self.state.tournament_metrics.get("hypothesis_stats", {}).get(hypothesis_id, {})
        matches = [m for m in self.state.matches if hypothesis_id in [m.hypothesis_a, m.hypothesis_b]]
        
        return {
            "matches_played": stats.get("matches", 0),
            "wins": stats.get("wins", 0),
            "average_score": stats.get("total_score", 0) / stats.get("matches", 1),
            "current_rating": self.state.elo_ratings.get(hypothesis_id, self.initial_rating),
            "recent_matches": [m.dict() for m in matches[-5:]],
            "win_rate": stats.get("wins", 0) / stats.get("matches", 1) if stats.get("matches", 0) > 0 else 0
        }
        
    def analyze_tournament_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in tournament results."""
        metrics = self.state.tournament_metrics
        
        # Calculate criteria averages
        criteria_averages = {}
        for criteria, stats in metrics.get("criteria_stats", {}).items():
            if stats["count"] > 0:
                criteria_averages[criteria] = stats["total_score"] / stats["count"]
                
        # Get top performing hypotheses
        hypothesis_stats = metrics.get("hypothesis_stats", {})
        top_performers = sorted(
            [
                (h_id, stats)
                for h_id, stats in hypothesis_stats.items()
                if stats["matches"] >= 5  # Minimum matches threshold
            ],
            key=lambda x: x[1]["wins"] / x[1]["matches"],
            reverse=True
        )[:5]
        
        return {
            "total_matches": metrics.get("total_matches", 0),
            "criteria_averages": criteria_averages,
            "top_performers": [
                {
                    "hypothesis_id": h_id,
                    "win_rate": stats["wins"] / stats["matches"],
                    "matches_played": stats["matches"],
                    "average_score": stats["total_score"] / stats["matches"]
                }
                for h_id, stats in top_performers
            ],
            "rating_distribution": {
                "min": min(self.state.elo_ratings.values()) if self.state.elo_ratings else self.initial_rating,
                "max": max(self.state.elo_ratings.values()) if self.state.elo_ratings else self.initial_rating,
                "average": sum(self.state.elo_ratings.values()) / len(self.state.elo_ratings) if self.state.elo_ratings else self.initial_rating
            }
        } 
```

.\agents\co_scientist\reflection\reflection_agent.py
```python
"""Reflection agent for hypothesis evaluation."""

from typing import Any, Dict, List, Optional, Literal
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableSerializable
from datetime import datetime

from ..base_agent import BaseAgent, AgentState
from ..generation.generation_agent import Hypothesis

class Review(BaseModel):
    """Model for hypothesis reviews."""
    hypothesis_id: str = Field(description="ID of the hypothesis being reviewed")
    review_type: str = Field(description="Type of review conducted")
    score: float = Field(description="Overall review score (0-1)", ge=0.0, le=1.0)
    confidence: float = Field(description="Confidence in the review (0-1)", ge=0.0, le=1.0)
    key_points: List[str] = Field(description="Key points from the review")
    strengths: List[str] = Field(description="Identified strengths")
    weaknesses: List[str] = Field(description="Identified weaknesses")
    suggestions: List[str] = Field(description="Suggestions for improvement")
    timestamp: str = Field(description="When the review was conducted")

class ReflectionState(AgentState):
    """Reflection agent state."""
    reviews: Dict[str, List[Review]] = Field(default_factory=dict)
    review_history: List[Dict[str, Any]] = Field(default_factory=list)
    verification_tools: Dict[str, Any] = Field(default_factory=dict)
    review_metrics: Dict[str, Any] = Field(default_factory=dict)
    current_strategy: Optional[str] = None

class ReflectionAgent(BaseAgent):
    """Agent responsible for evaluating research hypotheses."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "reflection",
        system_prompt: Optional[str] = None
    ):
        """Initialize the reflection agent."""
        # Create output parser
        parser = PydanticOutputParser(pydantic_object=Review)
        format_instructions = parser.get_format_instructions()
        
        if system_prompt is None:
            system_prompt = """You are the reflection agent responsible for evaluating research hypotheses.
Your role is to:
1. Conduct thorough reviews of research hypotheses
2. Identify strengths and weaknesses
3. Verify assumptions and claims
4. Assess practical feasibility
5. Suggest concrete improvements
6. Maintain high scientific standards

Review Types:
- Initial Review: Quick assessment of basic validity
- Full Review: Comprehensive evaluation with literature
- Deep Verification: Detailed analysis of assumptions
- Observation Review: Check against existing evidence
- Simulation Review: Mental simulation of mechanisms
- Tournament Review: Comparative evaluation

Follow these guidelines:
- Be thorough and systematic
- Support all claims with evidence
- Consider multiple perspectives
- Identify potential issues early
- Suggest concrete improvements
- Maintain scientific rigor
- Be constructive in criticism

Your response MUST be a valid JSON object with the following structure:
{{
  "hypothesis_id": "string - ID of the hypothesis being reviewed",
  "review_type": "string - Type of review (e.g., initial, full, deep_verification)",
  "score": "number between 0 and 1 (e.g., 0.85)",
  "confidence": "number between 0 and 1 (e.g., 0.90)",
  "key_points": ["array of strings - key points from review"],
  "strengths": ["array of strings - identified strengths"],
  "weaknesses": ["array of strings - identified weaknesses"],
  "suggestions": ["array of strings - suggestions for improvement"],
  "timestamp": "current timestamp string in ISO format"
}}

{format_instructions}"""

        # Create prompt template with escaped brackets
        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt),
            HumanMessagePromptTemplate.from_template("""Please review the following hypothesis:

Hypothesis: {{{{hypothesis}}}}
Review Type: {{{{review_type}}}}
Web Knowledge: {{{{web_knowledge}}}}
Previous Reviews: {{{{previous_reviews}}}}

{format_instructions}

Generate a single, well-formed JSON review that follows the required format exactly.
Focus on providing constructive and actionable feedback.
Do not omit any required fields or deviate from the specified formats.""")
        ])

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="reflection",
            system_prompt=system_prompt,
            output_parser=parser
        )
        
        # Initialize reflection-specific state
        self.state = ReflectionState(
            agent_id=agent_id,
            agent_type="reflection",
            reviews={},
            review_history=[],
            verification_tools={},
            review_metrics={},
            current_strategy=None
        )
        
        # Create chain with format instructions
        self.chain = prompt | self.llm | parser
        
    async def review_hypothesis(
        self,
        hypothesis: Hypothesis,
        review_type: Literal["initial", "full", "deep_verification", "observation", "simulation", "tournament"],
        context: Dict[str, Any]
    ) -> Review:
        """Review a research hypothesis."""
        # Get previous reviews
        previous_reviews = [r.dict() for r in self.state.reviews.get(hypothesis.id, [])]
        
        # Format web knowledge for prompt
        web_knowledge_summary = []
        for source in context.get("web_knowledge", []):
            if isinstance(source, dict):
                summary = {
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "summary": source.get("summary", ""),
                    "key_points": source.get("content", "")[:500] + "..."  # First 500 chars
                }
                web_knowledge_summary.append(summary)
        
        # Generate review using LLM
        result = await self.chain.ainvoke({
            "format_instructions": PydanticOutputParser(pydantic_object=Review).get_format_instructions(),
            "hypothesis": hypothesis.dict(),
            "review_type": review_type,
            "web_knowledge": web_knowledge_summary if web_knowledge_summary else "No web knowledge available",
            "previous_reviews": previous_reviews
        })
        
        # Create review object
        if isinstance(result, dict):
            review = Review(**result)
        else:
            # If result is already a Review (from output parser), use it directly
            review = result
            
        # Add timestamp if not present
        if not review.timestamp:
            review.timestamp = datetime.now().isoformat()
        
        # Update state
        if hypothesis.id not in self.state.reviews:
            self.state.reviews[hypothesis.id] = []
        self.state.reviews[hypothesis.id].append(review)
        
        self.state.review_history.append({
            "hypothesis_id": hypothesis.id,
            "review_type": review_type,
            "context": context,
            "timestamp": review.timestamp
        })
        
        # Update metrics
        self._update_review_metrics(review)
        
        return review
        
    def register_verification_tool(self, name: str, tool: Any) -> None:
        """Register a verification tool."""
        self.state.verification_tools[name] = tool
        
    def get_reviews(
        self,
        hypothesis_id: Optional[str] = None,
        review_type: Optional[str] = None
    ) -> List[Review]:
        """Get reviews, optionally filtered by hypothesis and type."""
        if hypothesis_id and hypothesis_id not in self.state.reviews:
            return []
            
        reviews = []
        if hypothesis_id:
            reviews = self.state.reviews[hypothesis_id]
        else:
            reviews = [r for rs in self.state.reviews.values() for r in rs]
            
        if review_type:
            reviews = [r for r in reviews if r.review_type == review_type]
            
        return reviews
        
    def get_review_metrics(self) -> Dict[str, Any]:
        """Get current review metrics."""
        return self.state.review_metrics
        
    def _update_review_metrics(self, review: Review) -> None:
        """Update review metrics with new review."""
        metrics = self.state.review_metrics
        
        # Update review type counts
        type_counts = metrics.get("review_type_counts", {})
        type_counts[review.review_type] = type_counts.get(review.review_type, 0) + 1
        metrics["review_type_counts"] = type_counts
        
        # Update average scores
        scores = metrics.get("average_scores", {})
        review_type = review.review_type
        current_avg = scores.get(review_type, {"sum": 0, "count": 0})
        current_avg["sum"] += review.score
        current_avg["count"] += 1
        scores[review_type] = current_avg
        metrics["average_scores"] = scores
        
        # Update common issues
        all_weaknesses = metrics.get("common_weaknesses", {})
        for weakness in review.weaknesses:
            all_weaknesses[weakness] = all_weaknesses.get(weakness, 0) + 1
        metrics["common_weaknesses"] = all_weaknesses
        
        # Update improvement suggestions
        all_suggestions = metrics.get("common_suggestions", {})
        for suggestion in review.suggestions:
            all_suggestions[suggestion] = all_suggestions.get(suggestion, 0) + 1
        metrics["common_suggestions"] = all_suggestions
        
        self.state.review_metrics = metrics
        
    def analyze_review_patterns(self) -> Dict[str, Any]:
        """Analyze patterns in reviews."""
        metrics = self.state.review_metrics
        
        # Calculate averages
        avg_scores = {}
        for review_type, data in metrics.get("average_scores", {}).items():
            if data["count"] > 0:
                avg_scores[review_type] = data["sum"] / data["count"]
                
        # Get top issues and suggestions
        top_weaknesses = sorted(
            metrics.get("common_weaknesses", {}).items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        top_suggestions = sorted(
            metrics.get("common_suggestions", {}).items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        return {
            "total_reviews": sum(metrics.get("review_type_counts", {}).values()),
            "review_type_distribution": metrics.get("review_type_counts", {}),
            "average_scores_by_type": avg_scores,
            "top_weaknesses": top_weaknesses,
            "top_suggestions": top_suggestions
        }
        
    def set_review_strategy(self, strategy: str) -> None:
        """Set the current review strategy."""
        self.state.current_strategy = strategy
        self.update_memory("current_strategy", strategy)
        
    def get_current_strategy(self) -> Optional[str]:
        """Get the current review strategy."""
        return self.state.current_strategy
        
    def get_verification_tools(self) -> Dict[str, Any]:
        """Get the registered verification tools."""
        return self.state.verification_tools
        
    def get_review_history(self) -> List[Dict[str, Any]]:
        """Get the review history."""
        return self.state.review_history

```

.\agents\co_scientist\supervisor\supervisor_agent.py
```python
"""Supervisor agent for orchestrating the AI co-scientist system."""

from typing import Any, Dict, List, Optional, Type
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import PydanticOutputParser
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableSerializable
import networkx as nx

from ..base_agent import BaseAgent, AgentState

class ResearchGoal(BaseModel):
    """Research goal specification."""
    goal: str = Field(description="The research goal/question")
    domain: str = Field(description="Research domain")
    constraints: List[str] = Field(default_factory=list, description="Research constraints")
    preferences: Dict[str, Any] = Field(default_factory=dict, description="Research preferences")

class ResearchPlan(BaseModel):
    """Research plan configuration."""
    goal: ResearchGoal
    tasks: List[Dict[str, Any]] = Field(default_factory=list, description="List of research tasks")
    agent_assignments: Dict[str, List[str]] = Field(default_factory=dict, description="Map of tasks to agent IDs")
    dependencies: Dict[str, List[str]] = Field(default_factory=dict, description="Task dependencies")

class SupervisorState(AgentState):
    """Supervisor agent state."""
    research_goal: Optional[ResearchGoal] = None
    research_plan: Optional[ResearchPlan] = None
    active_agents: Dict[str, AgentState] = Field(default_factory=dict)
    task_queue: List[Dict[str, Any]] = Field(default_factory=list)
    completed_tasks: List[Dict[str, Any]] = Field(default_factory=list)
    context_memory: Dict[str, Any] = Field(default_factory=dict)

class SupervisorAgent(BaseAgent):
    """Supervisor agent for orchestrating the AI co-scientist system."""
    
    def __init__(
        self,
        llm: BaseChatModel,
        agent_id: str = "supervisor",
        system_prompt: Optional[str] = None
    ):
        """Initialize the supervisor agent."""
        if system_prompt is None:
            system_prompt = """You are the supervisor agent responsible for orchestrating the AI co-scientist system.
Your role is to:
1. Parse and understand research goals
2. Create and manage research plans
3. Assign tasks to specialized agents
4. Monitor progress and handle failures
5. Manage system resources and context memory
6. Ensure high-quality research output

Follow these guidelines:
- Break down complex research goals into manageable tasks
- Assign tasks to the most suitable specialized agents
- Track dependencies and ensure proper task ordering
- Maintain context and state across the system
- Handle errors and adapt plans as needed
- Optimize resource utilization
- Ensure research quality and novelty

IMPORTANT: Your response MUST be a valid JSON object with the exact structure shown below.
The goal field MUST be a ResearchGoal object containing the EXACT values from the input.
Do not omit any required fields or deviate from the specified formats.

Example valid response:
{{
    "goal": {{
        "goal": "Investigate potential drug repurposing candidates for treating acute myeloid leukemia (AML)",
        "domain": "drug_repurposing",
        "constraints": ["Focus on FDA-approved drugs"],
        "preferences": {{"prioritize_novel_mechanisms": true}}
    }},
    "tasks": [
        {{
            "id": "task1",
            "name": "Literature Review",
            "description": "Review existing literature on AML drug repurposing",
            "expected_duration": "2 days"
        }}
    ],
    "agent_assignments": {{
        "task1": ["generation", "reflection"]
    }},
    "dependencies": {{
        "task1": []
    }}
}}

{format_instructions}"""

        # Create output parser
        parser = PydanticOutputParser(pydantic_object=ResearchPlan)
        
        # Create prompt template with escaped brackets
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_prompt),
            HumanMessage(content="""Please create a research plan based on:

Research Goal: {goal}
Available Agents: {available_agents}
Context: {context}

The research goal contains:
- goal: The research goal statement
- domain: The research domain
- constraints: List of research constraints
- preferences: Dictionary of research preferences

Your response MUST be a valid JSON object with the exact structure shown in the format instructions.
The goal field MUST be a ResearchGoal object containing the EXACT values from the input.
Do not omit any required fields or deviate from the specified formats.

{format_instructions}""")
        ])

        super().__init__(
            llm=llm,
            agent_id=agent_id,
            agent_type="supervisor",
            system_prompt=system_prompt,
            output_parser=parser
        )
        
        # Initialize supervisor-specific state
        self.state = SupervisorState(
            agent_id=agent_id,
            agent_type="supervisor",
            active_agents={},
            task_queue=[],
            completed_tasks=[],
            context_memory={}
        )
        
        # Initialize agent graph
        self.agent_graph = nx.DiGraph()
        
    def register_agent(self, agent: BaseAgent) -> None:
        """Register a specialized agent with the supervisor."""
        self.state.active_agents[agent.state.agent_id] = agent.state
        self.agent_graph.add_node(
            agent.state.agent_id,
            type=agent.state.agent_type,
            state=agent.state.dict()
        )
        
    def set_research_goal(self, goal: ResearchGoal) -> None:
        """Set the current research goal."""
        self.state.research_goal = goal
        self.update_memory("current_goal", goal.dict())
        
    async def create_research_plan(self, context: Optional[Dict[str, Any]] = None) -> ResearchPlan:
        """Create a research plan for the current goal.
        
        Args:
            context: Optional additional context for plan creation
        """
        if not self.state.research_goal:
            raise ValueError("No research goal set")
            
        # Generate plan using LLM
        result = await self.arun({
            "goal": self.state.research_goal.dict(),
            "available_agents": [
                {
                    "id": agent_id,
                    "type": state.agent_type,
                    "capabilities": list(state.tools.keys())
                }
                for agent_id, state in self.state.active_agents.items()
            ],
            "context": context or {},
            "format_instructions": PydanticOutputParser(pydantic_object=ResearchPlan).get_format_instructions()
        })
        
        # Create plan object
        if isinstance(result, dict):
            plan = ResearchPlan(**result)
        else:
            # If result is already a ResearchPlan (from output parser), use it directly
            plan = result
            
        self.state.research_plan = plan
        return plan
        
    def assign_tasks(self) -> None:
        """Assign tasks to agents based on the research plan."""
        if not self.state.research_plan:
            raise ValueError("No research plan available")
            
        # Clear existing queue
        self.state.task_queue = []
        
        # Add tasks to queue based on dependencies
        for task in self.state.research_plan.tasks:
            deps = self.state.research_plan.dependencies.get(task["id"], [])
            if not deps or all(d in self.state.completed_tasks for d in deps):
                self.state.task_queue.append(task)
                
    async def execute_next_task(self) -> Optional[Dict[str, Any]]:
        """Execute the next task in the queue."""
        if not self.state.task_queue:
            return None
            
        task = self.state.task_queue.pop(0)
        
        # Check if research plan exists
        if not self.state.research_plan:
            self.update_memory("error", "No research plan available")
            return {"task_id": task["id"], "error": "No research plan available"}
            
        agent_ids = self.state.research_plan.agent_assignments.get(task["id"], [])
        
        results = []
        for agent_id in agent_ids:
            if agent_id in self.state.active_agents:
                agent = self.state.active_agents[agent_id]
                try:
                    # Get agent instance and execute task
                    agent_instance = self._get_agent_instance(agent_id)
                    if agent_instance:
                        result = await agent_instance.arun(task)
                        results.append(result)
                except Exception as e:
                    self.update_memory(f"error_{task['id']}", str(e))
                    return {"task_id": task["id"], "error": str(e)}
                    
        # Mark task as completed
        self.state.completed_tasks.append(task["id"])
        
        # Update context memory
        self.update_memory(f"result_{task['id']}", results)
        
        return {
            "task_id": task["id"],
            "results": results
        }
        
    def _get_agent_instance(self, agent_id: str) -> Optional[BaseAgent]:
        """Get agent instance by ID."""
        # This would be implemented to return actual agent instances
        # For now it's a placeholder
        return None
        
    def get_research_status(self) -> Dict[str, Any]:
        """Get current research status."""
        return {
            "goal": self.state.research_goal.dict() if self.state.research_goal else None,
            "plan": self.state.research_plan.dict() if self.state.research_plan else None,
            "completed_tasks": len(self.state.completed_tasks),
            "pending_tasks": len(self.state.task_queue),
            "active_agents": len(self.state.active_agents),
            "context_memory_size": len(self.state.context_memory)
        }
        
    def save_context(self) -> None:
        """Save current context to persistent storage."""
        # This would be implemented to save context to disk/database
        pass
        
    def load_context(self) -> None:
        """Load context from persistent storage."""
        # This would be implemented to load context from disk/database
        pass 
```

.\examples\research_example.py
```python
"""Example usage of the AI co-scientist system."""

import asyncio
import sys
import os
from pathlib import Path

# Add project root to Python path
project_root = str(Path(__file__).parent.parent)
sys.path.append(project_root)

from rich.console import Console
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import PydanticOutputParser

from agents.co_scientist.main import AICoScientist

async def main():
    """Run an example research process."""
    # Initialize console
    console = Console()
    console.print("[bold blue]Starting AI Co-scientist Example[/bold blue]")
    
    try:
        # Initialize language model
        llm = ChatOllama(
            model="deepscaler",  # Using deepscaler as base model
            format="json",  # Ensure JSON output format
            temperature=0.7,  # Add some variability
            stop=["\n\n"],  # Add stop sequence for better output control
            seed=42  # For reproducibility
        )
        
        # Initialize AI co-scientist
        scientist = AICoScientist(llm, console)
        
        # Define research goal
        research_goal = {
            "goal": "Investigate potential drug repurposing candidates for treating acute myeloid leukemia (AML)",
            "domain": "drug_repurposing",
            "constraints": [
                "Focus on FDA-approved drugs",
                "Consider only small molecule drugs",
                "Must have known safety profiles",
                "Should be cost-effective"
            ],
            "preferences": {
                "prioritize_novel_mechanisms": True,
                "consider_combination_therapy": True,
                "focus_on_targeted_therapy": True,
                "min_evidence_level": "preclinical",
                "max_candidates": 5
            }
        }
        
        # Run the research process
        console.print("\n[bold green]Starting Research Process[/bold green]")
        results = await scientist.run(research_goal)
        
        # Print summary
        console.print("\n[bold blue]Research Summary[/bold blue]")
        
        # Print key findings from each cycle
        if results.get("cycles"):
            console.print("\n[bold]Key Findings by Cycle:[/bold]")
            for i, cycle in enumerate(results["cycles"], 1):
                console.print(f"\n[bold]Cycle {i}:[/bold]")
                overview = cycle["overview"]
                for finding in overview["key_findings"]:
                    console.print(f"- {finding}")
                    
            # Print top hypotheses
            if results["cycles"]:
                console.print("\n[bold]Top Ranked Hypotheses:[/bold]")
                final_cycle = results["cycles"][-1]
                for h_id, score in final_cycle["rankings"][:3]:
                    for h in final_cycle["hypotheses"]:
                        if h["id"] == h_id:
                            console.print(f"\nHypothesis: {h['statement']}")
                            console.print(f"Score: {score:.2f}")
                            console.print(f"Evidence: {', '.join(h['evidence'])}")
                            
                # Print research directions
                console.print("\n[bold]Promising Research Directions:[/bold]")
                for direction in final_cycle["overview"]["promising_directions"]:
                    console.print(f"\n- {direction['title']}")
                    if "rationale" in direction:
                        console.print(f"  Rationale: {direction['rationale']}")
        else:
            console.print("\n[bold red]No research cycles were completed.[/bold red]")
            console.print("Please check the system status and logs for more information.")
                
    except Exception as e:
        console.print(f"[bold red]Error:[/bold red] {str(e)}")
        import traceback
        console.print(traceback.format_exc())
        raise
        
if __name__ == "__main__":
    asyncio.run(main()) 
```

.\prompts\compiler\compiler_prompts.py
```python
"""Compiler prompts for LLM compiler system."""

from typing import List, Dict, Any, Optional, TypedDict
from pydantic import BaseModel, Field, validator
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class Task(BaseModel):
    """Schema for a task in the execution plan."""
    idx: int = Field(..., description="Task index")
    tool: str = Field(..., description="Tool to use")
    args: Dict[str, Any] = Field(..., description="Tool arguments")
    dependencies: List[int] = Field(default_factory=list, description="Task dependencies")

class Plan(BaseModel):
    """Schema for an execution plan."""
    tasks: List[Task] = Field(..., description="List of tasks to execute")
    thought: Optional[str] = Field("", description="Planner's reasoning")

class TaskResult(BaseModel):
    """Schema for task execution results."""
    task_id: int = Field(..., description="Task ID")
    result: Optional[Dict[str, Any]] = Field(None, description="Task result")
    error: Optional[str] = Field(None, description="Error if any")

    @validator("result")
    def validate_result(cls, v):
        """Validate that result includes a thought field if present."""
        if v is not None and "thought" not in v:
            raise ValueError("Result must include a thought field explaining the execution reasoning")
        return v

class JoinDecision(BaseModel):
    """Schema for join decisions."""
    complete: bool = Field(..., description="Whether execution is complete")
    thought: str = Field(..., description="Joiner's reasoning")
    replan: bool = Field(..., description="Whether replanning is needed")
    feedback: Optional[str] = Field(None, description="Feedback for replanning")

class CompilerState(TypedDict):
    """State for LLM compiler workflow."""
    content: str
    domain_name: str
    plan: Optional[Plan]
    results: List[TaskResult]
    join_decision: Optional[JoinDecision]
    final_result: Optional[Any]
    error: Optional[str]
    feedback: Optional[str]
    knowledge_sources: List[Dict[str, Any]]
    synthetic_knowledge: List[Dict[str, Any]]
    training_examples: List[Dict[str, Any]]
    model_metrics: Dict[str, Any]

def get_plan_generation_prompt() -> ChatPromptTemplate:
    """Get the prompt template for plan generation."""
    parser = PydanticOutputParser(pydantic_object=Plan)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a planning expert that generates execution plans.
IMPORTANT: State variables MUST be referenced using {state[variable_name]} format, e.g. {state[domain_name]}.
Your plans should be efficient and well-organized.

{format_instructions}

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text or explanation.
DO NOT wrap the JSON in markdown code blocks or any other formatting.
DO NOT include ```json or ``` markers.

Available tools and their required args:
- research_topics: {{"domain": "{state[domain_name]}"}} (MUST use {state[domain_name]} exactly like this)
- synthesize_knowledge: {{"sources": <array of source references>}}
- generate_examples: {{"knowledge": <array of knowledge references>}}
- train_model: {{"examples": <array of example objects with input_text and output_text>}}

Example valid response:
{{
  "tasks": [
    {{
      "idx": 0,
      "tool": "research_topics",
      "args": {{"domain": "{state[domain_name]}"}},
      "dependencies": []
    }},
    {{
      "idx": 1,
      "tool": "synthesize_knowledge",
      "args": {{"sources": [{{"id": 0}}]}},
      "dependencies": [0]
    }},
    {{
      "idx": 2,
      "tool": "generate_examples",
      "args": {{"knowledge": [{{"id": 1}}]}},
      "dependencies": [1]
    }},
    {{
      "idx": 3,
      "tool": "train_model",
      "args": {{"examples": [{{"id": 2}}]}},
      "dependencies": [2]
    }}
  ],
  "thought": "First research topics to gather sources, then synthesize knowledge from those sources, generate training examples from the knowledge, and finally train the model on those examples"
}}"""

    human_template = """Current state:
{state}

Generate a plan to execute the workflow based on the current state.

Remember:
1. Return ONLY a valid JSON object
2. Include both tasks and thought fields
3. Use {state[variable_name]} format for state variables (e.g. {state[domain_name]})
4. Make task IX start from 0 and increment sequentially
5. Ensure dependencies refer to valid task IDs
6. Include a clear thought explaining your plan
7. NEVER use {state} directly - always use {state[domain_name]} for the domain argument"""

    messages = [
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]

    return ChatPromptTemplate(messages=messages)

def get_task_execution_prompt() -> ChatPromptTemplate:
    """Get the prompt template for task execution."""
    parser = PydanticOutputParser(pydantic_object=TaskResult)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a task execution expert that carries out planned tasks.
Your executions should be precise and reliable.

{format_instructions}

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text or explanation.
DO NOT wrap the JSON in markdown code blocks or any other formatting.
DO NOT include ```json or ``` markers.

Tool-specific result formats:
1. research_topics:
   "result": {{
     "knowledge_sources": [
       {{
         "content": <string>,
         "metadata": {{
           "source_type": "text",
           "confidence": <float>,
           "timestamp": <string in ISO format>
         }}
       }}
     ],
     "thought": <string explaining the research process>
   }}

2. synthesize_knowledge:
   "result": {{
     "synthetic_knowledge": [
       {{
         "content": <string>,
         "patterns": [<object>],
         "hypotheses": [<object>],
         "relationships": [<object>],
         "confidence": <float>,
         "metadata": <object>
       }}
     ],
     "thought": <string explaining the synthesis process>
   }}

3. generate_examples:
   "result": {{
     "training_examples": [
       {{
         "input_text": <string>,
         "output_text": <string>,
         "metadata": <object>,
         "quality_score": <float>
       }}
     ],
     "thought": <string explaining the example generation process>
   }}

4. train_model:
   "result": {{
     "model_metrics": {{
       "accuracy": <float>,
       "loss": <float>,
       "epochs": <integer>,
       "training_time": <float>
     }},
     "thought": <string explaining the training process>
   }}"""

    human_template = """Current state:
{state}

Execute the following task:
{task}

Remember to:
1. Return ONLY a valid JSON object
2. Include task_id, result, and error fields
3. Set task_id to the actual task index (not a template variable)
4. Include a thought field in the result explaining your execution
5. Set error to null if successful, or an error message if failed
6. Follow the tool-specific result format for the task's tool"""

    messages = [
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]

    return ChatPromptTemplate(messages=messages)

def get_join_decision_prompt() -> ChatPromptTemplate:
    """Get the prompt template for join decisions."""
    parser = PydanticOutputParser(pydantic_object=JoinDecision)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a decision-making expert that evaluates execution results.
Your decisions should be based on clear evidence and reasoning.

{format_instructions}

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text or explanation.
DO NOT wrap the JSON in markdown code blocks or any other formatting.
DO NOT include ```json or ``` markers.

Example valid response for success case:
{{
  "complete": true,
  "thought": "All tasks completed successfully with expected results",
  "replan": false,
  "feedback": null
}}

Example valid response for failure case:
{{
  "complete": false,
  "thought": "Task 2 failed due to missing data",
  "replan": true,
  "feedback": "Need to add data_gathering task before task 2"
}}"""

    human_template = """Please evaluate this execution state and make a join decision:
{state}

Remember to:
1. Return ONLY a valid JSON object
2. Include complete, thought, replan, and feedback fields
3. Set complete=true ONLY if ALL tasks succeeded
4. Set replan=true if ANY task can be retried
5. Include clear thought explaining your decision
6. Provide feedback if replanning is needed
7. Check ALL task results and dependencies"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=format_instructions)

    return prompt 
```

.\prompts\compiler\task_execution.py
```python
"""Task execution prompts."""
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class TaskResult(BaseModel):
    """Schema for task execution results."""
    task_id: int = Field(..., description="Task ID")
    result: Optional[Dict[str, Any]] = Field(None, description="Task result")
    error: Optional[str] = Field(None, description="Error if any")

def get_task_execution_prompt() -> ChatPromptTemplate:
    """Get the task execution prompt template."""
    parser = PydanticOutputParser(pydantic_object=TaskResult)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Execute knowledge acquisition tasks.
{format_instructions}

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text or explanation.
DO NOT wrap the JSON in markdown code blocks or any other formatting.
DO NOT include ```json or ``` markers.
The JSON object MUST EXACTLY match this structure:
{{
  "task_id": <integer - MUST be the idx value from the task>,
  "result": <object with tool-specific format or null>,
  "error": <string or null>
}}

CRITICAL: The task_id field MUST be an integer matching the idx value from the task.
DO NOT include the entire task object as the task_id.

CRITICAL: State variables MUST be referenced using {state[variable_name]} format.
For example, to reference the domain_name from state, use: {state[domain_name]}
DO NOT use {{state}} or {{state.variable_name}} formats - these are invalid and will cause errors.

Tool-specific result formats:
1. research_topics:
   "result": {{
     "knowledge_sources": [
       {{
         "content": <string>,
         "metadata": {{
           "source_type": "text",
           "confidence": <float>,
           "timestamp": <string in ISO format>
         }}
       }}
     ],
     "thought": <string explaining the research process>
   }}

2. synthesize_knowledge:
   "result": {{
     "synthetic_knowledge": [
       {{
         "content": <string>,
         "patterns": [<object>],
         "hypotheses": [<object>],
         "relationships": [<object>],
         "confidence": <float>,
         "metadata": <object>
       }}
     ],
     "thought": <string explaining the synthesis process>
   }}

3. generate_examples:
   "result": {{
     "training_examples": [
       {{
         "input_text": <string>,
         "output_text": <string>,
         "metadata": <object>,
         "quality_score": <float>
       }}
     ],
     "thought": <string explaining the example generation process>
   }}

4. train_model:
   "result": {{
     "model_metrics": {{
       "loss": <float>,
       "eval_loss": <float>,
       "train_samples": <integer>,
       "eval_samples": <integer>,
       "training_time": <float>
     }},
     "thought": <string explaining the training process>
   }}

CRITICAL FORMATTING RULES:
1. Use ONLY double quotes (") for strings and property names
2. Arrays must be comma-separated and enclosed in square brackets []
3. Objects must be comma-separated and enclosed in curly braces {{}}
4. No trailing commas after the last item in arrays or objects
5. No comments or explanatory text
6. No JavaScript/Python syntax - ONLY valid JSON
7. No extra fields or properties beyond what is specified
8. No malformed JSON or syntax errors
9. No single quotes (') - use double quotes (") only
10. No unescaped newlines in strings
11. No extra whitespace or indentation
12. No extra quotes around the entire JSON object
13. No extra quotes around individual fields
14. No extra quotes around arrays or objects
15. ALWAYS include both task_id and result fields
16. NEVER return just a task_id without a result
17. If execution fails, set result=null and include error message
18. task_id MUST match the idx field from the task being executed
19. Dependencies MUST be checked - if any dependency task failed, this task should fail with "Dependencies not met"
20. The result field MUST match the tool-specific format exactly
21. The result field MUST be null if there is an error
22. The error field MUST be null if there is a result
23. NEVER include any text before or after the JSON object
24. NEVER include any comments or explanations
25. NEVER include any extra fields or properties
26. ALWAYS include a thought field in the result object explaining your reasoning
27. State variables in args (e.g. {state[domain_name]}) will be replaced with actual values
28. Function names MUST start with a letter or underscore
29. Function names MUST contain only letters, numbers, underscores, dots, or dashes
30. Function names MUST be at most 64 characters long
31. For train_model task, examples must be an array of objects with input_text and output_text fields
32. For train_model task, if examples are referenced by ID, fetch full examples from system state

Example valid response for success:
{{
  "task_id": 0,
  "result": {{
    "knowledge_sources": [
      {{
        "content": "Example source content",
        "metadata": {{
          "source_type": "text",
          "confidence": 0.95,
          "timestamp": "2024-02-07T12:00:00Z"
        }}
      }}
    ],
    "thought": "Successfully researched topics and extracted knowledge"
  }},
  "error": null
}}

Example valid response for failure:
{{
  "task_id": 1,
  "result": null,
  "error": "Dependencies not met - task 0 failed"
}}"""

    human_template = """Execute this task:

Task:
{{
  "idx": {task.idx},
  "tool": "{task.tool}",
  "args": {task.args},
  "dependencies": {task.dependencies}
}}

Remember:
1. Return ONLY valid JSON with the EXACT structure shown above
2. No text before or after the JSON
3. No explanation, just the JSON object
4. Always include all required fields
5. Set error=null for successful execution
6. Follow the tool-specific result format exactly
7. NEVER return just a task_id without a result
8. task_id MUST match the idx field from the task being executed
9. Dependencies MUST be checked - if any dependency task failed, this task should fail with "Dependencies not met"
10. The result field MUST match the tool-specific format exactly
11. The result field MUST be null if there is an error
12. The error field MUST be null if there is a result
13. NEVER include any text before or after the JSON object
14. NEVER include any comments or explanations
15. NEVER include any extra fields or properties
16. ALWAYS include a thought field in the result object
17. State variables in args will be replaced with actual values
18. Function names MUST start with a letter or underscore
19. Function names MUST contain only letters, numbers, underscores, dots, or dashes
20. Function names MUST be at most 64 characters long"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt 
```

.\prompts\knowledge_acquisition\confidence_evaluation.py
```python
"""Confidence evaluation prompts."""
from typing import Dict
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class ConfidenceFactors(BaseModel):
    """Schema for confidence evaluation factors"""
    content_quality: float = Field(default=0.5, description="Quality of content", ge=0.0, le=1.0)
    entity_relevance: float = Field(default=0.5, description="Relevance of entities", ge=0.0, le=1.0)
    relationship_validity: float = Field(default=0.5, description="Validity of relationships", ge=0.0, le=1.0)
    source_reliability: float = Field(default=0.5, description="Reliability of source", ge=0.0, le=1.0)
    context_relevance: float = Field(default=0.5, description="Relevance of context", ge=0.0, le=1.0)
    overall: float = Field(default=0.5, description="Overall confidence score", ge=0.0, le=1.0)

class ConfidenceEvaluation(BaseModel):
    """Schema for confidence evaluation"""
    confidence: float = Field(description="Overall confidence score", ge=0.0, le=1.0)
    factors: ConfidenceFactors = Field(description="Detailed confidence factors")
    reasoning: str = Field(description="Reasoning behind confidence evaluation")

def get_confidence_evaluation_prompt() -> ChatPromptTemplate:
    """Get the confidence evaluation prompt template."""
    parser = PydanticOutputParser(pydantic_object=ConfidenceEvaluation)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are an expert at evaluating confidence in extracted knowledge.
{format_instructions}

CRITICAL RULES:
1. You MUST output ONLY a valid JSON object
2. The JSON MUST match the schema exactly
3. All confidence scores MUST be numbers between 0.0 and 1.0
4. The factors object MUST include all required fields
5. All strings MUST be properly escaped if they contain special characters
6. Do not include any text before or after the JSON object
7. Do not include any explanations or notes
8. The response should look exactly like this:
{{
    "confidence": 0.85,
    "factors": {{
        "content_quality": 0.9,
        "entity_relevance": 0.8,
        "relationship_validity": 0.85,
        "source_reliability": 0.9,
        "context_relevance": 0.8,
        "overall": 0.85
    }},
    "reasoning": "The content is well-structured and coherent. Entities are relevant to the domain and relationships are logically valid. The source appears reliable and the context is appropriate."
}}

EVALUATION GUIDELINES:
1. Content Quality:
   - Clarity and coherence
   - Completeness of information
   - Technical accuracy
   - Writing quality

2. Entity Relevance:
   - Domain specificity
   - Technical accuracy
   - Coverage of key concepts
   - Proper naming/terminology

3. Relationship Validity:
   - Logical connections
   - Proper directionality
   - Appropriate relationship types
   - Consistency with domain knowledge

4. Source Reliability:
   - Author expertise
   - Publication venue
   - Citation quality
   - Peer review status

5. Context Relevance:
   - Domain alignment
   - Temporal relevance
   - Scope appropriateness
   - Target audience match"""

    human_template = """Evaluate confidence for this content:

Content: {content}
Entities: {entities}
Relationships: {relationships}
Source Type: {source_type}

Remember:
1. Return ONLY a valid JSON object
2. Include all required confidence factors
3. Use proper JSON formatting
4. Do not include any text before or after the JSON"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt 
```

.\prompts\knowledge_acquisition\entity_extraction.py
```python
"""Entity extraction prompts."""
from typing import List
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class EntityResponse(BaseModel):
    """Schema for entity extraction response"""
    entities: List[str] = Field(
        description="List of extracted entities",
        default_factory=list
    )

def get_entity_extraction_prompt() -> ChatPromptTemplate:
    """Get the entity extraction prompt template."""
    parser = PydanticOutputParser(pydantic_object=EntityResponse)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are an expert at extracting medical and scientific entities from research text.
{format_instructions}

CRITICAL RULES:
1. You MUST output ONLY a valid JSON object
2. The JSON MUST match the schema exactly
3. The entities field MUST be an array of strings
4. Each entity MUST be properly escaped if it contains special characters
5. Do not include any text before or after the JSON object
6. Do not include any explanations or notes
7. The response should look exactly like this:
{{
    "entities": [
        "PANDAS",
        "Streptococcal Infection",
        "Gut-Brain Axis",
        "Vagus Nerve",
        "Plant-Based Therapy"
    ]
}}

GUIDELINES for medical entity extraction:
1. Extract meaningful medical and scientific terms:
   - Medical conditions and diseases
   - Anatomical structures and systems
   - Biological mechanisms and pathways
   - Therapeutic approaches and treatments
   - Biomarkers and clinical indicators
   - Research methodologies and protocols
   - Patient outcomes and symptoms
   - Drug classes and compounds
   - Plant-based and alternative therapies

2. Entity Categories to Extract:
   - Diseases and Conditions:
     * Primary conditions (e.g., "PANDAS", "Autoimmune Encephalitis")
     * Related disorders
     * Comorbidities
   
   - Anatomical/Biological:
     * Body systems (e.g., "Gut-Brain Axis", "Immune System")
     * Organs and tissues
     * Neural pathways
     * Cellular components
   
   - Therapeutic:
     * Treatment modalities
     * Medications and compounds
     * Natural remedies
     * Therapeutic approaches
   
   - Clinical:
     * Symptoms and signs
     * Diagnostic tests
     * Biomarkers
     * Clinical outcomes

3. Entity Validation Rules:
   - Must be recognized medical/scientific terms
   - Should be specific rather than general
   - Must be relevant to the medical domain
   - Should be supported by context
   - Must be properly normalized (e.g., "IL-6" for "Interleukin 6")

4. Formatting Guidelines:
   - Use standard medical terminology
   - Maintain proper capitalization for proper nouns
   - Keep acronyms in uppercase (e.g., "TNF-α", "IL-6")
   - Use full names for clarity
   - Include both common and scientific names where relevant

5. Quality Requirements:
   - Extract at least 10-15 entities per text
   - Ensure balanced coverage across categories
   - Focus on domain-relevant entities
   - Include both specific and general terms
   - Capture key relationships and hierarchies"""

    human_template = """Extract medical and scientific entities from this text, focusing on PANDAS, gut-brain axis, and therapeutic relationships:

{content}

Remember:
1. Return ONLY a valid JSON object
2. Extract medical/scientific entities only
3. Use proper medical terminology
4. Ensure entities are domain-relevant
5. Include all entity categories (diseases, anatomical, therapeutic, clinical)"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt 
```

.\prompts\knowledge_acquisition\extraction.py
```python
"""Knowledge extraction prompts."""
from typing import List, Dict, Literal
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class Relationship(BaseModel):
    """Schema for knowledge relationships"""
    source: str = Field(description="Source entity or concept")
    relation: Literal[
        # Methodology relationships
        "uses", "applies", "implements",
        # Performance relationships
        "improves", "outperforms", "achieves",
        # Component relationships
        "contains", "consists_of", "part_of",
        # Comparison relationships
        "better_than", "similar_to", "different_from",
        # Causal relationships
        "leads_to", "causes", "affects",
        # Temporal relationships
        "precedes", "follows", "concurrent_with",
        # Legacy relationships
        "is_a", "has_part", "related_to"
    ] = Field(description="Type of relationship")
    target: str = Field(description="Target entity or concept")
    domain: str = Field(default="knowledge", description="Domain this relationship belongs to")

class SourceMetadata(BaseModel):
    """Schema for source metadata"""
    source_type: Literal["text", "pdf", "web"] = Field(description="Type of source")
    confidence_score: float = Field(description="Confidence in source reliability", ge=0.0, le=1.0)
    domain_relevance: float = Field(description="Relevance to current domain", ge=0.0, le=1.0)
    timestamp: str = Field(description="When the source was processed")
    validation_status: Literal["pending", "processed", "failed"] = Field(description="Validation status")
    domain: str = Field(default="knowledge", description="Domain this source belongs to")

class ExtractedKnowledge(BaseModel):
    """Schema for extracted knowledge"""
    content: str = Field(description="A clear, comprehensive summary of the key knowledge extracted from the text")
    entities: List[str] = Field(description="List of important concepts, terms, and entities mentioned in the text")
    relationships: List[Relationship] = Field(default_factory=list, description="List of relationships between entities")
    confidence: float = Field(description="Overall confidence in the extraction", ge=0.0, le=1.0)
    metadata: SourceMetadata = Field(description="Source metadata")

class KeyTermsResponse(BaseModel):
    """Schema for key terms extraction response"""
    terms: List[str] = Field(description="List of key search terms")

def get_knowledge_extraction_prompt() -> ChatPromptTemplate:
    """Get the knowledge extraction prompt template."""
    parser = PydanticOutputParser(pydantic_object=ExtractedKnowledge)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Extract knowledge from text, with special handling for academic papers and technical content. Return in JSON format.
{format_instructions}

GUIDELINES:
1. Extract meaningful knowledge, focusing on:
   - Research objectives and goals
   - Methodologies and approaches
   - Key findings and results
   - Technical concepts and terminology
   - Experimental setups and configurations
   - Metrics and measurements
   - Conclusions and implications

2. For entities, identify:
   - Technical terms and concepts
   - Methods and algorithms
   - Tools and frameworks
   - Metrics and measurements
   - Research domains and fields
   - Components and systems
   - Datasets and benchmarks

3. For relationships, capture:
   - Methodology relationships (uses, applies, implements)
   - Performance relationships (improves, outperforms, achieves)
   - Component relationships (contains, consists_of, part_of)
   - Comparison relationships (better_than, similar_to, different_from)
   - Causal relationships (leads_to, causes, affects)
   - Temporal relationships (precedes, follows, concurrent_with)

4. Evaluate confidence based on:
   - Clarity of presentation
   - Experimental validation
   - Statistical significance
   - Reproducibility of results
   - Citation of related work
   - Methodology rigor

5. Include metadata about:
   - Paper type (research, survey, case study)
   - Domain relevance
   - Publication venue
   - Research context
   - Validation approach

Example response:
{{
    "content": "The paper presents a novel approach to domain adaptation for test case generation using CodeT5. The method improves line coverage by 49.9% compared to baselines.",
    "entities": [
        "CodeT5",
        "domain adaptation",
        "test case generation",
        "line coverage",
        "automated testing",
        "machine learning"
    ],
    "relationships": [
        {{
            "source": "domain adaptation",
            "relation": "improves",
            "target": "line coverage"
        }},
        {{
            "source": "CodeT5",
            "relation": "used_for",
            "target": "test case generation"
        }}
    ],
    "confidence": 0.85,
    "metadata": {{
        "source_type": "text",
        "confidence_score": 0.85,
        "domain_relevance": 0.9,
        "timestamp": "2024-02-09T11:42:32.000Z",
        "validation_status": "processed"
    }}
}}"""

    human_template = """Extract knowledge from this text:

{text}

Remember:
1. Return a valid JSON object
2. Include any knowledge you find
3. Use proper JSON formatting"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt

def get_key_terms_prompt() -> ChatPromptTemplate:
    """Get the key terms extraction prompt template."""
    parser = PydanticOutputParser(pydantic_object=KeyTermsResponse)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Extract 3-5 key search terms from the text.
{format_instructions}

IMPORTANT:
1. The terms field is required and must be an array of strings
2. Each term should be specific and focused on a single concept
3. Terms should be meaningful for web search
4. Do not include any text before or after the JSON
5. Use proper JSON formatting with double quotes

Example response:
{{
    "terms": [
        "machine learning algorithms",
        "neural network architectures",
        "deep learning frameworks"
    ]
}}"""

    human_template = """Extract key search terms from this text:

{text}

Remember:
1. Return 3-5 specific, focused terms
2. Make terms suitable for web search
3. Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt
```

.\prompts\knowledge_acquisition\join_decision.py
```python
"""Join decision prompts."""
from typing import Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class JoinDecision(BaseModel):
    """Schema for join decisions"""
    complete: bool = Field(description="Whether execution is complete")
    thought: str = Field(description="Joiner's reasoning")
    replan: bool = Field(description="Whether replanning is needed")
    feedback: Optional[str] = Field(None, description="Feedback for replanning if needed")

def get_join_decision_prompt() -> ChatPromptTemplate:
    """Get the join decision prompt template."""
    parser = PydanticOutputParser(pydantic_object=JoinDecision)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Analyze the results and decide whether to complete or replan.
{format_instructions}

IMPORTANT:
1. All fields are required except feedback
2. complete and replan must be boolean values
3. thought must explain your decision
4. feedback is optional and only needed for replanning
5. Do not include any text before or after the JSON
6. Use proper JSON formatting with double quotes"""

    human_template = """Analyze these results:

Plan: {plan}
Results: {results}

Decide whether to:
1. Complete - if all tasks succeeded
2. Replan - if some tasks failed but can be retried
3. Fail - if critical tasks failed

Output ONLY a valid JSON object following the format instructions."""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]) 
```

.\prompts\knowledge_acquisition\metadata_generation.py
```python
"""Metadata generation prompts."""
from typing import Dict, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class MetadataResponse(BaseModel):
    """Schema for metadata generation response"""
    metadata: Dict[str, Any] = Field(description="Generated metadata")

def get_metadata_generation_prompt() -> ChatPromptTemplate:
    """Get the metadata generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=MetadataResponse)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are an expert at generating metadata for content.
{format_instructions}

CRITICAL RULES:
1. You MUST output ONLY a valid JSON object
2. The JSON MUST match the schema exactly
3. The metadata field MUST be an object with specific fields
4. All strings MUST be properly escaped if they contain special characters
5. Do not include any text before or after the JSON object
6. Do not include any explanations or notes
7. The response should look exactly like this:
{{
    "metadata": {{
        "source_type": "text",
        "confidence_score": 0.85,
        "domain_relevance": 0.9,
        "timestamp": "2024-02-09T11:42:32.000Z",
        "validation_status": "processed",
        "domain": "machine_learning"
    }}
}}

GUIDELINES for metadata generation:
1. Analyze content to determine:
   - Source type (text, pdf, web)
   - Domain relevance
   - Confidence in reliability
   - Validation status
   - Domain classification

2. Consider factors like:
   - Content quality and coherence
   - Technical depth
   - Citation of sources
   - Author expertise
   - Publication context

3. Ensure metadata is:
   - Accurate and objective
   - Well-structured
   - Complete
   - Consistent"""

    human_template = """Generate metadata for this text:

{text}

Remember:
1. Return ONLY a valid JSON object
2. Include all required metadata fields
3. Use proper JSON formatting
4. Do not include any text before or after the JSON"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt 
```

.\prompts\knowledge_acquisition\plan_generation.py
```python
"""Plan generation prompts."""
from typing import List, Dict, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class Task(BaseModel):
    """Schema for a task in the execution plan"""
    idx: int = Field(description="Task index")
    tool: str = Field(description="Tool to use")
    args: Dict[str, Any] = Field(description="Tool arguments")
    dependencies: List[int] = Field(default_factory=list, description="Task dependencies")

class Plan(BaseModel):
    """Schema for an execution plan"""
    tasks: List[Task] = Field(description="List of tasks to execute")
    thought: str = Field(description="Planner's reasoning")

def get_plan_generation_prompt() -> ChatPromptTemplate:
    """Get the plan generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Plan)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Generate a plan to acquire and process knowledge from the given content.
{format_instructions}

IMPORTANT: State variables MUST be referenced using {state[variable_name]} format, e.g. {state[domain_name]}.
NEVER use {state} directly - always use {state[variable_name]} for state variables.

Available tools:
- extract_knowledge: Extract structured knowledge from text
  args: {{"text": "{state[content]}"}}
- generate_embeddings: Generate embeddings for text
  args: {{"text": "{state[content]}"}}
- update_graph: Update knowledge graph
  args: {{"knowledge": {state[knowledge]}}}
- create_documents: Create final documents
  args: {{"content": {state[content]}, "metadata": {state[metadata]}}}

CRITICAL:
1. Each task must have a unique idx starting from 0
2. Dependencies must refer to valid task indices
3. Tool names must match exactly
4. All tasks must have required args
5. State variables must use {state[variable_name]} format
6. Never use {state} directly

Example response:
{{
    "tasks": [
        {{
            "idx": 0,
            "tool": "extract_knowledge",
            "args": {{"text": "{state[content]}"}},
            "dependencies": []
        }},
        {{
            "idx": 1,
            "tool": "generate_embeddings",
            "args": {{"text": "{state[content]}"}},
            "dependencies": [0]
        }}
    ],
    "thought": "First extract knowledge from content, then generate embeddings"
}}"""

    human_template = """Current state:
{state}

Generate a plan to process the content based on the current state.

Remember:
1. Return ONLY a valid JSON object
2. Include both tasks and thought fields
3. Use {state[variable_name]} format for state variables
4. Make task idx start from 0 and increment sequentially
5. Ensure dependencies refer to valid task IDs
6. Include a clear thought explaining your plan
7. NEVER use {state} directly"""

    messages = [
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]

    return ChatPromptTemplate(messages=messages) 
```

.\prompts\knowledge_acquisition\query_generation.py
```python
"""Query generation prompts."""
from typing import List, Tuple
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class QueryGroup(BaseModel):
    """Group of related search queries"""
    group_name: str = Field(description="Name of the query group")
    queries: List[str] = Field(description="List of search queries in this group")
    strategy: str = Field(description="Search strategy/approach for this group")

class QueryGenerationResponse(BaseModel):
    """Response for query generation"""
    query_groups: List[QueryGroup] = Field(description="Generated query groups")
    reasoning: str = Field(description="Explanation of query generation strategy")

def get_query_generation_prompt() -> ChatPromptTemplate:
    """Get the query generation prompt template."""
    system_template = """You are an expert at generating diverse and effective search queries for academic and medical research.
Given a base query and domain, generate groups of related search queries that will help gather comprehensive information.

{format_instructions}

CRITICAL RULES:
1. You MUST output ONLY a valid JSON object
2. The JSON MUST match the schema exactly
3. Each query group must have 3-5 unique queries
4. Queries must be specific and targeted
5. No duplicate queries across groups
6. Each query must be clear and well-formed
7. Use proper medical/scientific terminology
8. Include different search strategies per group

Query Group Types to Generate:
1. Overview & Background:
   - Current understanding
   - Historical context
   - Key concepts
   
2. Clinical Research:
   - Clinical trials
   - Patient outcomes
   - Treatment efficacy
   
3. Mechanisms & Pathways:
   - Biological mechanisms
   - Molecular pathways
   - Physiological processes
   
4. Treatment Approaches:
   - Therapeutic strategies
   - Intervention methods
   - Treatment protocols
   
5. Reviews & Meta-analyses:
   - Systematic reviews
   - Meta-analyses
   - Literature reviews

6. Latest Developments:
   - Recent advances
   - New findings
   - Emerging research

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text.
DO NOT include ```json or ``` markers.
DO NOT include any explanatory text.
ENSURE all JSON is properly escaped and formatted."""

    human_template = """Generate diverse search queries for this topic:

Base Query: {base_query}
Domain: {domain}

Remember to:
1. Generate unique, specific queries
2. Use proper terminology
3. Cover different aspects
4. Ensure queries are well-formed
5. Include various search strategies"""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]) 
```

.\prompts\knowledge_acquisition\relationship_extraction.py
```python
"""Relationship extraction prompts."""
from typing import List, Dict, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class RelationshipResponse(BaseModel):
    """Schema for relationship extraction response"""
    relationships: List[Dict[str, str]] = Field(description="Extracted relationships")

def get_relationship_extraction_prompt() -> ChatPromptTemplate:
    """Get the relationship extraction prompt template."""
    parser = PydanticOutputParser(pydantic_object=RelationshipResponse)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are an expert at extracting medical and therapeutic relationships from scientific text.
{format_instructions}

CRITICAL RULES:
1. You MUST output ONLY a valid JSON object
2. The JSON MUST match the schema exactly
3. The relationships field MUST be an array of objects
4. Each relationship object MUST have exactly these fields:
   - source: string (medical entity)
   - relation: string (one of the valid types)
   - target: string (medical entity)
5. All strings MUST be properly escaped if they contain special characters
6. Do not include any text before or after the JSON object
7. Focus on medical/therapeutic relationships only

VALID RELATIONSHIP TYPES:
1. Medical Relationships:
   - treats (for treatment relationships)
   - causes (for causative relationships)
   - prevents (for preventive relationships)
   - affects (for impact relationships)
   - regulates (for regulatory relationships)
   - part_of (for anatomical relationships)
   - interacts_with (for drug/therapy interactions)

2. Therapeutic Relationships:
   - improves (for therapeutic benefits)
   - reduces (for symptom reduction)
   - increases (for enhancement effects)
   - modulates (for biological modulation)
   - targets (for therapeutic targeting)

3. Research Relationships:
   - studied_in (for research context)
   - measured_by (for assessment methods)
   - associated_with (for correlations)
   - supported_by (for evidence basis)
   - compared_to (for comparative studies)

4. Mechanism Relationships:
   - activates (for activation pathways)
   - inhibits (for inhibitory effects)
   - mediates (for mediating processes)
   - signals_through (for signaling pathways)
   - binds_to (for molecular binding)

RELATIONSHIP VALIDATION RULES:
1. Medical Validity:
   - Both source and target must be valid medical entities
   - Relationship must be supported by medical literature
   - Direction of relationship must be biologically plausible

2. Therapeutic Relevance:
   - Focus on relationships relevant to treatment
   - Include mechanism of action where possible
   - Consider patient safety and outcomes

3. Evidence Quality:
   - Prefer relationships from clinical studies
   - Note strength of evidence
   - Consider replication status

4. Domain Specificity:
   - Focus on PANDAS-relevant relationships
   - Include gut-brain axis connections
   - Consider vagus nerve pathways
   - Include plant-based therapeutic mechanisms"""

    human_template = """Extract medical and therapeutic relationships from this text:

{content}

Remember:
1. Return ONLY a valid JSON object
2. Focus on PANDAS, gut-brain axis, and therapeutic relationships
3. Include mechanism of action where possible
4. Validate relationships against medical knowledge
5. Consider evidence quality"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt
```

.\prompts\knowledge_acquisition\task_execution.py
```python
"""Task execution prompts."""
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class TaskResult(BaseModel):
    """Schema for task execution results."""
    task_id: int = Field(..., description="Task ID")
    result: Optional[Dict[str, Any]] = Field(None, description="Task result")
    error: Optional[str] = Field(None, description="Error if any")

def get_task_execution_prompt() -> ChatPromptTemplate:
    """Get the task execution prompt template."""
    parser = PydanticOutputParser(pydantic_object=TaskResult)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Execute knowledge acquisition tasks.
{format_instructions}

Tool-specific result formats:
1. research_topics:
   "result": {{
     "knowledge_sources": [
       {{
         "content": <string>,
         "metadata": {{
           "source_type": "text",
           "confidence": <float>,
           "timestamp": <string in ISO format>
         }}
       }}
     ],
     "thought": <string explaining the research process>
   }}

2. synthesize_knowledge:
   "result": {{
     "synthetic_knowledge": [
       {{
         "content": <string>,
         "patterns": [<object>],
         "hypotheses": [<object>],
         "relationships": [<object>],
         "confidence": <float>,
         "metadata": <object>
       }}
     ],
     "thought": <string explaining the synthesis process>
   }}

3. generate_examples:
   "result": {{
     "training_examples": [
       {{
         "input_text": <string>,
         "output_text": <string>,
         "metadata": <object>,
         "quality_score": <float>
       }}
     ],
     "thought": <string explaining the example generation process>
   }}

4. train_model:
   "result": {{
     "model_metrics": {{
       "loss": <float>,
       "eval_loss": <float>,
       "train_samples": <integer>,
       "eval_samples": <integer>,
       "training_time": <float>
     }},
     "thought": <string explaining the training process>
   }}

CRITICAL FORMATTING RULES:
1. Use ONLY double quotes (") for strings and property names
2. Arrays must be comma-separated and enclosed in square brackets []
3. Objects must be comma-separated and enclosed in curly braces {{}}
4. No trailing commas after the last item in arrays or objects
5. No comments or explanatory text
6. No JavaScript/Python syntax - ONLY valid JSON
7. No extra fields or properties beyond what is specified
8. No malformed JSON or syntax errors
9. No single quotes (') - use double quotes (") only
10. No unescaped newlines in strings
11. No extra whitespace or indentation
12. No extra quotes around the entire JSON object
13. No extra quotes around individual fields
14. No extra quotes around arrays or objects
15. ALWAYS include both task_id and result fields
16. NEVER return just a task_id without a result
17. If execution fails, set result=null and include error message
18. task_id MUST match the idx field from the task being executed
19. Dependencies MUST be checked - if any dependency task failed, this task should fail with "Dependencies not met"
20. The result field MUST match the tool-specific format exactly
21. The result field MUST be null if there is an error
22. The error field MUST be null if there is a result
23. NEVER include any text before or after the JSON object
24. NEVER include any comments or explanations
25. NEVER include any extra fields or properties
26. ALWAYS include a thought field in the result object explaining your reasoning
27. State variables in args (e.g. {{state.domain_name}}) will be replaced with actual values

Example valid response for success:
{{
  "task_id": 0,
  "result": {{
    "knowledge_sources": [
      {{
        "content": "Example source content",
        "metadata": {{
          "source_type": "text",
          "confidence": 0.95,
          "timestamp": "2024-02-07T12:00:00Z"
        }}
      }}
    ],
    "thought": "Successfully researched topics and extracted knowledge"
  }},
  "error": null
}}

Example valid response for failure:
{{
  "task_id": 1,
  "result": null,
  "error": "Dependencies not met - task 0 failed"
}}"""

    human_template = """Execute this task:

Task:
{{
  "idx": {task.idx},
  "tool": "{task.tool}",
  "args": {task.args},
  "dependencies": {task.dependencies}
}}

Remember:
1. Return ONLY valid JSON with the EXACT structure shown above
2. No text before or after the JSON
3. No explanation, just the JSON object
4. Always include all required fields
5. Set error=null for successful execution
6. Follow the tool-specific result format exactly
7. NEVER return just a task_id without a result
8. task_id MUST match the idx field from the task being executed
9. Dependencies MUST be checked - if any dependency task failed, this task should fail with "Dependencies not met"
10. The result field MUST match the tool-specific format exactly
11. The result field MUST be null if there is an error
12. The error field MUST be null if there is a result
13. NEVER include any text before or after the JSON object
14. NEVER include any comments or explanations
15. NEVER include any extra fields or properties
16. ALWAYS include a thought field in the result object
17. State variables in args will be replaced with actual values"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt 
```

.\prompts\lora\training_config.py
```python
"""LoRA training configuration prompt."""

from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class TrainingConfig(BaseModel):
    """Schema for LoRA training configuration."""
    model_name: str = Field(..., description="Base model name")
    r: int = Field(..., description="LoRA attention dimension", ge=1, le=256)
    lora_alpha: int = Field(..., description="LoRA alpha parameter", ge=1)
    target_modules: List[str] = Field(..., description="Target modules for LoRA")
    lora_dropout: float = Field(..., description="LoRA dropout rate", ge=0.0, le=1.0)
    bias: str = Field(..., description="LoRA bias type", pattern="^(none|all|lora_only)$")
    learning_rate: float = Field(..., description="Learning rate", gt=0.0)
    num_epochs: int = Field(..., description="Number of training epochs", ge=1)
    batch_size: int = Field(..., description="Training batch size", ge=1)

def get_training_config_prompt() -> ChatPromptTemplate:
    """Get the prompt template for generating LoRA training configuration."""
    parser = PydanticOutputParser(pydantic_object=TrainingConfig)
    
    system_template = """You are a LoRA training expert that helps configure training parameters.
Your configurations should be optimized for the specific task while being computationally efficient.

{format_instructions}

Remember to:
1. Choose appropriate model for the task
2. Set reasonable LoRA parameters
3. Configure training hyperparameters
4. Consider computational constraints"""

    human_template = """Please generate a LoRA training configuration for this task:
{task}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt 
```

.\prompts\qa\answer_generation.py
```python
"""Answer generation prompts."""
from typing import List, Literal
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class Answer(BaseModel):
    """Schema for generated answers"""
    answer: str = Field(description="Your detailed answer")
    sources: List[str] = Field(description="List of sources used")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    reasoning: str = Field(description="Explanation of how you arrived at the answer")
    validation_status: Literal["pending", "validated", "failed"] = Field(description="Validation status")

def get_answer_generation_prompt() -> ChatPromptTemplate:
    """Get the answer generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Answer)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are an answer generation expert. Generate comprehensive answers based on the provided context.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The confidence field must be a number between 0.0 and 1.0
3. Validation status must be one of: pending, validated, failed
4. Provide detailed reasoning to support your answer"""

    human_template = """Answer this question based on the provided context:

Question: {question}
Context: {context}

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\qa\join_decision.py
```python
"""QA join decision prompts."""

from typing import Optional, Tuple
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class JoinDecision(BaseModel):
    """Schema for join decisions"""
    complete: bool = Field(description="Whether execution is complete")
    thought: str = Field(description="Joiner's reasoning")
    replan: bool = Field(description="Whether replanning is needed")
    feedback: Optional[str] = Field(None, description="Feedback for replanning if needed")

def get_join_decision_prompt() -> Tuple[ChatPromptTemplate, PydanticOutputParser]:
    """Get the join decision prompt template and parser."""
    parser = PydanticOutputParser(pydantic_object=JoinDecision)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Analyze the results and decide whether to complete or replan.
{format_instructions}

IMPORTANT:
1. All fields are required except feedback
2. complete and replan must be boolean values
3. thought must explain your decision
4. feedback is optional and only needed for replanning
5. Do not include any text before or after the JSON
6. Use proper JSON formatting with double quotes"""

    human_template = """Analyze these results:

Plan: {plan}
Results: {results}

Decide whether to:
1. Complete - if all tasks succeeded
2. Replan - if some tasks failed but can be retried
3. Fail - if critical tasks failed

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    return prompt, parser 
```

.\prompts\qa\knowledge_gap.py
```python
"""Knowledge gap identification prompts."""
from typing import List
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class KnowledgeGap(BaseModel):
    """Schema for identified knowledge gaps"""
    topic: str = Field(description="Topic where knowledge is missing")
    context: str = Field(description="Context around the knowledge gap")
    priority: float = Field(description="Priority score between 0.0 and 1.0", ge=0.0, le=1.0)
    suggested_questions: List[str] = Field(description="Questions to fill the gap")

def get_knowledge_gap_prompt() -> ChatPromptTemplate:
    """Get the knowledge gap identification prompt template."""
    parser = PydanticOutputParser(pydantic_object=KnowledgeGap)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a knowledge gap identification expert. Identify gaps in knowledge based on questions and answers.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The priority field must be a number between 0.0 and 1.0
3. Suggested questions should help fill identified knowledge gaps"""

    human_template = """Identify knowledge gaps based on:

Question: {question}
Answer: {answer}
Confidence: {confidence}
Context: {context}

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\qa\qa_planning.py
```python
"""QA planning prompts."""

from typing import List, Dict, Any, Tuple
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class Plan(BaseModel):
    """Schema for QA plan"""
    tasks: List[Dict[str, Any]] = Field(description="List of tasks to execute")
    thought: str = Field(description="Explanation of the plan")

def get_qa_plan_prompt() -> Tuple[ChatPromptTemplate, PydanticOutputParser]:
    """Get the QA plan generation prompt template and parser."""
    parser = PydanticOutputParser(pydantic_object=Plan)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Generate a plan to answer the question.
{{{{format_instructions}}}}

IMPORTANT:
1. All fields are required
2. tasks must be an array of task objects with:
   - idx: unique integer index
   - tool: one of [retrieve_context, generate_answer, validate_answer, identify_gaps]
   - args: object with required arguments
   - dependencies: array of task indices this depends on
3. thought must explain your planning reasoning
4. Do not include any text before or after the JSON
5. Use proper JSON formatting with double quotes

Available tools:
- retrieve_context: Retrieve relevant context from knowledge base
- generate_answer: Generate answer using context
- validate_answer: Validate generated answer
- identify_gaps: Identify knowledge gaps"""

    human_template = """Generate a plan to answer this question:

{question}

Remember:
1. Each task must have a unique idx
2. Dependencies must refer to valid task indices
3. Tool names must match exactly
4. All tasks must have required args

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    return prompt, parser 
```

.\prompts\qa\qa_prompt.py
```python
"""Question answering prompt for answering questions with context."""

from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class QAResponse(BaseModel):
    """Response from QA system."""
    answer: str = Field(description="The answer text")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    sources: List[str] = Field(default_factory=list, description="Source documents used")
    reasoning: Optional[str] = Field(None, description="Reasoning behind the answer")

def get_qa_prompt() -> ChatPromptTemplate:
    """Get the prompt template for question answering."""
    parser = PydanticOutputParser(pydantic_object=QAResponse)
    
    system_template = """You are a question answering assistant that helps answer questions using provided context.
Your responses should be accurate and well-supported by the context.

{format_instructions}

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text or explanation.
DO NOT wrap the JSON in markdown code blocks or any other formatting.
DO NOT include ```json or ``` markers.
DO NOT include any text before or after the JSON.

Your response should look exactly like this:
{
    "answer": "The detailed answer to the question",
    "confidence": 0.8,
    "sources": ["Source 1", "Source 2"],
    "reasoning": "Clear explanation of how the answer was derived from the context"
}

Remember to:
1. Use only information from the provided context
2. Cite specific sources from the context
3. Explain your reasoning clearly
4. Express confidence based on context relevance
5. Always include all required fields in the JSON response
6. Use proper JSON formatting with double quotes
7. Ensure all strings are properly escaped"""

    human_template = """Please answer this question using the provided context:

Question: {question}

Context: {context}

Remember to respond with ONLY a valid JSON object."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt 
```

.\prompts\qa\qa_prompts.py
```python
"""Question answering prompts."""
from typing import List, Dict, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class QAResponse(BaseModel):
    """Schema for QA responses"""
    answer: str = Field(description="Detailed answer to the question")
    sources: List[str] = Field(description="List of sources used")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    reasoning: str = Field(description="Explanation of reasoning")

def get_qa_prompt() -> ChatPromptTemplate:
    """Get the QA prompt template."""
    parser = PydanticOutputParser(pydantic_object=QAResponse)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Answer the given question based on the provided context. Return in JSON format.
{format_instructions}

IMPORTANT:
1. All fields are required
2. answer must be detailed and comprehensive
3. sources must list all used references
4. confidence must be between 0.0 and 1.0
5. reasoning must explain your thought process
6. Do not include any text before or after the JSON
7. Use proper JSON formatting with double quotes

Example response:
{{
    "answer": "Paris is the capital of France",
    "sources": ["World Geography Database"],
    "confidence": 0.95,
    "reasoning": "This is a well-established geographical fact"
}}"""

    human_template = """Answer this question:

Question: {question}
Context: {context}

Focus on:
1. Providing a detailed answer
2. Citing relevant sources
3. Explaining your reasoning
4. Assessing your confidence

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    prompt = prompt.partial(format_instructions=format_instructions)
    return prompt 
```

.\prompts\qa\question_answering.py
```python
"""Question answering system prompts."""
from typing import List, Literal
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class Answer(BaseModel):
    """Schema for generated answers"""
    answer: str = Field(description="Detailed answer to the question")
    sources: List[str] = Field(description="List of sources used")
    confidence: float = Field(description="Confidence in the answer", ge=0.0, le=1.0)
    reasoning: str = Field(description="Explanation of how the answer was derived")
    validation_status: str = Field(description="Must be one of: pending, validated, failed")

class KnowledgeGap(BaseModel):
    """Schema for knowledge gaps"""
    topic: str = Field(description="Topic where knowledge is missing")
    context: str = Field(description="Context around the knowledge gap")
    priority: float = Field(description="Priority score for filling this gap", ge=0.0, le=1.0)
    suggested_questions: List[str] = Field(description="Questions to help fill the gap")

class Question(BaseModel):
    """Schema for generated questions"""
    question: str = Field(description="The generated question")
    topic: str = Field(description="The topic this question relates to")
    difficulty: float = Field(description="Difficulty score between 0.0 and 1.0", ge=0.0, le=1.0)
    type: Literal["general", "factual", "conceptual", "analytical", "error"] = Field(description="Type of question")
    context: str = Field(description="Context that prompted this question")

def get_answer_generation_prompt() -> ChatPromptTemplate:
    """Get the answer generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Answer)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are an answer generation expert. Generate comprehensive answers based on the provided context.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The confidence field must be a number between 0.0 and 1.0
3. validation_status must be one of: pending, validated, failed
4. Provide detailed reasoning to support your answer"""

    human_template = """Answer this question based on the provided context:

Question: {question}
Context: {context}"""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

def get_knowledge_gap_prompt() -> ChatPromptTemplate:
    """Get the knowledge gap prompt template."""
    parser = PydanticOutputParser(pydantic_object=KnowledgeGap)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a knowledge gap identification expert. Identify gaps in knowledge based on questions and answers.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The priority field must be a number between 0.0 and 1.0
3. Suggested questions should help fill identified knowledge gaps"""

    human_template = """Identify knowledge gaps based on this Q&A:

Question: {question}
Answer: {answer}
Context: {context}"""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

def get_question_generation_prompt() -> ChatPromptTemplate:
    """Get the question generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Question)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a question generation expert. Generate specific, focused questions based on the content.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The difficulty field must be a number between 0.0 and 1.0
3. Question type must be one of: general, factual, conceptual, analytical, error
4. Generate questions that test understanding and critical thinking"""

    human_template = """Answer this question based on the provided context:

Question: {question}
Context: {context}

Output ONLY a valid JSON object following the format instructions."""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]) 
```

.\prompts\qa\question_generation.py
```python
"""Question generation prompts."""
from typing import List, Literal
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate

class Question(BaseModel):
    """Question model."""
    question: str = Field(description="The question text")
    type: Literal["knowledge_recall", "concept_application", "analysis", "problem_solving", "critical_thinking"] = Field(description="Type of question")
    difficulty: float = Field(description="Difficulty score between 0 and 1", ge=0.0, le=1.0)
    topic: str = Field(description="Topic of the question")
    expected_answer: str = Field(description="Expected answer to the question")
    skills_tested: List[str] = Field(description="Skills being tested by the question")

class QuestionList(BaseModel):
    """List of questions."""
    questions: List[Question] = Field(description="List of generated questions")

def get_question_generation_prompt() -> ChatPromptTemplate:
    """Get question generation prompt."""
    return ChatPromptTemplate.from_messages([
        ("system", """You are an expert at generating high-quality questions and answers for knowledge assessment.
Given a topic and context, generate diverse questions that test different aspects of understanding.
Each question must have a clear, comprehensive answer that directly addresses the question.

{format_instructions}

CRITICAL RULES:
1. Question Types (MUST use EXACTLY these types with underscores, not hyphens):
   - "knowledge_recall": Testing basic understanding and recall
   - "concept_application": Testing application of concepts
   - "analysis": Testing analysis and breakdown
   - "problem_solving": Testing practical problem solving (use underscore, NOT hyphen)
   - "critical_thinking": Testing evaluation and synthesis

   IMPORTANT: Always use underscores (_) not hyphens (-) in type names.
   Example: "problem_solving" is correct, "problem-solving" is WRONG.

2. Question Distribution:
   - Aim for balanced mix of types
   - No more than 30% of one type
   - Include at least one of each type

3. Question Quality:
   - Questions must be clear and specific
   - No yes/no questions
   - Use proper grammar and punctuation
   - Include context when needed
   - Questions should be answerable from the provided context

4. Answer Quality:
   - Answers must directly address the question
   - Include explanations and reasoning
   - Use specific examples from context
   - Be comprehensive but concise
   - Include relevant details and evidence

5. Difficulty Levels:
   - Basic (0.0-0.3): Simple recall and understanding
   - Intermediate (0.3-0.7): Application and analysis
   - Advanced (0.7-1.0): Synthesis and evaluation
   - Distribute evenly across levels

6. Skills Coverage:
   Each question should test 2-3 skills from:
   - Comprehension
   - Application
   - Analysis
   - Synthesis
   - Evaluation
   - Problem Solving
   - Critical Thinking
   - Pattern Recognition

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text.
DO NOT include ```json or ``` markers.
DO NOT include any explanatory text.
ENSURE all JSON is properly escaped and formatted."""),
        ("human", "Generate {num_questions} questions about this topic:\n{topic}\n\nContext:\n{context}")
    ]) 
```

.\prompts\reformulation\reformulation_prompts.py
```python
"""Reformulation prompts for reformulating conversation outputs."""

from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class ReformulatedAnswer(BaseModel):
    """Schema for reformulated answer"""
    answer: str = Field(description="The reformulated answer")
    confidence: float = Field(description="Confidence in the reformulation", ge=0.0, le=1.0)
    reasoning: str = Field(description="Reasoning behind the reformulation")

def get_reformulation_prompt() -> tuple[ChatPromptTemplate, PydanticOutputParser]:
    """Get the reformulation prompt template and parser."""
    parser = PydanticOutputParser(pydantic_object=ReformulatedAnswer)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a reformulation expert that converts conversation outputs into clear, concise answers.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The answer field must be as concise as possible while being complete
3. The confidence field must be between 0.0 and 1.0
4. The reasoning field must explain your reformulation process
5. Format numbers and lists according to the original request
6. Do not include units unless specifically requested
7. Do not use articles or abbreviations unless specified
8. Do not include final punctuation

You must respond in the following format:
{{"answer": "your concise answer here",
  "confidence": 0.95,
  "reasoning": "your reasoning here"}}"""

    human_template = """Earlier you were asked:
{original_task}

Here is the conversation transcript:
{conversation}

Reformulate a clear, concise answer following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    return prompt, parser 
```

.\prompts\synthetic_knowledge\generation.py
```python
"""Synthetic knowledge generation prompts."""
from typing import List, Dict, Literal
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class Pattern(BaseModel):
    """Schema for recognized patterns"""
    pattern_type: str = Field(description="Type of pattern")
    description: str = Field(description="Detailed description")
    supporting_evidence: List[str] = Field(description="Specific evidence")
    confidence: float = Field(description="Confidence in pattern", ge=0.0, le=1.0)

class Hypothesis(BaseModel):
    """Schema for generated hypotheses"""
    statement: str = Field(description="Clear hypothesis statement")
    reasoning: str = Field(description="Detailed reasoning")
    evidence: List[str] = Field(description="Supporting evidence")
    confidence: float = Field(description="Confidence in hypothesis", ge=0.0, le=1.0)
    validation_status: Literal["pending", "processed", "failed"] = Field(description="Validation status")

class Relationship(BaseModel):
    """Schema for inferred relationships"""
    source: str = Field(description="Source entity")
    relation: Literal["is_a", "has_part", "related_to"] = Field(description="Type of relationship")
    target: str = Field(description="Target entity")

class SyntheticKnowledge(BaseModel):
    """Schema for synthesized knowledge"""
    content: str = Field(description="Clear, comprehensive synthesis")
    patterns: List[Pattern] = Field(description="Recognized patterns")
    hypotheses: List[Hypothesis] = Field(description="Generated hypotheses")
    relationships: List[Relationship] = Field(description="Inferred relationships")
    confidence: float = Field(description="Overall confidence", ge=0.0, le=1.0)
    validation_status: Literal["pending", "processed", "failed"] = Field(description="Validation status")
    metadata: Dict = Field(description="Additional metadata")

def get_pattern_recognition_prompt() -> ChatPromptTemplate:
    """Get the pattern recognition prompt template."""
    parser = PydanticOutputParser(pydantic_object=Pattern)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a pattern recognition expert. Identify meaningful patterns in the data.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The confidence field must be a number between 0.0 and 1.0
3. Focus on identifying meaningful, non-obvious patterns
4. Provide specific evidence to support each pattern"""

    human_template = """Identify patterns in this data:

{data}"""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

def get_hypothesis_generation_prompt() -> ChatPromptTemplate:
    """Get the hypothesis generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Hypothesis)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a hypothesis generation expert. Generate meaningful hypotheses based on patterns.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The confidence field must be a number between 0.0 and 1.0
3. validation_status must be one of: pending, processed, failed
4. Focus on generating testable, meaningful hypotheses
5. Provide clear reasoning and evidence"""

    human_template = """Generate hypotheses based on these patterns:

{patterns}"""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

def get_relationship_inference_prompt() -> ChatPromptTemplate:
    """Get the relationship inference prompt template."""
    parser = PydanticOutputParser(pydantic_object=Relationship)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a relationship inference expert. Identify meaningful relationships between concepts.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The relation field MUST be EXACTLY one of: is_a, has_part, related_to
3. Focus on identifying meaningful relationships that reveal domain structure"""

    human_template = """Infer meaningful relationships from these hypotheses:

{hypotheses}

Focus on identifying relationships that reveal the domain's structure."""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

def get_knowledge_synthesis_prompt() -> ChatPromptTemplate:
    """Get the knowledge synthesis prompt template."""
    parser = PydanticOutputParser(pydantic_object=SyntheticKnowledge)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a knowledge synthesis expert. Synthesize coherent knowledge from patterns, hypotheses, and relationships.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. All confidence fields must be numbers between 0.0 and 1.0
3. validation_status must be one of: pending, processed, failed
4. All relationship types must be EXACTLY one of: is_a, has_part, related_to
5. Focus on synthesizing a coherent understanding of the domain"""

    human_template = """Synthesize knowledge from:

Patterns: {patterns}
Hypotheses: {hypotheses}
Relationships: {relationships}

Focus on creating a coherent understanding of the domain."""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]) 
```

.\prompts\synthetic_knowledge\hypothesis_generation.py
```python
"""Hypothesis generation prompts."""
from typing import List, Literal
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class Hypothesis(BaseModel):
    """Schema for generated hypotheses"""
    statement: str = Field(description="A clear, testable hypothesis statement")
    reasoning: str = Field(description="Detailed reasoning explaining why this hypothesis is plausible")
    evidence: List[str] = Field(description="Specific evidence points that support this hypothesis")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    validation_status: Literal["pending", "processed", "failed"] = Field(description="Validation status")

def get_hypothesis_generation_prompt() -> ChatPromptTemplate:
    """Get the hypothesis generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Hypothesis)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a hypothesis generation expert. Generate insightful hypotheses from patterns.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. Generate hypotheses that explain relationships, predict outcomes, or suggest underlying mechanisms
3. Evidence must include specific supporting points
4. Validation status must be one of: pending, processed, failed"""

    human_template = """Generate insightful hypotheses based on these patterns:

{patterns}

Focus on explaining relationships, predicting outcomes, or suggesting underlying mechanisms.
Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\synthetic_knowledge\join_decision.py
```python
"""Join decision prompts."""
from typing import Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class JoinDecision(BaseModel):
    """Schema for join decisions"""
    complete: bool = Field(description="Whether execution is complete")
    thought: str = Field(description="Joiner's reasoning")
    replan: bool = Field(description="Whether replanning is needed")
    feedback: Optional[str] = Field(None, description="Feedback for replanning")

def get_join_decision_prompt() -> ChatPromptTemplate:
    """Get the join decision prompt template."""
    parser = PydanticOutputParser(pydantic_object=JoinDecision)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a workflow decision expert. Analyze the results and decide whether to complete or replan.
{format_instructions}

IMPORTANT RULES:
1. All fields are required except feedback
2. Provide clear reasoning for your decision
3. Consider task dependencies and critical failures
4. Recommend replanning only when tasks can be retried"""

    human_template = """Analyze these results:

Plan: {plan}
Results: {results}

Decide whether to:
1. Complete - if all tasks succeeded
2. Replan - if some tasks failed but can be retried
3. Fail - if critical tasks failed

Output ONLY a valid JSON object following the format instructions."""

    return ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ]) 
```

.\prompts\synthetic_knowledge\knowledge_generation.py
```python
"""Knowledge generation prompts for synthetic knowledge creation."""

from typing import List, Dict, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class Pattern(BaseModel):
    """Schema for identified pattern."""
    name: str = Field(..., description="Name of the pattern")
    description: str = Field(..., description="Description of the pattern")
    examples: List[str] = Field(..., description="Examples of the pattern")
    confidence: float = Field(..., description="Confidence score between 0 and 1", ge=0, le=1)

class Hypothesis(BaseModel):
    """Schema for generated hypothesis."""
    statement: str = Field(..., description="The hypothesis statement")
    evidence: List[str] = Field(..., description="Supporting evidence")
    confidence: float = Field(..., description="Confidence score between 0 and 1", ge=0, le=1)

class Relationship(BaseModel):
    """Schema for inferred relationship."""
    source: str = Field(..., description="Source concept/entity")
    target: str = Field(..., description="Target concept/entity")
    type: str = Field(..., description="Type of relationship")
    description: str = Field(..., description="Description of the relationship")
    confidence: float = Field(..., description="Confidence score between 0 and 1", ge=0, le=1)

class SyntheticKnowledge(BaseModel):
    """Schema for synthetic knowledge."""
    patterns: List[Pattern] = Field(default_factory=list, description="Identified patterns")
    hypotheses: List[Hypothesis] = Field(default_factory=list, description="Generated hypotheses")
    relationships: List[Relationship] = Field(default_factory=list, description="Inferred relationships")
    summary: str = Field(..., description="Summary of the synthetic knowledge")

def get_pattern_recognition_prompt() -> ChatPromptTemplate:
    """Get the prompt template for pattern recognition."""
    parser = PydanticOutputParser(pydantic_object=Pattern)
    
    system_template = """You are a pattern recognition expert that identifies meaningful patterns in information.
Your responses should be detailed and well-supported with specific examples.

{format_instructions}

Remember to:
1. Look for recurring themes and structures
2. Identify both obvious and subtle patterns
3. Provide clear examples of each pattern
4. Express confidence based on evidence strength"""

    human_template = """Please identify patterns related to this topic:
{topic}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt

def get_hypothesis_generation_prompt() -> ChatPromptTemplate:
    """Get the prompt template for hypothesis generation."""
    parser = PydanticOutputParser(pydantic_object=Hypothesis)
    
    system_template = """You are a hypothesis generation expert that formulates testable explanations.
Your hypotheses should be clear, falsifiable, and supported by evidence.

{format_instructions}

Remember to:
1. State hypotheses clearly and concisely
2. Provide supporting evidence
3. Consider alternative explanations
4. Express confidence based on evidence strength"""

    human_template = """Please generate hypotheses about this topic:
{topic}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt

def get_relationship_inference_prompt() -> ChatPromptTemplate:
    """Get the prompt template for relationship inference."""
    parser = PydanticOutputParser(pydantic_object=Relationship)
    
    system_template = """You are a relationship inference expert that identifies connections between concepts.
Your inferences should be logical and well-supported with evidence.

{format_instructions}

Remember to:
1. Identify meaningful connections
2. Specify relationship types clearly
3. Explain the nature of relationships
4. Express confidence based on evidence strength"""

    human_template = """Please infer relationships related to this topic:
{topic}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt

def get_knowledge_generation_prompt() -> ChatPromptTemplate:
    """Get the prompt template for knowledge generation."""
    parser = PydanticOutputParser(pydantic_object=SyntheticKnowledge)
    
    system_template = """You are a knowledge synthesis expert that generates synthetic knowledge.
Your synthesis should combine patterns, hypotheses, and relationships into coherent knowledge.

{format_instructions}

Remember to:
1. Integrate identified patterns
2. Connect related hypotheses
3. Map relationships between concepts
4. Provide a clear knowledge summary"""

    human_template = """Please generate synthetic knowledge about this topic:
{topic}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt 
```

.\prompts\synthetic_knowledge\knowledge_synthesis.py
```python
"""Knowledge synthesis prompts."""
from typing import List, Literal
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

from .pattern_recognition import Pattern
from .hypothesis_generation import Hypothesis
from .relationship_inference import Relationship

class SourceMetadata(BaseModel):
    """Schema for source metadata"""
    source_type: Literal["text", "pdf", "web"] = Field(description="Type of source")
    confidence_score: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    domain_relevance: float = Field(description="Domain relevance score between 0.0 and 1.0", ge=0.0, le=1.0)
    timestamp: str = Field(description="ISO format timestamp with timezone")
    validation_status: Literal["pending", "processed", "failed"] = Field(description="Validation status")

class SyntheticKnowledge(BaseModel):
    """Schema for synthetic knowledge"""
    content: str = Field(description="A clear, comprehensive synthesis of the key insights and findings")
    patterns: List[Pattern] = Field(description="Identified patterns")
    hypotheses: List[Hypothesis] = Field(description="Generated hypotheses")
    relationships: List[Relationship] = Field(description="Inferred relationships")
    confidence: float = Field(description="Overall confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    validation_status: Literal["pending", "processed", "failed"] = Field(description="Validation status")
    metadata: SourceMetadata = Field(description="Source metadata")

def get_knowledge_synthesis_prompt() -> ChatPromptTemplate:
    """Get the knowledge synthesis prompt template."""
    parser = PydanticOutputParser(pydantic_object=SyntheticKnowledge)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a knowledge synthesis expert. Synthesize coherent knowledge from patterns, hypotheses, and relationships.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. All confidence fields must be numbers between 0.0 and 1.0
3. Validation status must be one of: pending, processed, failed
4. Source type must be one of: text, pdf, web
5. All relationship types must be EXACTLY one of: is_a, has_part, related_to
6. Timestamp must be in ISO format with timezone
7. All arrays must be non-empty
8. Focus on synthesizing a coherent understanding of the domain"""

    human_template = """Synthesize knowledge from:

Patterns: {patterns}
Hypotheses: {hypotheses}
Relationships: {relationships}

Focus on creating a coherent understanding of the domain.
Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\synthetic_knowledge\pattern_recognition.py
```python
"""Pattern recognition prompts."""
from typing import List
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class Pattern(BaseModel):
    """Schema for identified patterns"""
    pattern_type: str = Field(description="Type of pattern (e.g., trend, relationship, behavior, concept)")
    description: str = Field(description="Detailed description of the pattern and its significance")
    supporting_evidence: List[str] = Field(description="Specific examples and evidence from the text that support this pattern")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)

def get_pattern_recognition_prompt() -> ChatPromptTemplate:
    """Get the pattern recognition prompt template."""
    parser = PydanticOutputParser(pydantic_object=Pattern)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a pattern recognition expert. Identify meaningful patterns in text.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. Focus on identifying meaningful, high-level patterns that reveal insights about the domain
3. Supporting evidence must include specific examples from the text"""

    human_template = """Identify meaningful patterns in this text:

{content}

Focus on identifying patterns that reveal important insights about the domain.
Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\synthetic_knowledge\relationship_inference.py
```python
"""Relationship inference prompts."""
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class Relationship(BaseModel):
    """Schema for inferred relationships"""
    source: str = Field(description="Source entity or concept")
    relation: Literal["is_a", "has_part", "related_to"] = Field(description="Type of relationship")
    target: str = Field(description="Target entity or concept")

def get_relationship_inference_prompt() -> ChatPromptTemplate:
    """Get the relationship inference prompt template."""
    parser = PydanticOutputParser(pydantic_object=Relationship)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a relationship inference expert. Identify meaningful relationships between concepts.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. The relation field MUST be EXACTLY one of: is_a, has_part, related_to
3. Focus on identifying meaningful relationships that reveal domain structure"""

    human_template = """Infer meaningful relationships from these hypotheses:

{hypotheses}

Focus on identifying relationships that reveal the domain's structure.
Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\text_inspection\text_analysis.py
```python
"""Text analysis prompt for analyzing text content."""

from typing import List, Dict, Optional, Tuple
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class TextSegment(BaseModel):
    """Schema for text segment."""
    content: str = Field(..., description="The text content of the segment")
    start_char: int = Field(..., description="Starting character position")
    end_char: int = Field(..., description="Ending character position")
    metadata: Dict[str, str] = Field(default_factory=dict, description="Additional metadata about the segment")

class Entity(BaseModel):
    """Schema for named entity."""
    text: str = Field(..., description="The entity text")
    title: str = Field(..., description="The entity title/type")

class TextAnalysis(BaseModel):
    """Schema for text analysis results."""
    content: str = Field(..., description="The original text content")
    segments: List[TextSegment] = Field(..., description="Text segments identified")
    key_points: List[str] = Field(..., description="Key points extracted from text")
    entities: List[Entity] = Field(..., description="Named entities found in text")
    relationships: List[str] = Field(..., description="Relationships between concepts")
    summary: str = Field(..., description="Overall summary of the text")

def get_text_analysis_prompt() -> Tuple[ChatPromptTemplate, PydanticOutputParser]:
    """Get the prompt template and parser for text analysis."""
    parser = PydanticOutputParser(pydantic_object=TextAnalysis)
    
    system_template = """You are a text analysis assistant that helps analyze and extract structured information from text.
Your responses should be detailed and well-organized, capturing the key information and relationships in the text.

{format_instructions}

Remember to:
1. Break text into logical segments
2. Identify key points and themes
3. Extract named entities and concepts
4. Map relationships between concepts
5. Provide a concise summary"""

    human_template = """Please analyze this text:
{text}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt, parser 
```

.\prompts\text_inspection\text_inspector_prompts.py
```python
"""Text inspector prompts for text analysis workflow."""

from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class Plan(BaseModel):
    """Schema for text inspection plan"""
    tasks: List[Dict[str, Any]] = Field(description="List of tasks to execute")
    thought: str = Field(description="Explanation of the plan")

class JoinDecision(BaseModel):
    """Schema for join decisions"""
    complete: bool = Field(description="Whether execution is complete")
    thought: str = Field(description="Joiner's reasoning")
    replan: bool = Field(description="Whether replanning is needed")
    feedback: Optional[str] = Field(None, description="Feedback for replanning if needed")

def get_plan_generation_prompt() -> tuple[ChatPromptTemplate, PydanticOutputParser]:
    """Get the plan generation prompt template and parser."""
    parser = PydanticOutputParser(pydantic_object=Plan)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Generate a plan to analyze the given text.
{{{{format_instructions}}}}

IMPORTANT:
1. All fields are required
2. tasks must be an array of task objects with:
   - idx: unique integer index
   - tool: one of [analyze_text, identify_segments, extract_entities, identify_relationships]
   - args: object with required arguments
   - dependencies: array of task indices this depends on
3. thought must explain your planning reasoning
4. Do not include any text before or after the JSON
5. Use proper JSON formatting with double quotes"""

    human_template = """Generate a plan to analyze this text:

{{content}}

Remember:
1. Each task must have a unique idx
2. Dependencies must refer to valid task indices
3. Tool names must match exactly
4. All tasks must have required args

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    return prompt, parser

def get_join_decision_prompt() -> tuple[ChatPromptTemplate, PydanticOutputParser]:
    """Get the join decision prompt template and parser."""
    parser = PydanticOutputParser(pydantic_object=JoinDecision)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Analyze the results and decide whether to complete or replan.
{{{{format_instructions}}}}

IMPORTANT:
1. All fields are required except feedback
2. complete and replan must be boolean values
3. thought must explain your decision
4. feedback is optional and only needed for replanning
5. Do not include any text before or after the JSON
6. Use proper JSON formatting with double quotes"""

    human_template = """Analyze these results:

Plan: {{plan}}
Results: {{results}}

Decide whether to:
1. Complete - if all tasks succeeded
2. Replan - if some tasks failed but can be retried
3. Fail - if critical tasks failed

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])
    
    return prompt, parser 
```

.\prompts\visual_qa\element_detection.py
```python
"""Visual element detection prompts."""
from typing import Dict, Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class Region(BaseModel):
    """Schema for visual region"""
    x: int = Field(description="X coordinate")
    y: int = Field(description="Y coordinate")
    width: int = Field(description="Width of region")
    height: int = Field(description="Height of region")
    content: str = Field(description="Region content description")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)

class VisualElement(BaseModel):
    """Schema for detected visual elements"""
    element_type: str = Field(description="Type of visual element")
    description: str = Field(description="Detailed description of the element")
    attributes: Dict[str, str] = Field(description="Element attributes")
    region: Optional[Region] = Field(None, description="Region information")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)

def get_element_detection_prompt() -> ChatPromptTemplate:
    """Get the visual element detection prompt template."""
    parser = PydanticOutputParser(pydantic_object=VisualElement)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a visual element detection expert. Detect and describe visual elements in the image.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. Confidence scores must be between 0.0 and 1.0
3. Region coordinates must be valid integers
4. Provide detailed descriptions of elements"""

    human_template = """Detect and describe visual elements in this image:

Image (base64): {{image}}

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\visual_qa\join_decision.py
```python
"""Visual QA join decision prompts."""

from typing import Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class JoinDecision(BaseModel):
    """Schema for join decisions"""
    complete: bool = Field(description="Whether execution is complete")
    thought: str = Field(description="Joiner's reasoning")
    replan: bool = Field(description="Whether replanning is needed")
    feedback: Optional[str] = Field(None, description="Feedback for replanning if needed")

def get_join_decision_prompt() -> ChatPromptTemplate:
    """Get the join decision prompt template."""
    parser = PydanticOutputParser(pydantic_object=JoinDecision)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Analyze the results and decide whether to complete or replan.
{format_instructions}

IMPORTANT RULES:
1. All fields are required except feedback
2. complete and replan must be boolean values
3. thought must explain your decision
4. feedback is optional and only needed for replanning
5. Do not include any text before or after the JSON"""

    human_template = """Analyze these results:

Plan: {plan}
Results: {results}

Decide whether to:
1. Complete - if all tasks succeeded
2. Replan - if some tasks failed but can be retried
3. Fail - if critical tasks failed

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\visual_qa\plan_generation.py
```python
"""Visual QA plan generation prompts."""

from typing import List, Dict, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class Plan(BaseModel):
    """Schema for visual QA plan"""
    tasks: List[Dict[str, Any]] = Field(description="List of tasks to execute")
    thought: str = Field(description="Explanation of the plan")

def get_plan_generation_prompt() -> ChatPromptTemplate:
    """Get the plan generation prompt template."""
    parser = PydanticOutputParser(pydantic_object=Plan)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Generate a plan to analyze the image and answer the question.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. tasks must be an array of task objects with:
   - idx: unique integer index
   - tool: one of [detect_elements, analyze_scene, answer_question]
   - args: object with required arguments
   - dependencies: array of task indices this depends on
3. thought must explain your planning reasoning
4. Do not include any text before or after the JSON

Available tools:
- detect_elements: Detect visual elements in image
- analyze_scene: Analyze scene composition
- answer_question: Answer specific question about image"""

    human_template = """Generate a plan to analyze this image and answer this question:

Image Path: {image_path}
Question: {question}

Remember:
1. Each task must have a unique idx
2. Dependencies must refer to valid task indices
3. Tool names must match exactly
4. All tasks must have required args

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\visual_qa\scene_analysis.py
```python
"""Scene analysis prompts."""
from typing import List, Dict
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class VisualAttributes(BaseModel):
    """Schema for visual attributes"""
    lighting: str = Field(description="Lighting description")
    composition: str = Field(description="Composition description")
    style: str = Field(description="Style description")

class SceneAnalysis(BaseModel):
    """Schema for scene analysis"""
    scene_description: str = Field(description="Overall description of the scene")
    key_objects: List[str] = Field(description="Important objects in scene")
    spatial_relationships: List[str] = Field(description="Relationships between objects")
    visual_attributes: VisualAttributes = Field(description="Visual attributes of the scene")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)

def get_scene_analysis_prompt() -> ChatPromptTemplate:
    """Get the scene analysis prompt template."""
    parser = PydanticOutputParser(pydantic_object=SceneAnalysis)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a scene analysis expert. Analyze the overall scene and relationships between elements.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. Confidence score must be between 0.0 and 1.0
3. Provide detailed descriptions of scene elements
4. Include all important spatial relationships"""

    human_template = """Analyze this scene:

Image (base64): {{image}}
Detected Elements: {{elements}}

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\visual_qa\visual_analysis.py
```python
"""Visual analysis prompt for analyzing images and answering questions."""

from typing import List, Optional
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import PydanticOutputParser

class VisualAnalysisResult(BaseModel):
    """Result of visual analysis."""
    answer: str = Field(..., description="The answer to the question")
    visual_evidence: List[str] = Field(..., description="Visual evidence supporting the answer")
    context: str = Field(..., description="Context and reasoning behind the answer")
    confidence: float = Field(..., description="Confidence score between 0 and 1", ge=0, le=1)

def get_visual_analysis_prompt() -> ChatPromptTemplate:
    """Get the prompt template for visual analysis."""
    parser = PydanticOutputParser(pydantic_object=VisualAnalysisResult)
    
    system_template = """You are a visual analysis assistant that helps analyze images and answer questions about them.
Your responses should be detailed and well-reasoned, supported by specific visual evidence from the image.

{format_instructions}

Remember to:
1. Carefully examine all visual details in the image
2. Provide specific visual evidence to support your answer
3. Explain your reasoning clearly
4. Express your confidence based on the clarity and completeness of the visual evidence"""

    human_template = """Here is an image encoded in base64:
{image}

Please analyze this image to answer the following question:
{question}"""

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ])

    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    return prompt 
```

.\prompts\visual_qa\visual_analysis_prompts.py
```python
"""Visual analysis prompts."""
from typing import List, Dict, Any
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage

class VisualAnalysisResult(BaseModel):
    """Schema for visual analysis results"""
    description: str = Field(description="Detailed description of the image")
    objects: List[str] = Field(description="List of identified objects")
    attributes: Dict[str, str] = Field(description="Visual attributes")
    relationships: List[str] = Field(description="Spatial relationships")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)

def get_visual_analysis_prompt() -> ChatPromptTemplate:
    """Get the visual analysis prompt template."""
    parser = PydanticOutputParser(pydantic_object=VisualAnalysisResult)
    format_instructions = parser.get_format_instructions()
    
    system_template = """Analyze the given image and provide a detailed description. Return in JSON format.
{{{{format_instructions}}}}

IMPORTANT:
1. All fields are required
2. description must be detailed and comprehensive
3. objects must be a non-empty array of identified objects
4. attributes must include visual properties like color, texture, lighting
5. relationships must describe spatial arrangements
6. confidence must be between 0.0 and 1.0
7. Do not include any text before or after the JSON
8. Use proper JSON formatting with double quotes"""

    human_template = """Analyze this image:

Image (base64): {{image}}
Question: {{question}}

Focus on:
1. Providing a detailed description
2. Identifying key objects and elements
3. Describing visual attributes
4. Explaining spatial relationships

Output ONLY a valid JSON object following the format instructions."""

    return ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        HumanMessage(content=human_template)
    ]) 
```

.\prompts\visual_qa\visual_qa.py
```python
"""Visual QA prompts."""
from typing import List
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class VisualAnswer(BaseModel):
    """Schema for visual QA answers"""
    answer: str = Field(description="Detailed answer to the question")
    visual_evidence: List[str] = Field(description="Visual evidence points")
    context: str = Field(description="Additional context if needed")
    confidence: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)

def get_visual_qa_prompt() -> ChatPromptTemplate:
    """Get the visual QA prompt template."""
    parser = PydanticOutputParser(pydantic_object=VisualAnswer)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a visual question answering expert. Answer questions about images based on analysis.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. Confidence score must be between 0.0 and 1.0
3. Provide detailed answers with visual evidence
4. Include relevant context from the scene"""

    human_template = """Answer this question about the image:

Question: {{question}}
Image (base64): {{image}}
Scene Description: {{scene_description}}
Key Objects: {{key_objects}}
Spatial Relationships: {{spatial_relationships}}

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\prompts\visual_qa\visual_qa_prompts.py
```python
"""Visual QA prompts for answering questions about images."""

from typing import List, Dict, Optional
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate

class VisualEvidence(BaseModel):
    """Schema for visual evidence"""
    region: Optional[Dict[str, int]] = Field(None, description="Region coordinates")
    description: str = Field(description="Description of the evidence")
    confidence: float = Field(description="Confidence in this evidence", ge=0.0, le=1.0)

class VisualAnswer(BaseModel):
    """Schema for visual QA answers"""
    answer: str = Field(description="Answer to the visual question")
    visual_evidence: List[VisualEvidence] = Field(description="Visual evidence supporting the answer")
    context: str = Field(description="Context from scene analysis")
    confidence: float = Field(description="Overall confidence in answer", ge=0.0, le=1.0)

def get_visual_qa_prompt() -> ChatPromptTemplate:
    """Get the visual QA prompt template."""
    parser = PydanticOutputParser(pydantic_object=VisualAnswer)
    format_instructions = parser.get_format_instructions()
    
    system_template = """You are a visual question answering expert. Answer questions about images using scene analysis.
{format_instructions}

IMPORTANT RULES:
1. All fields are required
2. Confidence scores must be between 0.0 and 1.0
3. Provide detailed visual evidence
4. Answer must be clear and concise
5. Context should explain reasoning"""

    human_template = """Answer this question about the image:

Question: {question}
Image (base64): {image}
Scene Description: {scene_description}
Key Objects: {key_objects}
Spatial Relationships: {spatial_relationships}

Output ONLY a valid JSON object following the format instructions."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_template),
        ("human", human_template)
    ])
    
    return prompt 
```

.\scripts\chat_langchain.py
```python
"""LangChain-based chat model implementation with structured output."""

from typing import Any, Dict, List, Optional, Type
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.outputs import ChatResult, ChatGeneration
from pydantic import BaseModel, Field, SecretStr
from langchain_google_genai import ChatGoogleGenerativeAI, HarmCategory, HarmBlockThreshold
import os

class ChatLangChain(BaseChatModel):
    """LangChain-based chat model with structured output support."""
    
    model: str = Field(default="gemini-1.5-flash")
    temperature: float = Field(default=0.1)
    api_key: SecretStr
    pydantic_schema: Optional[Type[BaseModel]] = Field(default=None)
    format: Optional[str] = Field(default=None)
    response_format: Optional[Dict[str, Any]] = Field(default=None)
    
    def __init__(self, **kwargs):
        """Initialize with API key and optional parameters."""
        super().__init__(**kwargs)
        
        # Initialize base model
        model_kwargs = {
            "model": self.model,
            "temperature": self.temperature,
            "api_key": SecretStr(self.api_key.get_secret_value()),
            "convert_system_message_to_human": False,  # Don't convert system messages
            "safety_settings": {
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE
            }
        }
        
        # Add format and response_format if specified
        if self.format:
            model_kwargs["format"] = self.format
            # If format is 'json', ensure response_format is set appropriately
            if self.format == 'json' and not self.response_format:
                model_kwargs["response_format"] = {"type": "json_object"}
        if self.response_format:
            model_kwargs["response_format"] = self.response_format
            
        self._model = ChatGoogleGenerativeAI(**model_kwargs)
        
        # If pydantic_schema is provided, bind it to the model and ensure JSON format
        if self.pydantic_schema:
            self._model = self._model.bind(functions=[self.pydantic_schema])
            # When using pydantic schema, always force JSON format
            if not self.format:
                self._model = self._model.bind(format='json')
                self._model = self._model.bind(response_format={"type": "json_object"})
    
    def _convert_messages(self, messages: List[BaseMessage]) -> List[BaseMessage]:
        """Convert messages to format expected by model."""
        converted = []
        for msg in messages:
            if isinstance(msg, SystemMessage):
                # For JSON format, ensure system message includes format requirement
                if self.format == 'json' or self.pydantic_schema:
                    content = msg.content
                    if "CRITICAL:" not in content:
                        content = f"{content}\n\nCRITICAL: You MUST respond with ONLY a valid JSON object. DO NOT include any text before or after the JSON. DO NOT wrap the JSON in code blocks."
                    converted.append(HumanMessage(content=f"System: {content}"))
                else:
                    converted.append(HumanMessage(content=f"System: {msg.content}"))
            else:
                converted.append(msg)
        return converted
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate chat response synchronously."""
        converted_messages = self._convert_messages(messages)
        response = self._model.invoke(converted_messages)
        return ChatResult(generations=[ChatGeneration(message=response)])
        
    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate chat response asynchronously."""
        converted_messages = self._convert_messages(messages)
        response = await self._model.ainvoke(converted_messages)
        return ChatResult(generations=[ChatGeneration(message=response)])
        
    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "chat-langchain"
        
    @classmethod
    def from_pydantic(cls, model: Type[BaseModel], **kwargs) -> "ChatLangChain":
        """Create ChatLangChain instance from a Pydantic model."""
        return cls(
            pydantic_schema=model,
            format='json',  # Always use JSON format with Pydantic models
            response_format={"type": "json_object"},
            **kwargs
        ) 
```

.\scripts\cookies.py
```python
from requests.cookies import RequestsCookieJar


COOKIES_LIST = [
    {
        "domain": ".youtube.com",
        "expirationDate": 1718884961,
        "hostOnly": False,
        "httpOnly": False,
        "name": "ST-xuwub9",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "session_logininfo=AFmmF2swRAIgf4gadACOuWOcipI1anW-dakEjtidNLkufnOC8uml7EECIDh2YisqWELDBJPTGUysCucJ3I0wjXxYjVHro1LHrdW0%3AQUQ3MjNmd2Jiajl3OWZYRnpFNnZlWWV5ZGJWZ0hpcmp4LVVPU280bk4zOS03Z0ozZG9fOFhWZ0dXaVo3NG1wTEg1b3hGaG10TFBlaFBnTlJfbER5bEp0aFhoNS1OLVhYNFRZT2F6ajgzOFpDbGhlUjZpMWRETlFFRjFfTTRiM0RnNTROSkdmMTFMVjFic1VuZ2trbGp4aktDa0JJUC1BWDh3",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753004444.745411,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-YEC",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "CgtRVnI5LW1zRHlQVSjbtNCzBjIhCgJGUhIbEhcSFRMLFBUWFwwYGRobHB0eHw4PIBAREiAk",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050824,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSID",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBB4ezJ_bdWu46a7YwObVn44wACgYKAakSARQSFQHGX2MicJcTzecTKH6bHzqU6TMbTxoVAUF8yKqQYK-MoI6Ql3vI2oYTB3E-0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974642,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SIDCC",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "AKEyXzWQZauHKOo8t87zoEcjaVNIYUX54ohoWXT-tX4aAhEuZzIIptxZAcNkHuG2oDXYL6t-lw",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050652,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBB6VHrZcC3gBAsFPbCQ0gF5AACgYKAYkSARQSFQHGX2Mi9kt0gHg5CxCYSkLQGHWaeBoVAUF8yKre_V6r3jZVak6JV4o2Q0FL0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420958.397534,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSIDTS",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "sidts-CjIB3EgAEkYL2L-GfrEzW5Dfy62S9oefGNLgst78S_986htCnGcfkxECch_9oz-qytSsZBAA",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753433494.44729,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_M0180HEFCY",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718871908.1.0.1718873494.0.0.0",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050933,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SAPISID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974764,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSIDCC",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AKEyXzWHDSoXGCZpZhPxRrnC7B1s8zGIUjeMVyvgtQfsm1fs92lXPtFEI_td9LBUyqVUe0xK",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050881,
        "hostOnly": False,
        "httpOnly": True,
        "name": "SSID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AmlwXHnQvOQ10LVd-",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050959,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__Secure-1PAPISID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050795,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBBrlk7lRpKQGywAHEon7WGQAACgYKAQsSARQSFQHGX2MirAmnSRdZl6GPG6KLd4hOihoVAUF8yKoV17Tcj1a_OenIOkf2wBjO0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050993,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__Secure-3PAPISID",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974815,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSIDCC",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AKEyXzXM5UjKUEXwSHVmRAIo6hGHA4G63adj3EE1VdNriD0f38jZQbsUKiD4LQbA3BValmTFDg",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420958.397647,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSIDTS",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "sidts-CjIB3EgAEkYL2L-GfrEzW5Dfy62S9oefGNLgst78S_986htCnGcfkxECch_9oz-qytSsZBAA",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050908,
        "hostOnly": False,
        "httpOnly": False,
        "name": "APISID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "IlQWLPjdNqziwCrV/ANG7Z4x5FF-IBxbZk",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050855,
        "hostOnly": False,
        "httpOnly": True,
        "name": "HSID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "AasA7hmRuTFv7vjoq",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753435873.577793,
        "hostOnly": False,
        "httpOnly": True,
        "name": "LOGIN_INFO",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AFmmF2swRAIgf4gadACOuWOcipI1anW-dakEjtidNLkufnOC8uml7EECIDh2YisqWELDBJPTGUysCucJ3I0wjXxYjVHro1LHrdW0:QUQ3MjNmd2Jiajl3OWZYRnpFNnZlWWV5ZGJWZ0hpcmp4LVVPU280bk4zOS03Z0ozZG9fOFhWZ0dXaVo3NG1wTEg1b3hGaG10TFBlaFBnTlJfbER5bEp0aFhoNS1OLVhYNFRZT2F6ajgzOFpDbGhlUjZpMWRETlFFRjFfTTRiM0RnNTROSkdmMTFMVjFic1VuZ2trbGp4aktDa0JJUC1BWDh3",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753444956.555608,
        "hostOnly": False,
        "httpOnly": False,
        "name": "PREF",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "f4=4000000&f6=40000000&tz=Europe.Paris&f5=30000&f7=100",
    },
]

COOKIES_LIST += [
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "isInstIp",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "False",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1734423981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__eoi",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "ID=c26f752377373146:T=1718871981:RT=1718884914:S=AA-AfjZw-T_OOX2kW2LLaFzXImgc",
    },
    {
        "domain": ".www.researchgate.net",
        "expirationDate": 1753444909.646103,
        "hostOnly": False,
        "httpOnly": True,
        "name": "ptc",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "RG1.8947708639250500550.1718872043",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750507578,
        "hostOnly": False,
        "httpOnly": False,
        "name": "euconsent-v2-didomi",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "CQAgmoAQAgmoAAHABBENA5EsAP_gAEPgAAYgJ2pB5G5UTWlBIG53YMskIAUFhFBoQEAgAACAAwIBSBIAIIwEAGAAIAgAICACAAIAIBIAIABAGAAAAAAAYIAAIAAIAAAQIAAKIAAAAAAAAgBQAAgIAgggEAAAgEBEABAAgAAAEIIAQNgACgAAACCAAAAAAAABAAAAAAAAQAAAAAAAYCQAAAJIAAAAACAIABAIAAAAAAAAAAAAAAAABBAAIJ2wPIAFAAXABQAFQALgAcAA8ACAAEgALwAZAA0ACIAEcAJgAUgAqgBcADEAGgAPQAfgBEACOAE4AMMAZYA0QBsgDkAHOAO4AfsBBwEIAItARwBHQC6gHUAO2Ae0A_4CHQEXgJ2AUOAo8BT4CpQFqALYAXmAwQBkgDLAGXANjAhCBG8CbAE3gJ1gTtAA.f_wACHwAAAAA",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718885236,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gat",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
    {
        "domain": "www.researchgate.net",
        "expirationDate": 1721477183,
        "hostOnly": True,
        "httpOnly": False,
        "name": "_pbjs_userid_consent_data",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "3524755945110770",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1752567981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__gads",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "ID=eca2adb88969c830:T=1718871981:RT=1718884914:S=ALNI_MY2qZchynrhWX6hWMlaI87Pcj9riQ",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718886709.646173,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__cf_bm",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "IkQ_J4ciBzKQduRvjqsfSmQu8UygDWbHeROO5JVccfo-1718884909-1.0.1.1-qvNGEdbfI0HfhFP6kwe7R7mkTqODNhFuKhs72lLly6K2BOPMG3kbahpQFGvPK0U8FUfkznkq65gngd1sWj7sDA",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1752567981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__gpi",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "UID=00000e4e9aa2e6f2:T=1718871981:RT=1718884914:S=ALNI_MYFNrgzkKn7K6Bd2y8hC6GJCvDiSg",
    },
    {
        "domain": ".researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "_cfuvid",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "_GPmGZkBymiH3UiqTqzakEpi98br3nfFUWC2_u_wqkc-1718884909785-0.0.1.1-604800000",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1753445177.271667,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.1.1525244793.1718885177",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1753445177.271482,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_4P31SJ70EJ",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718885177.1.0.1718885177.0.0.0",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718971576,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gid",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.854907463.1718885177",
    },
    {
        "domain": ".www.researchgate.net",
        "expirationDate": 1750407982.506505,
        "hostOnly": False,
        "httpOnly": True,
        "name": "did",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "1dWLO3C6am8l667Q4VUlBo0O1LI49Qi2Vw21SJEXHavBDYT56DI9007W5rYGVFVH",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750507578,
        "hostOnly": False,
        "httpOnly": False,
        "name": "didomi_token",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "eyJ1c2VyX2lkIjoiMTkwMzU4YTUtNWU2My02Y2UzLWJlNzAtZGFjNzVmYjdiY2ExIiwiY3JlYXRlZCI6IjIwMjQtMDYtMjBUMTI6MDY6MTYuODA2WiIsInVwZGF0ZWQiOiIyMDI0LTA2LTIwVDEyOjA2OjE4Ljc4MVoiLCJ2ZW5kb3JzIjp7ImVuYWJsZWQiOlsidHdpdHRlciIsImdvb2dsZSIsImM6bGlua2VkaW4tbWFya2V0aW5nLXNvbHV0aW9ucyIsImM6b3duZXJpcSIsImM6b21uaXR1cmUtYWRvYmUtYW5hbHl0aWNzIiwiYzp0ZWNobm9yYXRpLW1lZGlhIiwiYzppbnRlcmNvbSIsImM6aW50ZW50LWlxIiwiYzppcHJvbSIsImM6bGlua2VkaW4iLCJjOmFtYXpvbmFkdi16Y1hGTEI2WCIsImM6bWVkaWFuZXQtY1V3YUtFNnoiLCJjOmluZGV4ZXhjaC1OWkNRTTY4UCIsImM6emVvdGFwZ21iLWQ3YndtdGp3IiwiYzp0cmlwbGVsaWYtZGRKSDM0clkiLCJjOnJ0YmhvdXNlLWI4Y2RIOHRNIiwiYzptZHByaW1pcy1lYU4yOVdjUCIsImM6bG9vcG1lbGktVGRhWXRCUHEiLCJjOm1hZ25pdGVpbi05d1RZTHFSRCIsImM6Ymlkc3dpdGNoLWQ2N0V3N1c5IiwiYzpvcmFjbGVhZHYtcUhlREptQUwiLCJjOmdvb2dsZWFuYS00VFhuSmlnUiIsImM6bG90YW1lc29sLURIaTdMUmpNIiwiYzpuZXh0bWlsbGUtR0pyZlg4VWMiLCJjOm5yaWNodGVjLXFVVlEyUlFxIiwiYzpicml0ZXBvb2wtQldWeVdHeVUiLCJjOnRhcGFkaW5jLXFxY2tVN1BXIiwiYzppZDV0ZWNobi16Tk1KNGR3ZiIsImM6bWljcm9zb2Z0IiwiYzpwZXJtdXRpdmUtSjdpaHJlTWsiLCJjOm9wZXJhc29mdC1CY1hjRFZKTSIsImM6cG9zdGhvZy1Cakp4RmRGOSJdfSwicHVycG9zZXMiOnsiZW5hYmxlZCI6WyJnZW9sb2NhdGlvbl9kYXRhIiwiZGV2aWNlX2NoYXJhY3RlcmlzdGljcyJdfSwidmVuZG9yc19saSI6eyJlbmFibGVkIjpbImdvb2dsZSIsImM6b3BlcmFzb2Z0LUJjWGNEVkpNIl19LCJ2ZXJzaW9uIjoyLCJhYyI6IkRIU0FvQUZrQWNnQTVnSHFnUUhBeGdCNndEMTRJR0FRTkFqMEJJd0NTY0VyQUtCd1YtZ3MxQmgwREc0R09nQUEuREhTQW9BRmtBY2dBNWdIcWdRSEF4Z0I2d0QxNElHQVFOQWowQkl3Q1NjRXJBS0J3Vi1nczFCaDBERzRHT2dBQSJ9",
    },
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "hasPdpNext",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "False",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750421183,
        "hostOnly": False,
        "httpOnly": False,
        "name": "ph_phc_ma1XTQyee96N1GML6qUTgLQRiDifnRcE9STiHTZ0CfZ_posthog",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "%7B%22distinct_id%22%3A%220190358a-56a1-7313-83b0-d13dddeac787%22%2C%22%24sesid%22%3A%5B1718885183223%2C%220190358a-56a1-7313-83b0-d13b2b87778d%22%2C1718885176993%5D%2C%22%24session_is_sampled%22%3Atrue%7D",
    },
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "sid",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "qmH5Lc4f0CUJ3zeaxORcV0S8I8V1MuCFZtcIQqPYtv1XPejrbSLAQRbT50PL40TqeKQ1XsQDWt9gtYVzuL80bRmPjw6jn3cQ0ikNqW40maHcQ3JL2Vfa8ZZf0j7p35eJ",
    },
]

COOKIES_LIST += [
    {
        "domain": "github.com",
        "hostOnly": True,
        "httpOnly": True,
        "name": "_gh_sess",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "P%2Fmof1avuqwHaUQUIJR%2FZYn7jqbT7lgGuTGjp1BGAFIG5UpNDusEE3b8dRjz0eATE5xPdPjLYFqMs%2FI9AOalKX4YuYfSEEnxCMawU01099b4o9Xzzcv%2BmecrmO0Q8q%2Bdq1h8SIv6nvPP7HzlFesl8ysafb9b%2F0q6dTArKdSOurasza8UgLSYD08ofA50Pcm0IG7CTzF8ZCizrGgGTMi%2F%2B7L3E17jav5PM1Sf2vQKg15Gbg1QIOppJJHzlufgQoZigqFv%2BWznaws0Tt7Y2lSFCw%3D%3D--CJRhqMXJnwOaJgk4--DhUErlL4GdROikEjKD4O9g%3D%3D",
    },
    {
        "domain": ".github.com",
        "expirationDate": 1750408875.763785,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_octo",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "GH1.1.728652011.1718872875",
    },
    {
        "domain": ".github.com",
        "expirationDate": 1750408875.763926,
        "hostOnly": False,
        "httpOnly": True,
        "name": "logged_in",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "no",
    },
    {
        "domain": ".github.com",
        "hostOnly": False,
        "httpOnly": False,
        "name": "preferred_color_mode",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "dark",
    },
    {
        "domain": ".github.com",
        "hostOnly": False,
        "httpOnly": False,
        "name": "tz",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "Europe%2FParis",
    },
]

COOKIES_LIST += [
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718886430,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gat",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718972770,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gid",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.402246368.1606169825",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1753446370.315621,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.1301409987.1606169825",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1750422367,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_hjid",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "07f80263-a631-4bf4-8ffd-8fc8912085e2",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718888167,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_hjFirstSeen",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
]
COOKIES_LIST += [
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "AWSELBCORS",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "CBD1D7FF1216388FA48838CBCA4774FD22800B8FB548A40EF92BB0994D5B77A8410307CDEAA69C52236663F2BF89B252C17BC0FCDF790FD59771BDDF6EA8CA4CFD29D8733F",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1753452454.637671,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_9R61FWK9H5",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718892454.1.0.1718892454.0.0.0",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1753452454.63421,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.1.2021310691.1718892455",
    },
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "AWSELB",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": True,
        "storeId": None,
        "value": "CBD1D7FF1216388FA48838CBCA4774FD22800B8FB548A40EF92BB0994D5B77A8410307CDEAA69C52236663F2BF89B252C17BC0FCDF790FD59771BDDF6EA8CA4CFD29D8733F",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1750428454,
        "hostOnly": False,
        "httpOnly": False,
        "name": "OptanonAlertBoxClosed",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "2024-06-20T14:07:34.583Z",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1750428454,
        "hostOnly": False,
        "httpOnly": False,
        "name": "OptanonConsent",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "isGpcEnabled=0&datestamp=Thu+Jun+20+2024+16%3A07%3A34+GMT%2B0200+(heure+d%E2%80%99%C3%A9t%C3%A9+d%E2%80%99Europe+centrale)&version=202310.2.0&browserGpcFlag=0&isIABGlobal=False&hosts=&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1",
    },
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "XSRF-TOKEN",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "6957be7a-bcb4-4d59-a522-ea9b6b210ed9",
    },
]

# Create a RequestsCookieJar instance
COOKIES = RequestsCookieJar()

# Add cookies to the jar
for cookie in COOKIES_LIST:
    COOKIES.set(cookie["name"], cookie["value"], domain=cookie["domain"], path=cookie["path"])

```

.\scripts\example_generator.py
```python
"""Example generation module."""
from typing import Dict, Any, List, Optional, Literal, Set, cast
from datetime import datetime
from pydantic import BaseModel, Field, SecretStr
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from scripts.chat_langchain import ChatLangChain
from scripts.qa_system import QASystem, Question
from scripts.text_web_browser_fixed import SimpleTextBrowser, web_search
from scripts.logging_config import log_error_with_traceback, log_warning_with_context, log_info_with_context
from langchain_neo4j import Neo4jGraph
import os

class Example(BaseModel):
    """Training example model."""
    input_text: str = Field(description="Input text for the example")
    output_text: str = Field(description="Expected output text")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
    quality_score: float = Field(default=0.8, description="Quality score between 0 and 1", ge=0.0, le=1.0)
    example_type: Literal["knowledge_recall", "concept_application", "analysis", "problem_solving", "critical_thinking"] = Field(description="Type of example")
    difficulty: float = Field(default=0.5, description="Difficulty score between 0 and 1", ge=0.0, le=1.0)
    skills_tested: List[str] = Field(default_factory=list, description="Skills being tested")

class ExampleGenerationResult(BaseModel):
    """Result of example generation."""
    examples: List[Example] = Field(default_factory=list, description="Generated examples")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

class ExampleGenerator:
    """Generates training examples from content."""
    def __init__(self, config: Dict[str, Any]):
        """Initialize example generator."""
        self.config = config
        
        # Initialize output parser
        self.parser = PydanticOutputParser(pydantic_object=ExampleGenerationResult)
        
        # Initialize LLM with ChatLangChain
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise EnvironmentError("GOOGLE_API_KEY environment variable must be set")
            
        self.llm = ChatLangChain(
            model="gemini-1.5-flash",
            temperature=0.7,
            api_key=SecretStr(api_key),
            pydantic_schema=ExampleGenerationResult,
            format='json',
            response_format={"type": "json_object"}
        )
        
        # Initialize QA system
        graph = config.get("graph")
        if not isinstance(graph, Neo4jGraph):
            raise ValueError("Config must contain a valid Neo4jGraph instance")
            
        self.qa_system = QASystem(
            graph=graph,
            llm=self.llm
        )
        
        # Initialize vector store
        self.vector_store = Chroma(
            collection_name=config.get("collection_name", "examples"),
            embedding_function=OllamaEmbeddings(model='bge-m3'),
            persist_directory=config.get("persist_directory", "./data/chroma")
        )
        
        # Initialize web browser for augmentation
        self.browser = SimpleTextBrowser()
        
        # Create prompt template
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert at generating high-quality training examples.
Given some content, generate diverse training examples that capture different aspects of the knowledge.
Each example should have an input question/prompt and the expected output answer/response.
The examples should be varied in format, complexity and skills tested.

{format_instructions}

CRITICAL: You MUST respond with ONLY a valid JSON object, no other text or explanation.
DO NOT wrap the JSON in markdown code blocks or any other formatting.
DO NOT include ```json or ``` markers.
DO NOT include any text before or after the JSON.
DO NOT include any explanatory text or comments.
ENSURE all JSON is properly escaped and formatted.

CRITICAL EXAMPLE TYPE RULES:
You MUST use ONLY these exact example types:
- "knowledge_recall" - For testing basic understanding and recall
- "concept_application" - For applying concepts in context
- "analysis" - For analyzing and breaking down concepts
- "problem_solving" - For solving practical problems
- "critical_thinking" - For evaluation and synthesis

IMPORTANT INSTRUCTIONS:
1. Example Type Distribution:
   - knowledge_recall: ~25% of examples
   - concept_application: ~25% of examples
   - analysis: ~20% of examples
   - problem_solving: ~15% of examples
   - critical_thinking: ~15% of examples
   
2. Difficulty Distribution:
   - Basic (0.0-0.3): 35-45% of examples
     * Simple recall questions
     * Basic understanding checks
     * Straightforward definitions
   
   - Intermediate (0.3-0.7): 35-45% of examples
     * Application questions
     * Analysis tasks
     * Compare/contrast exercises
   
   - Advanced (0.7-1.0): 15-25% of examples
     * Complex problem solving
     * Synthesis tasks
     * Critical evaluation
   
3. Required Skills Coverage:
   Each example MUST test at least 2 skills from:
   - Comprehension (understanding concepts)
   - Application (using knowledge)
   - Analysis (breaking down concepts)
   - Synthesis (combining ideas)
   - Evaluation (assessing and judging)
   - Problem Solving (practical application)
   - Critical Thinking (reasoned judgment)
   - Pattern Recognition (identifying trends)
   
4. Quality Requirements:
   Input Text:
   - Must be clear and unambiguous
   - Must use proper grammar and punctuation
   - Must be appropriately complex for difficulty level
   - Must include context when needed
   - Must use proper terminology
   
   Output Text:
   - Must be comprehensive and complete
   - Must directly answer the input
   - Must include explanations where appropriate
   - Must demonstrate understanding
   - Must use proper terminology
   - Must be well-structured"""),
            ("human", "Generate training examples from this content:\n{content}\nKnowledge Graph Context:\n{kg_context}\nSimilar Examples:\n{similar_examples}")
        ])
        
    async def generate_examples(self, content: str) -> ExampleGenerationResult:
        """Generate examples from content."""
        try:
            # Get knowledge graph context
            kg_context = await self._get_kg_context(content)
            
            # Get similar examples from vector store
            similar_examples = await self._get_similar_examples(content)
            
            # Generate base examples using LLM
            chain = self.prompt | self.llm | self.parser
            result = await chain.ainvoke({
                "content": content,
                "kg_context": kg_context,
                "similar_examples": similar_examples,
                "format_instructions": self.parser.get_format_instructions()
            })
            
            # Generate additional QA pairs using QA system
            qa_examples = await self._generate_qa_examples(content)
            
            # Augment with web knowledge
            web_examples = await self._augment_with_web_knowledge(content)
            
            # Combine all examples
            all_examples = result.examples + qa_examples + web_examples
            
            # Validate and filter examples
            validated_examples = []
            for example in all_examples:
                if await self._validate_example(example, content):
                    validated_examples.append(example)
                    
            # Ensure minimum number of examples
            while len(validated_examples) < 100:
                # Generate more examples with different prompts/approaches
                more_examples = await self._generate_more_examples(content, len(validated_examples))
                for example in more_examples:
                    if await self._validate_example(example, content):
                        validated_examples.append(example)
                        if len(validated_examples) >= 500:  # Cap at 500
                            break
                            
            # Update result with validated examples
            result.examples = validated_examples
            result.metadata.update({
                "num_examples": len(validated_examples),
                "avg_quality": sum(ex.quality_score for ex in validated_examples) / len(validated_examples) if validated_examples else 0,
                "avg_difficulty": sum(ex.difficulty for ex in validated_examples) / len(validated_examples) if validated_examples else 0,
                "example_types": list(set(ex.example_type for ex in validated_examples)),
                "skills_covered": list(set(skill for ex in validated_examples for skill in ex.skills_tested)),
                "timestamp": datetime.now().isoformat()
            })
            
            # Save examples to vector store
            await self._save_to_vector_store(validated_examples)
            
            return result
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating examples")
            return ExampleGenerationResult()
            
    async def _get_kg_context(self, content: str) -> str:
        """Get relevant context from knowledge graph."""
        try:
            # Extract key terms
            key_terms = self._extract_key_concepts(content)
            
            # Query graph for related information
            context_parts = []
            for term in key_terms:
                results = self.qa_system.graph.query(f"""
                    MATCH (n)-[r]-(m)
                    WHERE toLower(n.name) CONTAINS toLower($term)
                    RETURN n.name as source, type(r) as relation, m.name as target
                    LIMIT 10
                """, {"term": term})
                
                for result in results:
                    context_parts.append(f"{result['source']} {result['relation']} {result['target']}")
                    
            return "\n".join(context_parts)
            
        except Exception as e:
            log_error_with_traceback(e, "Error getting KG context")
            return ""
            
    async def _get_similar_examples(self, content: str) -> List[Dict[str, Any]]:
        """Get similar examples from vector store."""
        try:
            # Get embeddings for content
            embeddings = OllamaEmbeddings(model='bge-m3').embed_documents([content])[0]
            
            # Search vector store
            results = self.vector_store.similarity_search_by_vector(
                embeddings,
                k=10
            )
            
            return [doc.metadata for doc in results if "example" in doc.metadata]
            
        except Exception as e:
            log_error_with_traceback(e, "Error getting similar examples")
            return []
            
    async def _generate_qa_examples(self, content: str) -> List[Example]:
        """Generate examples using QA system."""
        try:
            # Generate questions
            questions = await self.qa_system.generate_questions(content, num_questions=20)
            
            # Generate answers
            examples = []
            for question in questions:
                response = await self.qa_system.process_qa_chain(question.question)
                if response and response.answer:
                    example = Example(
                        input_text=question.question,
                        output_text=response.answer,
                        metadata={
                            "source": "qa_system",
                            "confidence": response.confidence,
                            "sources": response.sources
                        },
                        quality_score=response.confidence,
                        example_type=cast(
                            Literal["knowledge_recall", "concept_application", "analysis", "problem_solving", "critical_thinking"],
                            self._map_question_type(question.type)
                        ),
                        difficulty=question.difficulty,
                        skills_tested=["comprehension", "analysis"]  # Default skills
                    )
                    examples.append(example)
                    
            return examples
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating QA examples")
            return []
            
    async def _augment_with_web_knowledge(self, content: str) -> List[Example]:
        """Augment examples with web knowledge."""
        try:
            # Search web for related content with config
            config = {
                "domain_name": "medical research",  # Default domain
                "search_depth": 2,
                "max_results": 10
            }
            search_results = await web_search(content[:200], config)  # Use first 200 chars as query
            
            # Generate examples from web content
            examples = []
            for result in search_results.split("---"):
                if result.strip():
                    # Generate examples from web content
                    chain = self.prompt | self.llm | self.parser
                    result = await chain.ainvoke({
                        "content": result,
                        "kg_context": "",
                        "similar_examples": "",
                        "format_instructions": self.parser.get_format_instructions()
                    })
                    
                    if result and result.examples:
                        for example in result.examples:
                            example.metadata["source"] = "web"
                        examples.extend(result.examples)
                        
            return examples
            
        except Exception as e:
            log_error_with_traceback(e, "Error augmenting with web knowledge")
            return []
            
    async def _generate_more_examples(self, content: str, current_count: int) -> List[Example]:
        """Generate additional examples to meet minimum requirements."""
        try:
            # Try different approaches based on current count
            examples = []
            
            # Approach 1: Use different temperature
            if current_count < 200:
                temp_llm = ChatLangChain(
                    model="gemini-1.5-flash",
                    temperature=0.9,  # Higher temperature for more variety
                    api_key=SecretStr(os.getenv("GOOGLE_API_KEY", "")),
                    pydantic_schema=ExampleGenerationResult,
                    format='json'
                )
                chain = self.prompt | temp_llm | self.parser
                result = await chain.ainvoke({
                    "content": content,
                    "kg_context": await self._get_kg_context(content),
                    "similar_examples": "",
                    "format_instructions": self.parser.get_format_instructions()
                })
                if result and result.examples:
                    examples.extend(result.examples)
                    
            # Approach 2: Focus on specific example types
            if current_count < 300:
                for example_type in ["problem_solving", "critical_thinking"]:
                    questions = await self.qa_system.generate_questions(
                        content,
                        num_questions=10
                    )
                    for question in questions:
                        response = await self.qa_system.process_qa_chain(question.question)
                        if response and response.answer:
                            example = Example(
                                input_text=question.question,
                                output_text=response.answer,
                                metadata={"source": "focused_qa"},
                                quality_score=response.confidence,
                                example_type=cast(
                                    Literal["knowledge_recall", "concept_application", "analysis", "problem_solving", "critical_thinking"],
                                    example_type
                                ),
                                difficulty=0.7,
                                skills_tested=["critical_thinking", "problem_solving"]
                            )
                            examples.append(example)
                            
            # Approach 3: Use web knowledge more aggressively
            if current_count < 400:
                web_examples = await self._augment_with_web_knowledge(content)
                examples.extend(web_examples)
                
            return examples
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating more examples")
            return []
            
    async def _save_to_vector_store(self, examples: List[Example]) -> None:
        """Save examples to vector store."""
        try:
            texts = []
            metadatas = []
            
            for example in examples:
                # Combine input and output for embedding
                text = f"Question: {example.input_text}\nAnswer: {example.output_text}"
                texts.append(text)
                
                # Store full example in metadata
                metadatas.append({
                    "example": example.model_dump(),
                    "timestamp": datetime.now().isoformat()
                })
                
            # Generate embeddings
            embeddings = OllamaEmbeddings(model='bge-m3').embed_documents(texts)
            
            # Add to vector store
            self.vector_store.add_texts(
                texts=texts,
                embeddings=embeddings,
                metadatas=metadatas
            )
            
        except Exception as e:
            log_error_with_traceback(e, "Error saving to vector store")
            
    def _map_question_type(self, qa_type: str) -> str:
        """Map QA system question type to example type."""
        mapping = {
            "factual": "knowledge_recall",
            "conceptual": "concept_application",
            "analytical": "analysis",
            "problem": "problem_solving",
            "synthesis": "critical_thinking"
        }
        return mapping.get(qa_type, "knowledge_recall")

    async def _validate_example(self, example: Example, content: str) -> bool:
        """Validate a generated example."""
        try:
            # Check input text quality
            if not example.input_text or len(example.input_text.split()) < 3:
                log_warning_with_context("Input text too short or empty", "Example Generation")
                return False
                
            # Check output text quality
            if not example.output_text or len(example.output_text.split()) < 5:
                log_warning_with_context("Output text too short or empty", "Example Generation")
                return False
                
            # Check if example tests content knowledge
            input_relevance = any(term.lower() in example.input_text.lower() for term in content.split())
            output_relevance = any(term.lower() in example.output_text.lower() for term in content.split())
            if not (input_relevance or output_relevance):
                log_warning_with_context("Example not relevant to content", "Example Generation")
                return False

            # Check skills coverage
            if not example.skills_tested:
                log_warning_with_context("No skills specified", "Example Generation")
                return False
            
            # Ensure diverse skills
            if len(example.skills_tested) < 2:
                log_warning_with_context("Too few skills tested", "Example Generation")
                return False

            # Check quality factors with lower thresholds
            quality_factors = {
                "input_clarity": self._assess_input_clarity(example.input_text),
                "output_completeness": self._assess_output_completeness(example.output_text),
                "relevance": self._assess_relevance(example, content),
                "skills_coverage": len(example.skills_tested) / 4.0  # Normalize to 0-1, reduced from 5.0
            }
            
            # Calculate overall quality score with adjusted weights
            weights = {
                "input_clarity": 0.25,  # Reduced from 0.3
                "output_completeness": 0.25,  # Reduced from 0.3
                "relevance": 0.3,  # Increased from 0.25
                "skills_coverage": 0.2  # Increased from 0.15
            }
            quality_score = sum(score * weights[factor] for factor, score in quality_factors.items())
            
            # Update example's quality score
            example.quality_score = quality_score
            
            if quality_score < 0.55:  # Reduced from 0.65
                log_warning_with_context(f"Example quality too low: {quality_score:.2f}", "Example Generation")
                return False
                
            # Initialize difficulty counts if not exists
            if not hasattr(self, '_difficulty_counts'):
                self._difficulty_counts = {
                    'basic': 0,      # 0.0-0.3
                    'intermediate': 0,  # 0.3-0.7
                    'advanced': 0    # 0.7-1.0
                }
                
            # Categorize difficulty
            if example.difficulty <= 0.3:
                category = 'basic'
            elif example.difficulty <= 0.7:
                category = 'intermediate'
            else:
                category = 'advanced'
                
            # Check difficulty balance with more permissive thresholds
            current_count = self._difficulty_counts[category]
            total_examples = sum(self._difficulty_counts.values())
            
            # Calculate current distribution
            if total_examples > 0:
                basic_ratio = self._difficulty_counts['basic'] / total_examples
                intermediate_ratio = self._difficulty_counts['intermediate'] / total_examples
                advanced_ratio = self._difficulty_counts['advanced'] / total_examples
                
                # Define target ratios with wider tolerance
                target_ratios = {
                    'basic': (0.30, 0.50),  # 30-50% basic (widened from 35-45%)
                    'intermediate': (0.30, 0.50),  # 30-50% intermediate (widened from 35-45%)
                    'advanced': (0.10, 0.30)  # 10-30% advanced (widened from 15-25%)
                }
                
                # Check if current category is within acceptable range
                min_ratio, max_ratio = target_ratios[category]
                current_ratio = self._difficulty_counts[category] / total_examples
                
                # More permissive in early stages
                if total_examples < 20:  # Increased from 10
                    # Allow any distribution initially
                    pass
                elif current_ratio > max_ratio:
                    log_warning_with_context(f"Too many {category} difficulty examples", "Example Generation")
                    return False
                elif current_ratio < min_ratio:
                    # Encourage this category
                    pass
                
            # Update difficulty counts
            self._difficulty_counts[category] += 1
            
            # Track example type distribution
            if not hasattr(self, '_type_counts'):
                self._type_counts = {
                    'knowledge_recall': 0,
                    'concept_application': 0,
                    'analysis': 0,
                    'problem_solving': 0,
                    'critical_thinking': 0
                }
            
            # Check example type distribution with more permissive limits
            type_count = self._type_counts.get(example.example_type, 0)
            max_per_type = max(5, total_examples // 3)  # Allow up to 33% of one type (increased from 25%)
            
            if type_count >= max_per_type:
                log_warning_with_context(f"Too many examples of type {example.example_type}", "Example Generation")
                return False
                
            self._type_counts[example.example_type] = type_count + 1
            return True
            
        except Exception as e:
            log_error_with_traceback(e, "Error validating example")
            return False

    def _assess_input_clarity(self, input_text: str) -> float:
        """Assess clarity of input text."""
        score = 0.0
        
        # Length scoring (0.0-0.3)
        words = len(input_text.split())
        if words < 4:  # Reduced from 5
            score += 0.1
        elif words < 8:  # Reduced from 10
            score += 0.2
        else:
            score += 0.3
            
        # Structure scoring (0.0-0.4)
        if '?' in input_text:
            score += 0.1
        if any(input_text.lower().startswith(w) for w in ['what', 'why', 'how', 'when', 'where', 'who', 'which']):
            score += 0.1
        if len(input_text.split(',')) > 1 or len(input_text.split('and')) > 1:
            score += 0.1
        if any(phrase in input_text.lower() for phrase in ['explain', 'describe', 'compare', 'analyze', 'evaluate']):
            score += 0.1
            
        # Quality scoring (0.0-0.3)
        if input_text[0].isupper():  # Removed period/question mark requirement
            score += 0.1
        if not any(char in input_text for char in '@#$%^&*'):  # No special characters
            score += 0.1
        if len(input_text) >= len(input_text.strip()):  # No extra whitespace
            score += 0.1
            
        return min(1.0, score)

    def _assess_output_completeness(self, output_text: str) -> float:
        """Assess completeness of output text."""
        score = 0.0
        
        # Length scoring (0.0-0.3)
        words = len(output_text.split())
        if words < 8:  # Reduced from 10
            score += 0.1
        elif words < 15:  # Reduced from 20
            score += 0.2
        else:
            score += 0.3
            
        # Structure scoring (0.0-0.4)
        sentences = len(output_text.split('.'))
        if sentences > 1:
            score += 0.1
        if any(phrase in output_text.lower() for phrase in ['because', 'therefore', 'thus', 'hence']):
            score += 0.1
        if any(phrase in output_text.lower() for phrase in ['for example', 'such as', 'specifically']):
            score += 0.1
        if any(phrase in output_text.lower() for phrase in ['first', 'second', 'finally', 'additionally']):
            score += 0.1
            
        # Quality scoring (0.0-0.3)
        if output_text[0].isupper():  # Removed period requirement
            score += 0.1
        if not any(char in output_text for char in '@#$%^&*'):  # No special characters
            score += 0.1
        if len(output_text) >= len(output_text.strip()):  # No extra whitespace
            score += 0.1
            
        return min(1.0, score)

    def _assess_relevance(self, example: Example, content: str) -> float:
        """Assess relevance to content."""
        score = 0.0
        
        # Content overlap scoring (0.0-0.4)
        content_terms = set(content.lower().split())
        input_terms = set(example.input_text.lower().split())
        output_terms = set(example.output_text.lower().split())
        
        input_overlap = len(input_terms.intersection(content_terms)) / len(input_terms) if input_terms else 0
        output_overlap = len(output_terms.intersection(content_terms)) / len(output_terms) if output_terms else 0
        
        score += (input_overlap * 0.2) + (output_overlap * 0.2)  # More balanced between input/output
        
        # Key concept scoring (0.0-0.3)
        key_concepts = self._extract_key_concepts(content)
        concept_count = sum(1 for concept in key_concepts 
                          if concept in example.input_text.lower() 
                          or concept in example.output_text.lower())
        score += min(0.3, concept_count * 0.15)  # Increased from 0.1
        
        # Context relevance scoring (0.0-0.3)
        if example.example_type == "knowledge_recall":
            score += 0.3 if input_overlap > 0.2 else 0.15  # Reduced threshold from 0.3
        elif example.example_type in ["concept_application", "analysis"]:
            score += 0.3 if output_overlap > 0.2 else 0.15  # Reduced threshold from 0.3
        else:  # problem_solving, critical_thinking
            score += 0.3 if (input_overlap + output_overlap) / 2 > 0.2 else 0.15  # Reduced threshold from 0.3
            
        return min(1.0, score)

    def _extract_key_concepts(self, content: str) -> Set[str]:
        """Extract key concepts from content."""
        # Simple extraction based on capitalized terms and common ML concepts
        concepts = set()
        
        # Add capitalized terms
        words = content.split()
        concepts.update(word.lower() for word in words if word[0].isupper())
        
        # Add common ML concepts
        ml_concepts = {
            'machine learning', 'deep learning', 'neural network', 'artificial intelligence',
            'training', 'model', 'algorithm', 'data', 'prediction', 'classification',
            'regression', 'accuracy', 'precision', 'recall', 'validation'
        }
        concepts.update(concept for concept in ml_concepts if concept in content.lower())
        
        return concepts 
```

.\scripts\gaia_scorer.py
```python
import re
import string
import warnings

from pyparsing import Any


def normalize_number_str(number_str: str) -> float:
    # we replace these common units and commas to allow
    # conversion to float
    for char in ["$", "%", ","]:
        number_str = number_str.replace(char, "")
    try:
        return float(number_str)
    except ValueError:
        print(f"String {number_str} cannot be normalized to number str.")
        return float("inf")


def split_string(
    s: str,
    char_list: list[str] = [",", ";"],
) -> list[str]:
    pattern = f"[{''.join(char_list)}]"
    return re.split(pattern, s)


def is_float(element: Any) -> bool:
    try:
        float(element)
        return True

    except ValueError:
        return False


def question_scorer(
    model_answer: str,
    ground_truth: str,
) -> bool:
    # if gt is a number
    if is_float(ground_truth):
        normalized_answer = normalize_number_str(str(model_answer))
        return normalized_answer == float(ground_truth)

    # if gt is a list
    elif any(char in ground_truth for char in [",", ";"]):
        # question with the fish: normalization removes punct

        gt_elems = split_string(ground_truth)
        ma_elems = split_string(model_answer)

        # check length is the same
        if len(gt_elems) != len(ma_elems):
            warnings.warn("Answer lists have different lengths, returning False.", UserWarning)
            return False

        # compare each element as float or str
        comparisons = []
        for ma_elem, gt_elem in zip(ma_elems, gt_elems):
            if is_float(gt_elem):
                normalized_ma_elem = normalize_number_str(ma_elem)
                comparisons.append(normalized_ma_elem == float(gt_elem))
            else:
                # we do not remove punct since comparisons can include punct
                comparisons.append(
                    normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False)
                )
        return all(comparisons)

    # if gt is a str
    else:
        return normalize_str(model_answer) == normalize_str(ground_truth)


def check_prediction_contains_answer_letters_in_order(prediction, true_answer):
    prediction = prediction.lower()
    true_answer = true_answer.lower()
    if len(prediction) > len(true_answer) * 3:
        return False
    i = 0
    for letter in true_answer:
        if letter in prediction[i:]:
            i += prediction[i:].index(letter)
        else:
            return False
    return True


def check_close_call(prediction, true_answer, is_correct):
    if is_correct:
        return True
    else:
        if is_float(true_answer):
            return is_correct
        else:
            if (
                check_prediction_contains_answer_letters_in_order(str(prediction), str(true_answer))
                and len(str(true_answer)) * 0.5 <= len(str(prediction)) <= len(str(true_answer)) * 2
            ):
                print(f"Close call: {prediction} vs {true_answer}")
                return True
            else:
                return False


def normalize_str(input_str, remove_punct=True) -> str:
    """
    Normalize a string by:
    - Removing all white spaces
    - Optionally removing punctuation (if remove_punct is True)
    - Converting to lowercase
    Parameters:
    - input_str: str, the string to normalize
    - remove_punct: bool, whether to remove punctuation (default: True)
    Returns:
    - str, the normalized string
    """
    # Remove all white spaces. Required e.g for seagull vs. sea gull
    no_spaces = re.sub(r"\s", "", input_str)

    # Remove punctuation, if specified.
    if remove_punct:
        translator = str.maketrans("", "", string.punctuation)
        return no_spaces.lower().translate(translator)
    else:
        return no_spaces.lower()

```

.\scripts\knowledge_acquisition.py
```python
from typing import (
    List,
    Dict,
    Optional,
    Any,
    TypedDict,
    Protocol,
    cast,
    Awaitable,
    Callable,
    Union,
    TypeVar,
    Coroutine,
    Sequence
)
from datetime import datetime
import json
from pydantic import BaseModel, Field, validator, field_validator, SecretStr
from langchain_core.output_parsers import PydanticOutputParser
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader
)
from langchain_chroma import Chroma
from langchain_neo4j import Neo4jGraph
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from scripts.models import (
    KnowledgeAcquisitionConfig,
    ExtractedKnowledge,
    Relationship,
    SourceMetadata,
    DomainConfig,
    ConfidenceEvaluation,
    ConfidenceFactors
)
from scripts.logging_config import (
    log_error_with_traceback,
    log_warning_with_context,
    log_info_with_context,
    setup_logging,
    create_progress,
    log_extraction_results
)
from scripts.text_web_browser_fixed import SimpleTextBrowser, web_search
from scripts.mdconvert import MarkdownConverter
from scripts.visual_qa import VisualAnalyzer
from scripts.text_inspector_tool import TextInspector, TextAnalysis
from scripts.llm_compiler import LLMCompiler, Task, Plan, TaskResult, JoinDecision, CompilerState
from scripts.chat_langchain import ChatLangChain
import os
from loguru import logger
import asyncio
from prompts.knowledge_acquisition import (
    get_knowledge_extraction_prompt,
    get_confidence_evaluation_prompt,
    EntityResponse,
    get_entity_extraction_prompt,
    RelationshipResponse,
    get_relationship_extraction_prompt,
    MetadataResponse,
    get_metadata_generation_prompt,
    get_plan_generation_prompt,
    get_join_decision_prompt
)
from langchain.prompts import ChatPromptTemplate
from bs4 import BeautifulSoup, Comment
import hashlib
from tqdm.rich import tqdm
import re
import google.api_core.exceptions

# Type variables for async functions
T = TypeVar('T')
AsyncFunc = Callable[..., Coroutine[Any, Any, T]]

# Initialize logging and rich console
setup_logging()
console = Console()

# Create shared progress instance
progress = create_progress()

# Initialize document converters
markdown_converter = MarkdownConverter()

class ContentProcessor(Protocol):
    """Protocol for content processors"""
    async def process(self, content: str, **kwargs) -> Dict[str, Any]: ...

class TextInspectorProcessor(ContentProcessor):
    """Text inspector processor using LLMCompiler pattern"""
    def __init__(self, llm):
        self.inspector = TextInspector(llm)
        
    async def process(self, content: str, **kwargs) -> Dict[str, Any]:
        """Process text content using text inspector"""
        try:
            result = await self.inspector.inspect_text(
                content,
              
            )
            return result.model_dump() if result else {}
        except Exception as e:
            log_error_with_traceback(e, "Error in text inspection")
            return {}

class VisualProcessor:
    """Visual processor using LLMCompiler pattern"""
    def __init__(self, llm):
        self.analyzer = VisualAnalyzer(llm)
        
    async def process(self, content: str, **kwargs) -> Dict[str, Any]:
        """Process visual content"""
        try:
            if "image_path" not in kwargs:
                return {}
                
            # Analyze image content
            analysis = await self.analyzer.analyze_image(
                kwargs["image_path"],
                kwargs.get("question", "Describe this image in detail.")
            )
            
            return {
                "analysis": analysis.model_dump() if hasattr(analysis, "model_dump") else analysis,
            }
            
        except Exception as e:
            log_error_with_traceback(e, "Error in visual processing")
            return {}

class DocumentProcessor:
    """Document processor using LLMCompiler pattern"""
    def __init__(self, llm):
        self.markdown_converter = MarkdownConverter()
        self.text_processor = TextInspectorProcessor(llm)
        self.visual_processor = VisualProcessor(llm)
        
    async def process_document(self, source_path: str, source_type: str) -> Document:
        """Process document with appropriate processor"""
        try:
            # Convert document to markdown
            result = self.markdown_converter.convert(source_path)
            
            # Process based on type
            if source_type in ["jpg", "jpeg", "png"]:
                analysis = await self.visual_processor.process(
                    result.text_content,
                    image_path=source_path
                )
            else:
                analysis = await self.text_processor.process(
                    result.text_content,
                    metadata={"source": source_path, "type": source_type}
                )
                
            # Create document with analysis
            return Document(
                page_content=result.text_content,
                metadata={
                    "source": source_path,
                    "type": source_type,
                    "title": result.title,
                    "analysis": analysis
                }
            )
            
        except Exception as e:
            log_error_with_traceback(e, f"Error processing document {source_path}")
            return Document(
                page_content="",
                metadata={
                    "source": source_path,
                    "type": source_type,
                    "error": str(e)
                }
            )

class EmbeddingGenerator(Protocol):
    """Protocol for embedding generators"""
    def embed_documents(self, texts: List[str]) -> List[List[float]]: ...

class TaskState(BaseModel):
    """State for task execution"""
    content: str = Field(description="Content to process")
    knowledge: Optional[ExtractedKnowledge] = None
    embeddings: Optional[List[float]] = None
    graph_updates: Optional[bool] = Field(default=None, description="Whether graph updates are complete")
    error: Optional[str] = None

    @field_validator("content")
    @classmethod
    def validate_content(cls, v: Any) -> str:
        if isinstance(v, str):
            return v
        if hasattr(v, "content"):
            return str(v.content)
        return str(v)

class ProcessingResult(TypedDict):
    """Result of processing"""
    success: bool
    error: Optional[str]

class ProcessingState(BaseModel):
    """State for processing workflow"""
    tasks: List[TaskState] = Field(default_factory=list)
    completed: List[TaskState] = Field(default_factory=list)
    errors: List[str] = Field(default_factory=list)

    def model_dump(self) -> Dict[str, Any]:
        """Convert state to dict"""
        return {
            "tasks": [task.model_dump() for task in self.tasks],
            "completed": [task.dict() for task in self.completed],
            "errors": [err for err in self.errors]
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ProcessingState":
        """Create state from dict"""
        return cls(
            tasks=[task if isinstance(task, TaskState) else TaskState(**task) for task in data.get("tasks", [])],
            completed=[task if isinstance(task, TaskState) else TaskState(**task) for task in data.get("completed", [])],
            errors=data.get("errors", [])
        )

class SearchFilter(TypedDict):
    confidence: Dict[str, float]

class WebSearchResponse(BaseModel):
    """Schema for web search response"""
    urls: List[str] = Field(description="List of relevant URLs")
    snippets: List[str] = Field(description="List of text snippets from search results")
    titles: List[str] = Field(description="List of result titles")

class WebContentResponse(BaseModel):
    """Schema for web content response"""
    content: str = Field(description="Extracted text content")
    url: str = Field(description="Source URL")
    title: Optional[str] = Field(None, description="Page title")
    metadata: Dict = Field(default_factory=dict, description="Additional metadata")

class KnowledgeAcquisitionSystem(LLMCompiler):
    """Knowledge acquisition system using LLMCompiler pattern"""
    def __init__(self, config: KnowledgeAcquisitionConfig):
        """Initialize the system"""
        try:
            # Initialize LLM first
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise EnvironmentError("GOOGLE_API_KEY environment variable must be set")
                
            llm = ChatLangChain(
                api_key=SecretStr(api_key),
                model="gemini-1.5-flash",
                temperature=0.7,
                format="json",
                pydantic_schema=EntityResponse
            )
            super().__init__(llm)
            
            # Initialize other components
            self.config = config
            self.embeddings = OllamaEmbeddings(model='bge-m3', base_url='http://localhost:11434')
            self.vector_store = None
            self.graph = None
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            
            # Initialize parsers with function calling
            self.entity_parser = PydanticOutputParser(pydantic_object=EntityResponse)
            self.relationship_parser = PydanticOutputParser(pydantic_object=RelationshipResponse)
            self.metadata_parser = PydanticOutputParser(pydantic_object=MetadataResponse)
            
            log_info_with_context("Knowledge acquisition system initialized", "KnowledgeAcquisition", include_locals=True)
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to initialize knowledge acquisition system", include_locals=True)
            raise

    async def initialize(self) -> "KnowledgeAcquisitionSystem":
        """Initialize vector store and graph database"""
        try:
            log_info_with_context("Starting system initialization", "KnowledgeAcquisition")
            
            # Initialize embeddings
            try:
                self.embeddings = OllamaEmbeddings(
                    model='bge-m3',  # Using a more reliable model
                    base_url='http://localhost:11434'
                )
                log_info_with_context("Embeddings initialized", "KnowledgeAcquisition")
            except Exception as e:
                log_error_with_traceback(e, "Failed to initialize embeddings", include_locals=True)
                raise
            
            # Initialize vector store
            if not self.vector_store:
                try:
                    self.vector_store = Chroma(
                        collection_name=self.config.collection_name,
                        embedding_function=self.embeddings,
                        persist_directory=self.config.persist_directory
                    )
                    log_info_with_context("Vector store initialized", "KnowledgeAcquisition")
                except Exception as e:
                    log_error_with_traceback(e, "Failed to initialize vector store", include_locals=True)
                    raise
            
            # Initialize graph database
            if not self.graph:
                try:
                    self.graph = Neo4jGraph(
                        url=self.config.neo4j_uri,
                        username=self.config.neo4j_username,
                        password=self.config.neo4j_password
                    )
                    log_info_with_context("Graph database initialized", "KnowledgeAcquisition")
                except Exception as e:
                    log_error_with_traceback(e, "Failed to initialize graph database", include_locals=True)
                    raise
            
            # Create necessary directories
            os.makedirs(self.config.persist_directory, exist_ok=True)
            os.makedirs("web", exist_ok=True)
            os.makedirs("downloads", exist_ok=True)
            
            return self
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to initialize system components", include_locals=True)
            raise

    async def add_source(self, source_path: str, source_type: str) -> List[Document]:
        """Add a source to the knowledge base"""
        try:
            log_info_with_context(f"Processing source: {source_path}", "KnowledgeAcquisition")
            
            # Ensure system is initialized
            if not self.vector_store or not self.graph:
                await self.initialize()
            
            # Load and split content
            chunks = await self._load_chunks(source_path, source_type)
            if not chunks:
                log_warning_with_context(f"No content extracted from {source_path}", "KnowledgeAcquisition", include_locals=True)
                return []
            
            # Create progress display
            progress = await create_progress()
            console.print(f"\n[cyan]Processing {len(chunks)} chunks from {source_path}[/cyan]")
            chunk_task = progress.add_task("[cyan]Processing chunks...", total=len(chunks))
            
            # Process chunks in parallel
            async def process_chunk(chunk: str) -> Optional[Document]:
                try:
                    # Process chunk
                    knowledge = await self.process_source(chunk)
                    if not knowledge:
                        return None
                    
                    # Generate embeddings (will be batched later)
                    embeddings = await self._generate_embeddings(chunk)
                    if not embeddings:
                        return None
                    
                    # Convert knowledge to serializable format
                    if isinstance(knowledge, dict):
                        knowledge_dict = knowledge
                    elif hasattr(knowledge, "model_dump"):
                        knowledge_dict = knowledge.model_dump()
                    elif hasattr(knowledge, "dict"):
                        knowledge_dict = knowledge.dict()
                    else:
                        knowledge_dict = dict(knowledge)
                    
                    # Create document
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "source": source_path,
                            "type": source_type,
                            "knowledge": knowledge_dict,
                            "embeddings": embeddings
                        }
                    )
                    progress.update(chunk_task, advance=1)
                    return doc
                    
                except Exception as e:
                    log_error_with_traceback(e, f"Failed to process chunk", include_locals=True)
                    return None
                    
            # Process chunks in parallel with semaphore to limit concurrency
            semaphore = asyncio.Semaphore(10)  # Limit concurrent processing
            async def bounded_process(chunk: str) -> Optional[Document]:
                async with semaphore:
                    return await process_chunk(chunk)
                    
            # Process all chunks
            documents = []
            chunk_tasks = [bounded_process(chunk) for chunk in chunks]
            results = await asyncio.gather(*chunk_tasks)
            documents = [doc for doc in results if doc is not None]
            
            # Batch add to vector store
            if documents and self.vector_store is not None:
                try:
                    texts = [doc.page_content for doc in documents]
                    embeddings = [doc.metadata["embeddings"] for doc in documents]
                    metadatas = [{"source": doc.metadata["source"], "type": doc.metadata["type"]} for doc in documents]
                    
                    def batch_add_to_vector_store():
                        if self.vector_store is not None:
                            self.vector_store.add_texts(
                                texts=texts,
                                embeddings=embeddings,
                                metadatas=metadatas
                            )
                            
                    await asyncio.to_thread(batch_add_to_vector_store)
                    
                except Exception as e:
                    log_error_with_traceback(e, "Failed to batch add to vector store", include_locals=True)
                    
            # Batch add to graph
            if documents:
                try:
                    await asyncio.gather(*[
                        self._add_to_graph(doc.metadata["knowledge"])
                        for doc in documents
                    ])
                except Exception as e:
                    log_error_with_traceback(e, "Failed to batch add to graph", include_locals=True)
            
            log_info_with_context(
                f"Processed {len(documents)} documents from {source_path}",
                "KnowledgeAcquisition",
                include_locals=True
            )
            return documents
            
        except Exception as e:
            log_error_with_traceback(e, f"Failed to add source: {source_path}", include_locals=True)
            raise

    async def process_source(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Process a source document to extract knowledge."""
        try:
            logger.info(f"Starting source processing with content length: {len(content)}")
            
            # Preview content
            if isinstance(content, dict):
                logger.info(f"Content preview: {str(content)[:500]}...")
            else:
                logger.info(f"Content preview: {str(content)[:500]}...")
                
            # Extract entities
            logger.info("Extracting entities...")
            entities = await self._extract_entities(content)
            if not entities:
                logger.warning("No entities extracted")
                return {"content": content, "entities": [], "relationships": [], "metadata": self._create_default_metadata(), "confidence": 0.0}

            logger.info(f"Extracted and validated {len(entities)} entities: {entities}")
                
            # Extract relationships
            logger.info("Extracting relationships...")
            relationships = await self._extract_relationships(content, entities)
                
            # Generate metadata
            logger.info("Generating metadata...")
            source_metadata = self._create_default_metadata()
            if metadata:
                if isinstance(metadata, dict):
                    # Create new metadata object with combined fields
                    source_metadata = SourceMetadata(
                        **{
                            **source_metadata.model_dump(),
                            **metadata
                        }
                    )
                elif isinstance(metadata, SourceMetadata):
                    # Update fields from other metadata object
                    source_metadata = SourceMetadata(
                        **{
                            **source_metadata.model_dump(),
                            **metadata.model_dump()
                        }
                    )
            logger.info(f"Generated metadata: {source_metadata}")

            # Evaluate confidence
            logger.info("Evaluating confidence...")
            confidence = await self._evaluate_confidence(entities, relationships)
            logger.info(f"Confidence score: {confidence}")

            # Return full knowledge object
            return {
                "content": content,
                "entities": entities,
                "relationships": relationships,
                "metadata": source_metadata,
                "confidence": confidence
            }
        except Exception as e:
            log_error_with_traceback(e, "Error processing source")
            return {
                "content": content,
                "entities": [],
                "relationships": [],
                "metadata": self._create_default_metadata(),
                "confidence": 0.0
            }

    async def _clean_content(self, content: str) -> str:
        """Clean and normalize content."""
        try:
            # Remove HTML
            soup = BeautifulSoup(content, "html.parser")
            
            # Remove unwanted elements
            for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
                element.decompose()
                
            # Remove comments
            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
                comment.extract()
                
            # Get main content
            main_content = ""
            main_tags = soup.find_all(["article", "main", "div[role='main']"])
            if main_tags:
                main_content = " ".join(tag.get_text(strip=True, separator=" ") for tag in main_tags)
            else:
                body = soup.find("body")
                if body:
                    main_content = body.get_text(strip=True, separator=" ")
                else:
                    main_content = soup.get_text(strip=True, separator=" ")
                    
            # Clean up text
            main_content = re.sub(r'\s+', ' ', main_content)  # Normalize whitespace
            main_content = re.sub(r'\[.*?\]', '', main_content)  # Remove square bracket content
            main_content = re.sub(r'[^\x00-\x7F]+', '', main_content)  # Remove non-ASCII
            main_content = re.sub(r'(\w)\1{3,}', r'\1\1', main_content)  # Normalize repeated chars
            
            return main_content.strip()
            
        except Exception as e:
            log_error_with_traceback(e, "Error cleaning content")
            return content
            
    async def _add_to_graph(self, knowledge: ExtractedKnowledge) -> None:
        """Add extracted knowledge to graph database"""
        try:
            # Track graph operations
            graph_metrics = {
                "entities_added": 0,
                "relationships_added": 0,
                "failed_operations": 0
            }
            
            # Add entities first
            for entity in knowledge.entities:
                try:
                    # Define synchronous function for graph operation
                    def add_entity() -> None:
                        if self.graph is not None:
                            # Create entity with metadata
                            self.graph.query(
                                """
                                MERGE (e:Entity {name: $name})
                                SET e.domain = $domain,
                                    e.confidence = $confidence,
                                    e.timestamp = $timestamp,
                                    e.source = $source
                                """,
                                {
                                    "name": entity,
                                    "domain": knowledge.domain,
                                    "confidence": float(knowledge.confidence),
                                    "timestamp": datetime.now().isoformat(),
                                    "source": knowledge.metadata.source_type if hasattr(knowledge.metadata, "source_type") else "unknown"
                                }
                            )
                    
                    # Run in thread pool
                    await asyncio.to_thread(cast(Callable[[], None], add_entity))
                    graph_metrics["entities_added"] += 1
                    log_info_with_context(f"Added entity to graph: {entity}", "Graph")
                    
                except Exception as e:
                    log_error_with_traceback(e, f"Failed to add entity: {entity}", include_locals=True)
                    graph_metrics["failed_operations"] += 1
                    continue
            
            # Add relationships with retries
            max_retries = 3
            for rel in knowledge.relationships:
                success = False
                for attempt in range(max_retries):
                    try:
                        # Convert dict to Relationship if needed
                        if isinstance(rel, dict):
                            rel = Relationship(**rel)
                        
                        # Define synchronous function for graph operation
                        def add_relationship() -> None:
                            if self.graph is not None:
                                # First ensure entities exist
                                self.graph.query(
                                    """
                                    MERGE (s:Entity {name: $source})
                                    ON CREATE SET s.domain = $domain,
                                                s.confidence = $confidence,
                                                s.timestamp = $timestamp
                                    MERGE (t:Entity {name: $target})
                                    ON CREATE SET t.domain = $domain,
                                                t.confidence = $confidence,
                                                t.timestamp = $timestamp
                                    """,
                                    {
                                        "source": rel.source,
                                        "target": rel.target,
                                        "domain": knowledge.domain,
                                        "confidence": float(knowledge.confidence),
                                        "timestamp": datetime.now().isoformat()
                                    }
                                )
                                
                                # Then create relationship with metadata
                                self.graph.query(
                                    """
                                    MATCH (s:Entity {name: $source})
                                    MATCH (t:Entity {name: $target})
                                    MERGE (s)-[r:RELATES {type: $relation}]->(t)
                                    SET r.confidence = $confidence,
                                        r.domain = $domain,
                                        r.timestamp = $timestamp,
                                        r.source = $source_type,
                                        r.bidirectional = $bidirectional
                                    """,
                                    {
                                        "source": rel.source,
                                        "target": rel.target,
                                        "relation": rel.relation,
                                        "confidence": float(knowledge.confidence),
                                        "domain": knowledge.domain,
                                        "timestamp": datetime.now().isoformat(),
                                        "source_type": knowledge.metadata.source_type if hasattr(knowledge.metadata, "source_type") else "unknown",
                                        "bidirectional": rel.relation in ["similar_to", "related_to"]
                                    }
                                )
                        
                        # Run in thread pool
                        await asyncio.to_thread(cast(Callable[[], None], add_relationship))
                        graph_metrics["relationships_added"] += 1
                        log_info_with_context(f"Added relationship to graph: {rel.source} -{rel.relation}-> {rel.target}", "Graph")
                        success = True
                        break
                        
                    except Exception as e:
                        if attempt == max_retries - 1:
                            log_error_with_traceback(e, f"Failed to add relationship after {max_retries} attempts: {rel}", include_locals=True)
                            graph_metrics["failed_operations"] += 1
                        await asyncio.sleep(1)  # Wait before retry
                        continue
                
                if not success:
                    continue
            
            # Log final metrics
            log_info_with_context(
                f"Graph operations completed - Added {graph_metrics['entities_added']} entities, "
                f"{graph_metrics['relationships_added']} relationships, "
                f"{graph_metrics['failed_operations']} operations failed",
                "Graph"
            )
            
            # Store metrics in knowledge metadata
            if hasattr(knowledge.metadata, "graph_metrics"):
                knowledge.metadata.graph_metrics = graph_metrics
                    
        except Exception as e:
            log_error_with_traceback(e, "Failed to add knowledge to graph", include_locals=True)
            raise

    async def _generate_embeddings(self, content: str) -> Optional[List[float]]:
        """Generate embeddings for content"""
        try:
            if not self.embeddings:
                raise ValueError("Embeddings model not initialized")
            
            # Generate embeddings using a synchronous function
            def generate_embeddings() -> Optional[List[float]]:
                if not self.embeddings:
                    return None
                result = self.embeddings.embed_documents([content])
                return result[0] if result else None
            
            # Run in thread pool
            embeddings = await asyncio.to_thread(generate_embeddings)
            return embeddings
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to generate embeddings", include_locals=True)
            return None

    async def _extract_entities(self, content: str) -> List[str]:
        """Extract entities from content"""
        try:
            # Clean HTML content first
            soup = BeautifulSoup(content, "html.parser")
            
            # Remove navigation, headers, footers etc
            for element in soup(["nav", "header", "footer", "aside", "script", "style"]):
                element.decompose()
            
            # Get clean text
            clean_content = soup.get_text(separator=" ", strip=True)
            
            # First try using the entity extraction prompt
            prompt = get_entity_extraction_prompt()
            chain = prompt | self.llm | self.entity_parser
            result = await chain.ainvoke({"content": clean_content})
            entities = result.entities if hasattr(result, 'entities') else []
            
            # If no entities found, try using the knowledge extraction prompt as backup
            if not entities:
                knowledge_prompt = get_knowledge_extraction_prompt()
                knowledge_chain = knowledge_prompt | self.llm
                knowledge_result = await knowledge_chain.ainvoke({"text": clean_content})
                if isinstance(knowledge_result, dict) and "entities" in knowledge_result:
                    entities = knowledge_result["entities"]
            
            # If still no entities, extract some basic ones from the content
            if not entities:
                # Extract capitalized phrases and medical terms
                words = clean_content.split()
                phrases = []
                current_phrase = []
                
                medical_indicators = [
                    "disease", "syndrome", "disorder", "treatment", "therapy",
                    "medicine", "drug", "symptom", "patient", "clinical",
                    "medical", "health", "brain", "neural", "immune",
                    "inflammation", "gut", "vagus", "nerve", "axis"
                ]
                
                for word in words:
                    # Check if word starts with capital letter
                    if word[0].isupper() or any(ind in word.lower() for ind in medical_indicators):
                        current_phrase.append(word)
                    elif current_phrase:
                        if len(current_phrase) > 0:
                            phrases.append(" ".join(current_phrase))
                        current_phrase = []
                        
                # Add any remaining phrase
                if current_phrase:
                    phrases.append(" ".join(current_phrase))
                    
                # Use extracted phrases as entities
                entities = list(set(phrases))[:10]  # Limit to top 10 unique entities
                
            # Ensure we have at least one entity
            if not entities:
                entities = ["general_concept"]
                
            return entities
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to extract entities", include_locals=True)
            # Return a default entity rather than empty list
            return ["general_concept"]

    async def _validate_entities(self, entities: List[str], domain: str) -> List[str]:
        """Validate extracted entities against domain ontology."""
        try:
            # Get domain config
            domain_config = next(
                (d for d in self.config.domains if d.name == domain),
                next(iter(self.config.domains))  # Fallback to first domain
            )
            
            # Get valid entity classes from ontology
            valid_classes = {cls.name.lower(): cls for cls in domain_config.classes}
            
            # Track validated entities
            validated = []
            for entity in entities:
                # Clean entity text
                clean_entity = entity.strip()
                if not clean_entity:
                    continue
                    
                # Check if entity matches any ontology class
                entity_type = None
                for class_name, cls in valid_classes.items():
                    # Check direct match
                    if clean_entity.lower() == class_name:
                        entity_type = class_name
                        break
                        
                    # Check if class name appears in entity
                    if class_name in clean_entity.lower():
                        entity_type = class_name
                        break
                        
                    # Check properties
                    for prop in cls.properties:
                        if prop.lower() in clean_entity.lower():
                            entity_type = class_name
                            break
                            
                    if entity_type:
                        break
                        
                # Validate entity
                if entity_type:
                    # Entity matches ontology - keep as is
                    validated.append(clean_entity)
                else:
                    # Try to map to ontology
                    mapped_entity = await self._map_to_ontology(clean_entity, valid_classes)
                    if mapped_entity:
                        validated.append(mapped_entity)
                    else:
                        # Keep original entity if it seems medical/scientific
                        medical_indicators = [
                            "disease", "syndrome", "disorder", "treatment", "therapy",
                            "medicine", "drug", "symptom", "patient", "clinical",
                            "medical", "health", "brain", "neural", "immune",
                            "inflammation", "gut", "vagus", "nerve", "axis"
                        ]
                        if any(ind in clean_entity.lower() for ind in medical_indicators):
                            validated.append(clean_entity)
                    
            # Ensure we have at least one entity
            if not validated:
                validated = ["general_medical_concept"]
                
            return validated
            
        except Exception as e:
            log_error_with_traceback(e, "Error validating entities")
            return entities  # Return original list on error

    async def _map_to_ontology(self, entity: str, valid_classes: Dict[str, Any]) -> Optional[str]:
        """Map an entity to the closest matching ontology class."""
        try:
            # Create mapping prompt
            prompt = ChatPromptTemplate.from_messages([
                ("system", """Map the given entity to the most appropriate ontology class.
The entity should be mapped to one of the following classes:
{classes}

Rules:
1. Only map if there is a clear semantic match
2. Consider synonyms and related terms
3. Return null if no good match exists
4. Maintain proper medical terminology
5. Preserve specificity where possible"""),
                ("human", "Entity to map: {entity}")
            ])
            
            # Get mapping
            result = await self.llm.ainvoke(
                prompt.format_messages(
                    classes="\n".join(f"- {name}: {cls.description}" for name, cls in valid_classes.items()),
                    entity=entity
                )
            )
            
            # Parse result
            if result and isinstance(result, str):
                mapped = result.strip()
                if mapped.lower() in valid_classes:
                    return mapped
                    
            return None
            
        except Exception as e:
            log_error_with_traceback(e, "Error mapping to ontology")
            return None

    async def _extract_relationships(self, content: str, entities: List[str]) -> List[Relationship]:
        """Extract relationships from content"""
        try:
            prompt = get_relationship_extraction_prompt()
            chain = prompt | self.llm | self.relationship_parser
            result = await chain.ainvoke({"content": content, "entities": entities})
            relationships = result.relationships if hasattr(result, 'relationships') else []
            
            # Convert any dict relationships to Relationship objects
            converted_relationships = []
            for rel in relationships:
                try:
                    if isinstance(rel, dict):
                        # Map non-standard relation types to standard ones
                        if 'relation' in rel:
                            relation = rel['relation'].lower()
                            # Map common variations
                            relation_mapping = {
                                # Methodology relationships
                                'used_for': 'uses',
                                'used_in': 'uses',
                                'utilizes': 'uses',
                                'requires': 'uses',
                                'needs': 'uses',
                                'depends_on': 'uses',
                                'applied_to': 'applies',
                                'applied_in': 'applies',
                                'applied_for': 'applies',
                                'implemented_by': 'implements',
                                'implemented_in': 'implements',
                                'implemented_with': 'implements',
                                
                                # Performance relationships
                                'enhances': 'improves',
                                'boosts': 'improves',
                                'increases': 'improves',
                                'optimizes': 'improves',
                                'performs_better': 'outperforms',
                                'better_performance': 'outperforms',
                                'superior_to': 'outperforms',
                                'reaches': 'achieves',
                                'attains': 'achieves',
                                'accomplishes': 'achieves',
                                
                                # Component relationships
                                'includes': 'contains',
                                'incorporates': 'contains',
                                'encompasses': 'contains',
                                'made_of': 'consists_of',
                                'composed_of': 'consists_of',
                                'comprised_of': 'consists_of',
                                'belongs_to': 'part_of',
                                'member_of': 'part_of',
                                'element_of': 'part_of',
                                
                                # Comparison relationships
                                'superior_to': 'better_than',
                                'exceeds': 'better_than',
                                'outranks': 'better_than',
                                'resembles': 'similar_to',
                                'like': 'similar_to',
                                'analogous_to': 'similar_to',
                                'differs_from': 'different_from',
                                'unlike': 'different_from',
                                'distinct_from': 'different_from',
                                
                                # Causal relationships
                                'results_in': 'leads_to',
                                'produces': 'causes',
                                'creates': 'causes',
                                'generates': 'causes',
                                'impacts': 'affects',
                                'influences': 'affects',
                                'modifies': 'affects',
                                
                                # Temporal relationships
                                'comes_before': 'precedes',
                                'before': 'precedes',
                                'prior_to': 'precedes',
                                'comes_after': 'follows',
                                'after': 'follows',
                                'subsequent_to': 'follows',
                                'happens_with': 'concurrent_with',
                                'simultaneous_with': 'concurrent_with',
                                'parallel_to': 'concurrent_with',
                                
                                # Legacy relationships
                                'type_of': 'is_a',
                                'kind_of': 'is_a',
                                'instance_of': 'is_a',
                                'contains_part': 'has_part',
                                'includes_part': 'has_part',
                                'possesses': 'has_part',
                                'connected_to': 'related_to',
                                'associated_with': 'related_to',
                                'linked_to': 'related_to'
                            }
                            
                            # Try to map the relation, fallback to related_to if no mapping found
                            rel['relation'] = relation_mapping.get(relation, 'related_to')
                            
                            # If relation is still not valid, use related_to as fallback
                            valid_relations = [
                                'uses', 'applies', 'implements',
                                'improves', 'outperforms', 'achieves',
                                'contains', 'consists_of', 'part_of',
                                'better_than', 'similar_to', 'different_from',
                                'leads_to', 'causes', 'affects',
                                'precedes', 'follows', 'concurrent_with',
                                'is_a', 'has_part', 'related_to'
                            ]
                            if rel['relation'] not in valid_relations:
                                log_warning_with_context(
                                    f"Invalid relation type '{rel['relation']}' mapped to 'related_to'",
                                    "Relationship Extraction"
                                )
                                rel['relation'] = 'related_to'
                            
                        # Validate source and target
                        if not rel.get('source') or not rel.get('target'):
                            log_warning_with_context(
                                f"Missing source or target in relationship: {rel}",
                                "Relationship Extraction"
                            )
                            continue
                            
                        # Validate relationship makes sense
                        if rel['source'] == rel['target']:
                            log_warning_with_context(
                                f"Self-referential relationship detected: {rel}",
                                "Relationship Extraction"
                            )
                            continue
                            
                        # Create Relationship object with mapped relation
                        converted_rel = Relationship(**rel)
                        
                        # Evaluate relationship confidence
                        confidence = await self._evaluate_relationship_confidence(
                            converted_rel,
                            content
                        )
                        converted_rel.confidence = confidence
                        
                        # Only add if confidence meets threshold
                        if confidence >= self.config.confidence_thresholds.get("relationship", 0.3):
                            converted_relationships.append(converted_rel)
                        else:
                            log_warning_with_context(
                                f"Relationship confidence too low: {confidence}",
                                "Relationship Extraction"
                            )
                    else:
                        converted_relationships.append(rel)
                except Exception as e:
                    log_error_with_traceback(e, f"Failed to convert relationship: {rel}", include_locals=True)
                    continue
            
            return converted_relationships
        except Exception as e:
            log_error_with_traceback(e, "Failed to extract relationships", include_locals=True)
            return []

    async def _evaluate_relationship_confidence(self, relationship: Relationship, context: str) -> float:
        """Evaluate confidence in a relationship."""
        try:
            # Check if relationship type is appropriate for entities
            type_confidence = 0.8  # Base confidence for valid relationship type
            
            # Check if entities are mentioned close to each other in context
            proximity_confidence = 0.0
            source_pos = context.lower().find(relationship.source.lower())
            target_pos = context.lower().find(relationship.target.lower())
            if source_pos >= 0 and target_pos >= 0:
                distance = abs(source_pos - target_pos)
                # Higher confidence for closer entities (more lenient distance scaling)
                proximity_confidence = max(0.0, 1.0 - (distance / 2000))  # Increased from 1000
            
            # Check if relationship is explicitly stated
            explicit_confidence = 0.0
            relation_terms = {
                'uses': ['uses', 'utilizing', 'employs', 'requires'],
                'applies': ['applies', 'applying', 'application'],
                'implements': ['implements', 'implementation'],
                'improves': ['improves', 'enhances', 'boosts'],
                'outperforms': ['outperforms', 'better than', 'superior to'],
                'achieves': ['achieves', 'attains', 'reaches'],
                'contains': ['contains', 'includes', 'incorporates'],
                'consists_of': ['consists of', 'made of', 'composed of'],
                'part_of': ['part of', 'belongs to', 'member of'],
                'better_than': ['better than', 'superior to', 'exceeds'],
                'similar_to': ['similar to', 'like', 'resembles'],
                'different_from': ['different from', 'unlike', 'distinct from'],
                'leads_to': ['leads to', 'results in', 'causes'],
                'causes': ['causes', 'produces', 'creates'],
                'affects': ['affects', 'impacts', 'influences'],
                'precedes': ['precedes', 'before', 'prior to'],
                'follows': ['follows', 'after', 'subsequent to'],
                'concurrent_with': ['concurrent with', 'simultaneous', 'parallel'],
                'is_a': ['is a', 'type of', 'kind of'],
                'has_part': ['has part', 'contains part', 'includes part'],
                'related_to': ['related to', 'connected to', 'associated with']
            }
            
            # Look for explicit relationship terms
            terms = relation_terms.get(relationship.relation, [])
            for term in terms:
                if term.lower() in context.lower():
                    explicit_confidence = 0.9
                    break
            
            # If no explicit terms found but entities are close, give some confidence
            if explicit_confidence == 0.0 and proximity_confidence > 0.5:
                explicit_confidence = 0.4  # Added base confidence for proximity
                
            # Calculate overall confidence with adjusted weights
            confidence = (
                type_confidence * 0.4 +  # Increased from 0.33
                proximity_confidence * 0.3 +  # Same weight
                explicit_confidence * 0.3  # Decreased from 0.33
            )
            
            return min(1.0, max(0.0, confidence))
            
        except Exception as e:
            log_error_with_traceback(e, "Error evaluating relationship confidence")
            return 0.5  # Default confidence on error

    async def _generate_metadata(self, content: str) -> Optional[SourceMetadata]:
        """Generate metadata for content"""
        try:
            prompt = get_metadata_generation_prompt()
            chain = prompt | self.llm | self.metadata_parser
            result = await chain.ainvoke({"text": content})
            return result.metadata if hasattr(result, 'metadata') else None
        except Exception as e:
            log_error_with_traceback(e, "Failed to generate metadata", include_locals=True)
            return None

    def _create_default_metadata(self) -> SourceMetadata:
        """Create default metadata object."""
        return SourceMetadata(
            source_type="text",
            confidence_score=0.8,  # Increased from 0.5
            domain_relevance=0.8,  # Increased from 0.5
            timestamp=datetime.now().isoformat(),
            validation_status="pending",
            domain=self.config.domains[0].name if self.config.domains else "medical"  # Use first configured domain or fallback to medical
        )

    async def _ensure_initialized(self) -> None:
        """Ensure all components are initialized"""
        if not self.llm or not self.embeddings or not self.vector_store or not self.graph:
            log_info_with_context("Initializing missing components", "Initialization")
            await self.initialize()

    async def _load_chunks(self, source_path: str, source_type: str = "text") -> List[str]:
        """Load and split content into chunks"""
        try:
            # Load content
            loader = TextLoader(source_path)
            docs = await asyncio.to_thread(loader.load)
            
            # Split into chunks with metadata
            chunks = []
            for doc in tqdm(docs, desc="Processing documents"):
                # Get metadata
                metadata = doc.metadata.copy()
                
                # Split content
                text_chunks = self.text_splitter.split_text(doc.page_content)
                
                # Create chunks with metadata
                for i, chunk in enumerate(text_chunks):
                    chunk_metadata = metadata.copy()
                    chunk_metadata.update({
                        "chunk_id": f"{metadata.get('source_id', 'unknown')}_{i}",
                        "chunk_index": i,
                        "total_chunks": len(text_chunks)
                    })
                    chunks.append(Document(page_content=chunk, metadata=chunk_metadata))
            
            return [chunk.page_content for chunk in chunks]
            
        except Exception as e:
            log_error_with_traceback(e, f"Failed to load chunks from {source_path}", include_locals=True)
            return []

    async def _evaluate_confidence(self, entities: List[str], relationships: List[Relationship]) -> float:
        """Evaluate confidence in extracted knowledge"""
        max_retries = 3
        base_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                prompt = get_confidence_evaluation_prompt()
                chain = prompt | self.llm | PydanticOutputParser(pydantic_object=ConfidenceEvaluation)
                
                # Convert relationships to dicts
                rel_dicts = []
                for rel in relationships:
                    if isinstance(rel, dict):
                        rel_dicts.append(rel)
                    else:
                        # Assuming rel is a Pydantic model
                        rel_dicts.append({
                            'source': rel.source,
                            'relation': rel.relation,
                            'target': rel.target,
                            'confidence': getattr(rel, 'confidence', 1.0)
                        })
                
                result = await chain.ainvoke({
                    "entities": entities,
                    "relationships": rel_dicts,
                    "source_type": "text"
                })
                return result.confidence
            except Exception as e:
                if isinstance(e, google.api_core.exceptions.ResourceExhausted):
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)  # Exponential backoff
                        log_warning_with_context(
                            f"Rate limit hit, retrying in {delay} seconds (attempt {attempt + 1}/{max_retries})",
                            "Confidence Evaluation"
                        )
                        await asyncio.sleep(delay)
                        continue
                
                # If we've exhausted retries or hit a different error, fall back to calculated confidence
                log_error_with_traceback(e, "Failed to evaluate confidence", include_locals=True)
                
                # Calculate fallback confidence based on available data
                entity_confidence = min(1.0, len(entities) * 0.2) if entities else 0.3
                relationship_confidence = min(1.0, len(relationships) * 0.2) if relationships else 0.3
                
                # Weight the components
                confidence = (entity_confidence * 0.6) + (relationship_confidence * 0.4)
                
                log_warning_with_context(
                    f"Using fallback confidence calculation: {confidence:.2f}",
                    "Confidence Evaluation"
                )
                return confidence
        
        # If we somehow exit the loop without returning, use a safe default
        return 0.5
```

.\scripts\llm_compiler.py
```python
"""LLM compiler system."""

import os
from typing import Any, Dict, List, Optional, Union, cast, Callable, Awaitable
from langchain_core.output_parsers import PydanticOutputParser
import json
from langchain_ollama import ChatOllama
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table
from rich import box
from loguru import logger
from pydantic import ValidationError, SecretStr
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, BaseMessage
from langchain_core.language_models.chat_models import BaseChatModel
from rich.progress import Progress

from prompts.compiler import (
    Task,
    Plan,
    TaskResult,
    JoinDecision,
    CompilerState,
    get_plan_generation_prompt,
    get_task_execution_prompt,
    get_join_decision_prompt
)

from scripts.chat_langchain import ChatLangChain
from scripts.logging_config import (
    log_info_with_context,
    log_warning_with_context,
    log_error_with_context,
    log_error_with_traceback,
    create_progress,
    cleanup_progress,
    console
)

class CompilerError(Exception):
    """Base class for compiler errors"""
    pass

class PlanningError(CompilerError):
    """Error during plan generation"""
    pass

class ExecutionError(CompilerError):
    """Error during task execution"""
    pass

class DecisionError(CompilerError):
    """Error during join decision"""
    pass

class LLMCompiler:
    """LLM compiler system."""

    def __init__(self, llm: BaseChatModel):
        """Initialize with language model."""
        try:
            if not llm:
                raise CompilerError("LLM instance is required")
                
            self.llm = llm
            self.state: CompilerState = {
                "content": "",
                "domain_name": "",
                "plan": None,
                "results": [],
                "join_decision": None,
                "final_result": None,
                "error": None,
                "feedback": None,
                "knowledge_sources": [],
                "synthetic_knowledge": [],
                "training_examples": [],
                "model_metrics": {}
            }
            self.tools: Dict[str, Callable[..., Awaitable[Dict[str, Any]]]] = {}
            log_info_with_context("Initialized LLM compiler", "Compiler")
            console.print(Panel("LLM Compiler Initialized", style="bold green"))
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to initialize compiler", include_locals=True)
            raise CompilerError("Failed to initialize compiler") from e

    def register_tool(self, name: str, func: Callable[..., Awaitable[Dict[str, Any]]]):
        """Register a tool with the compiler."""
        self.tools[name] = func
        log_info_with_context(f"Registered tool: {name}", "Compiler")

    def _log_plan(self, plan: Plan):
        """Log plan details in a rich format"""
        try:
            table = Table(title="[bold]Execution Plan[/bold]", box=box.ROUNDED)
            table.add_column("Task ID", style="cyan")
            table.add_column("Tool", style="green")
            table.add_column("Dependencies", style="yellow")
            table.add_column("Arguments", style="magenta")
            
            for task in plan.tasks:
                table.add_row(
                    str(task.idx),
                    task.tool,
                    str(task.dependencies),
                    str(task.args)
                )
            
            console.print(table)
            console.print(Panel(f"Planning Thought: {plan.thought}", title="[bold]Planning Reasoning[/bold]", border_style="blue"))
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to log plan", include_locals=True)

    def _log_task_result(self, result: TaskResult):
        """Log task result in a rich format"""
        try:
            if result.error:
                console.print(Panel(
                    f"[red]Error:[/red] {result.error}",
                    title=f"[bold red]Task {result.task_id} Failed[/bold red]",
                    border_style="red"
                ))
            else:
                console.print(Panel(
                    Syntax(str(result.result), "python", theme="monokai"),
                    title=f"[bold green]Task {result.task_id} Completed[/bold green]",
                    border_style="green"
                ))
                
        except Exception as e:
            log_error_with_traceback(e, "Failed to log task result", include_locals=True)

    def _log_join_decision(self, decision: JoinDecision):
        """Log join decision in a rich format"""
        try:
            status = "[green]Complete[/green]" if decision.complete else "[yellow]Replan[/yellow]" if decision.replan else "[red]Unknown[/red]"
            console.print(Panel(
                f"Status: {status}\nThought: {decision.thought}\nFeedback: {decision.feedback or 'None'}",
                title="[bold]Join Decision[/bold]",
                border_style="cyan"
            ))
            
        except Exception as e:
            log_error_with_traceback(e, "Failed to log join decision", include_locals=True)

    def _format_state(self, state: Any) -> Dict[str, Any]:
        """Format state for LLM input."""
        if hasattr(state, "__fields__"):  # If it's a Pydantic model
            formatted_state = {}
            for field in state.__fields__:
                if hasattr(state, field):
                    value = getattr(state, field)
                    if isinstance(value, str):
                        formatted_state[field] = f"{{state[{field}]}}"
                    else:
                        formatted_state[field] = value
            return formatted_state
        elif isinstance(state, dict):  # If it's a dictionary
            formatted_state = {}
            for key, value in state.items():
                if isinstance(value, str):
                    formatted_state[key] = f"{{state[{key}]}}"
                else:
                    formatted_state[key] = value
            return formatted_state
        else:
            raise ValueError(f"Unsupported state type: {type(state)}")

    async def generate_plan(self, state: CompilerState) -> Plan:
        """Generate a plan based on the current state."""
        try:
            # Format state for prompt using _format_state helper
            formatted_state = self._format_state(state)
            
            # Get predefined prompt template
            prompt = get_plan_generation_prompt()
            
            # Create chain with JSON formatting
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=Plan)
            
            # Generate plan with thought field
            response = await chain.ainvoke({
                "state": formatted_state,
                "domain_name": state.get("domain_name", "")  # Pass domain_name explicitly
            })
            
            # Validate and fix state variable format in task args
            for task in response.tasks:
                for key, value in task.args.items():
                    if isinstance(value, str):
                        if value == "{state}":
                            if task.tool == "research_topics" and key == "domain":
                                task.args[key] = state.get("domain_name", "")  # Use actual domain name
                            else:
                                error_msg = f"Invalid state variable format in task {task.idx}. Use {{state[variable_name]}} instead of {{state}}"
                                log_error_with_context(error_msg, "Planning")
                                raise PlanningError(error_msg)
                        elif "{state." in value:
                            error_msg = f"Invalid state variable format in task {task.idx}. Use {{state[variable_name]}} instead of {value}"
                            log_error_with_context(error_msg, "Planning")
                            raise PlanningError(error_msg)
                        elif value == "{state[domain_name]}":
                            task.args[key] = state.get("domain_name", "")  # Use actual domain name
            
            return response

        except Exception as e:
            log_error_with_traceback(e, "Failed to generate plan")
            raise PlanningError("Failed to generate plan") from e

    def _parse_plan_response(self, response: Union[str, Plan, Dict[str, Any]]) -> Plan:
        """Parse and validate plan response."""
        try:
            if isinstance(response, Plan):
                return response
                
            if isinstance(response, str):
                # Handle JSON string
                try:
                    if response.startswith('"{') and response.endswith('}"'):
                        response = response[1:-1]
                    parsed = json.loads(response)
                except json.JSONDecodeError as e:
                    raise PlanningError(f"Invalid JSON format: {str(e)}")
            else:
                parsed = response
                
            # Validate parsed response
            if not isinstance(parsed, dict):
                raise PlanningError("Response must be a dictionary")
                
            if "tasks" not in parsed:
                raise PlanningError("Response missing 'tasks' field")
                
            if "thought" not in parsed:
                raise PlanningError("Response missing 'thought' field")
                
            # Validate tasks array
            if not isinstance(parsed["tasks"], list):
                raise PlanningError("'tasks' must be an array")
                
            # Validate and fix each task
            for task in parsed["tasks"]:
                if not isinstance(task, dict):
                    raise PlanningError("Each task must be a dictionary")
                    
                # Validate required task fields
                required_fields = ["idx", "tool", "args", "dependencies"]
                for field in required_fields:
                    if field not in task:
                        raise PlanningError(f"Task missing required field: {field}")
                        
                # Ensure args is a dictionary
                if not isinstance(task["args"], dict):
                    if isinstance(task["args"], list) and len(task["args"]) == 1:
                        try:
                            task["args"] = json.loads(task["args"][0])
                        except:
                            raise PlanningError(f"Invalid args format for task {task['idx']}")
                    else:
                        raise PlanningError(f"Args must be a dictionary for task {task['idx']}")
                        
                # Validate state variable format
                for key, value in task["args"].items():
                    if isinstance(value, str):
                        if value == "{state}":
                            error_msg = f"Invalid state variable format in task {task['idx']}. Use {{state[variable_name]}} instead of {{state}}"
                            log_error_with_context(error_msg, "Planning")
                            raise PlanningError(error_msg)
                        elif "{state." in value:
                            error_msg = f"Invalid state variable format in task {task['idx']}. Use {{state[variable_name]}} instead of {value}"
                            log_error_with_context(error_msg, "Planning")
                            raise PlanningError(error_msg)
                        
                # Ensure dependencies is a list
                if not isinstance(task["dependencies"], list):
                    raise PlanningError(f"Dependencies must be a list for task {task['idx']}")
                    
            # Create Plan object
            try:
                return Plan(
                    tasks=[Task(**task) for task in parsed["tasks"]],
                    thought=parsed["thought"]
                )
            except ValidationError as e:
                raise PlanningError(f"Invalid plan format: {str(e)}")
                
        except Exception as e:
            log_error_with_traceback(e, "Failed to parse plan response", include_locals=True)
            if isinstance(e, PlanningError):
                raise
            raise PlanningError("Failed to parse plan response") from e

    async def execute_tasks(self, tasks: List[Task], state: CompilerState) -> List[TaskResult]:
        """Execute a list of tasks in order."""
        progress = Progress()
        task_progress = progress.add_task("Executing tasks...", total=len(tasks))
        results = []
        result_map = {}
        
        for task in tasks:
            try:
                # Check dependencies
                deps_met = True
                for dep in task.dependencies:
                    if dep not in result_map:
                        logger.warning(f"Dependency {dep} not found for task {task.idx}")
                        deps_met = False
                        break
                
                if not deps_met:
                    logger.warning(f"Dependencies not met for task {task.idx}")
                    continue
                
                # Format arguments using dependency results
                formatted_args = {}
                for arg_name, arg_value in task.args.items():
                    if isinstance(arg_value, list) and len(arg_value) > 0 and isinstance(arg_value[0], dict) and "id" in arg_value[0]:
                        # This is a reference to a dependency result
                        dep_id = arg_value[0]["id"]
                        if dep_id in result_map:
                            dep_result = result_map[dep_id]
                            if task.tool == "_train_model" and "training_examples" in dep_result:
                                formatted_args["examples"] = dep_result["training_examples"]
                            elif task.tool == "_generate_examples" and "synthetic_knowledge" in dep_result:
                                formatted_args["knowledge"] = dep_result["synthetic_knowledge"]
                            else:
                                formatted_args[arg_name] = dep_result
                        else:
                            formatted_args[arg_name] = arg_value
                    else:
                        formatted_args[arg_name] = arg_value
                
                # Execute task
                result = await getattr(self, task.tool)(**formatted_args)
                results.append(TaskResult(task_id=task.idx, result=result, error=None))
                result_map[task.idx] = result
                progress.update(task_progress, advance=1)
                
            except Exception as e:
                error_msg = f"Task {task.idx} failed: {str(e)}"
                logger.error(error_msg)
                logger.error(f"Local Variables: {locals()}")
                results.append(TaskResult(task_id=task.idx, result=None, error=str(e)))
                progress.update(task_progress, advance=1)
        
        return results

    def _parse_task_result(self, response: Union[str, TaskResult, Dict[str, Any]], task_id: int) -> TaskResult:
        """Parse and validate task result."""
        try:
            if isinstance(response, TaskResult):
                return response
                
            if isinstance(response, str):
                try:
                    # Handle double-encoded JSON
                    parsed = json.loads(response)
                    if isinstance(parsed, str):
                        parsed = json.loads(parsed)
                except json.JSONDecodeError as e:
                    raise ExecutionError(f"Invalid JSON format: {str(e)}")
            else:
                parsed = response
                
            # Validate parsed response
            if not isinstance(parsed, dict):
                raise ExecutionError("Response must be a dictionary")
                
            # Create TaskResult object
            try:
                result = TaskResult(
                    task_id=task_id,
                    result=parsed.get("result"),
                    error=parsed.get("error")
                )
                return result
            except ValidationError as e:
                raise ExecutionError(f"Invalid task result format: {str(e)}")
                
        except Exception as e:
            log_error_with_traceback(e, "Failed to parse task result", include_locals=True)
            if isinstance(e, ExecutionError):
                raise
            raise ExecutionError("Failed to parse task result") from e

    async def make_join_decision(self, state: CompilerState) -> JoinDecision:
        """Make join decision."""
        try:
            log_info_with_context("Making join decision", "Decision")
            console.print("\n[bold green]Making Join Decision...[/bold green]")
            
            # Format state for LLM
            try:
                formatted_state = self._format_state(state)
            except Exception as e:
                log_error_with_traceback(e, "Failed to format state", include_locals=True)
                raise DecisionError("Failed to format state for decision") from e
            
            # Get decision from LLM
            try:
                prompt = get_join_decision_prompt()
                chain = prompt | self.llm | PydanticOutputParser(pydantic_object=JoinDecision)
                response = await chain.ainvoke({"state": formatted_state})
            except Exception as e:
                log_error_with_traceback(e, "Failed to get decision from LLM", include_locals=True)
                raise DecisionError("Failed to make decision") from e
            
            # Parse and validate response
            try:
                decision = self._parse_join_decision(response)
                self._log_join_decision(decision)
                return decision
            except Exception as e:
                log_error_with_traceback(e, "Failed to parse decision response", include_locals=True)
                raise DecisionError("Failed to parse decision response") from e
            
        except Exception as e:
            log_error_with_traceback(e, "Error in join decision", include_locals=True)
            if isinstance(e, (DecisionError, CompilerError)):
                raise
            raise DecisionError("Join decision failed") from e

    def _parse_join_decision(self, response: Union[str, JoinDecision, Dict[str, Any]]) -> JoinDecision:
        """Parse and validate join decision."""
        try:
            if isinstance(response, JoinDecision):
                return response
                
            if isinstance(response, str):
                try:
                    if response.startswith('"{') and response.endswith('}"'):
                        response = response[1:-1]
                    parsed = json.loads(response)
                except json.JSONDecodeError as e:
                    raise DecisionError(f"Invalid JSON format: {str(e)}")
            else:
                parsed = response
                
            # Validate parsed response
            if not isinstance(parsed, dict):
                raise DecisionError("Response must be a dictionary")
                
            # Create JoinDecision object
            try:
                return JoinDecision(
                    complete=parsed.get("complete", False),
                    replan=parsed.get("replan", False),
                    thought=parsed.get("thought", ""),
                    feedback=parsed.get("feedback")
                )
            except ValidationError as e:
                raise DecisionError(f"Invalid join decision format: {str(e)}")
                
        except Exception as e:
            log_error_with_traceback(e, "Failed to parse join decision", include_locals=True)
            if isinstance(e, DecisionError):
                raise
            raise DecisionError("Failed to parse join decision") from e

    async def run(self, initial_state: Dict[str, Any]) -> Any:
        """Run the compiler workflow."""
        try:
            if not initial_state:
                raise CompilerError("Initial state is required")
                
            # Convert initial state to CompilerState and set it
            self.state = {
                "content": initial_state.get("content", ""),
                "domain_name": initial_state.get("domain_name", ""),
                "plan": None,
                "results": [],
                "join_decision": None,
                "final_result": None,
                "error": None,
                "feedback": None,
                "knowledge_sources": initial_state.get("knowledge_sources", []),
                "synthetic_knowledge": initial_state.get("synthetic_knowledge", []),
                "training_examples": initial_state.get("training_examples", []),
                "model_metrics": initial_state.get("model_metrics", {})
            }
            
            # Generate plan
            try:
                plan = await self.generate_plan(self.state)
                self.state["plan"] = plan
                self._log_plan(plan)
            except Exception as e:
                log_error_with_traceback(e, "Failed to generate plan", include_locals=True)
                if isinstance(e, PlanningError):
                    raise
                raise PlanningError("Failed to generate plan") from e
                
            # Execute tasks
            try:
                results = await self.execute_tasks(plan.tasks, self.state)
                self.state["results"] = results
                
                # Log failed tasks
                failed_tasks = [r for r in results if r.error]
                if failed_tasks:
                    console.print("\n[bold red]Task Failures:[/bold red]")
                    for task in failed_tasks:
                        console.print(f"Task {task.task_id} failed: {task.error}")
                        
            except Exception as e:
                log_error_with_traceback(e, "Failed to execute tasks", include_locals=True)
                if isinstance(e, ExecutionError):
                    raise
                raise ExecutionError("Failed to execute tasks") from e
                
            # Make join decision
            try:
                decision = await self.make_join_decision(self.state)
                self.state["join_decision"] = decision
                
                if decision.replan:
                    console.print(f"\nReplanning based on feedback: {decision.feedback}")
                    # Convert state back to dict for recursive call
                    return await self.run(dict(self.state))
                    
                if decision.complete:
                    return self.state
                    
            except Exception as e:
                log_error_with_traceback(e, "Failed to make join decision", include_locals=True)
                if isinstance(e, DecisionError):
                    raise
                raise DecisionError("Failed to make join decision") from e
                
            return self.state
            
        except Exception as e:
            log_error_with_traceback(e, "Error in compiler workflow", include_locals=True)
            if isinstance(e, CompilerError):
                raise
            raise CompilerError("Compiler workflow failed") from e

    async def _generate_final_result(self, state: CompilerState) -> Any:
        """Generate final result."""
        try:
            log_info_with_context("Generating final result", "Compiler")
            
            # Get successful results
            results = [r for r in state.get("results", []) if not r.error]
            if not results:
                log_warning_with_context("No successful results to generate final result", "Compiler", include_locals=True)
                return None
                
            # Combine results
            final_result = results[-1].result
            log_info_with_context("Generated final result", "Compiler")
            return final_result
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating final result", include_locals=True)
            if isinstance(e, CompilerError):
                raise
            raise CompilerError("Failed to generate final result") from e

    async def _execute_task(self, tool: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a single task."""
        try:
            # Get the tool function
            tool_func = self.tools.get(tool)
            if not tool_func:
                raise ExecutionError(f"Tool {tool} not found")
            
            # Execute the tool
            try:
                result = await tool_func(**args)
                return result
            except Exception as e:
                raise ExecutionError(f"Tool execution failed: {str(e)}")
            
        except Exception as e:
            raise ExecutionError(f"Task execution failed: {str(e)}")


```

.\scripts\logging_config.py
```python
import sys
from pathlib import Path
from traceback import format_exception
from loguru import logger
from rich.console import Console
from rich.traceback import install as install_rich_traceback
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich.syntax import Syntax
from rich.tree import Tree
from rich import box
import json
from typing import Any, Dict, Optional
from datetime import datetime
import asyncio

# Install rich traceback handler with more detailed settings
install_rich_traceback(
    show_locals=True,
    width=120,
    word_wrap=True,
    extra_lines=3,
    theme=None,
    max_frames=20
)

# Create console for rich output
console = Console()

# Create a single shared progress instance
_shared_progress = None
_progress_lock = asyncio.Lock()

async def create_progress() -> Progress:
    """Create or get the shared progress instance"""
    global _shared_progress
    async with _progress_lock:
        if _shared_progress is None:
            _shared_progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(complete_style="green", finished_style="bright_green"),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeRemainingColumn(),
                console=console,
                expand=True,
                refresh_per_second=10,
                auto_refresh=False  # Disable auto refresh to prevent conflicts
            )
            _shared_progress.start()
        return _shared_progress

async def cleanup_progress():
    """Clean up the shared progress instance"""
    global _shared_progress
    async with _progress_lock:
        if _shared_progress is not None:
            _shared_progress.stop()
            _shared_progress = None

def setup_logging(log_file: Optional[str] = None):
    """Configure logging with both file and console output"""
    # Remove default handler
    logger.remove()
    
    # Generate log file name if not provided
    if log_file is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = f"logs/debug_{timestamp}.log"
    
    # Ensure logs directory exists
    Path("logs").mkdir(exist_ok=True)
    
    # Add rich console handler with enhanced format
    logger.add(
        lambda msg: console.print(Panel(
            msg,
            border_style="blue",
            title=f"[cyan]{msg.record['time'].strftime('%Y-%m-%d %H:%M:%S')}[/cyan]",
            subtitle=f"[yellow]{msg.record['name']}:{msg.record['function']}:{msg.record['line']}[/yellow]"
        )),
        format="<level>{message}</level>",
        level="INFO",
        backtrace=True,
        diagnose=True,
        colorize=True,
        catch=True  # Catch exceptions in handlers
    )
    
    # Add file handler with detailed format including process and thread IDs
    logger.add(
        log_file,
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {process}:{thread} | {level: <8} | {name}:{function}:{line} | {message}",
        level="DEBUG",
        backtrace=True,
        diagnose=True,
        rotation="1 day",
        retention="30 days",
        compression="zip",
        catch=True,
        enqueue=True  # Thread-safe logging
    )
    
    # Add error file handler for critical errors
    error_log = log_file.replace("debug", "error")
    logger.add(
        error_log,
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {process}:{thread} | {level: <8} | {name}:{function}:{line} | {message}",
        level="ERROR",
        backtrace=True,
        diagnose=True,
        rotation="1 day",
        retention="30 days",
        compression="zip",
        catch=True,
        enqueue=True
    )

def log_error_with_traceback(e: Exception, context: str = "", include_locals: bool = True):
    """Log error with full traceback and local variables"""
    error_msg = f"{context}: {str(e)}"
    exc_info = (type(e), e, e.__traceback__)
    
    # Log to loguru with full context
    logger.opt(exception=exc_info, depth=1).error(error_msg)
    
    # Create detailed error panel
    error_details = []
    error_details.append(f"[red]Error Type:[/red] {type(e).__name__}")
    error_details.append(f"[red]Error Message:[/red] {str(e)}")
    error_details.append(f"[red]Context:[/red] {context}")
    
    if include_locals and hasattr(e, "__traceback__"):
        tb = e.__traceback__
        while tb:
            frame = tb.tb_frame
            error_details.append("\n[yellow]Local Variables:[/yellow]")
            for key, value in frame.f_locals.items():
                try:
                    value_str = str(value)
                    if len(value_str) > 200:
                        value_str = value_str[:200] + "..."
                    error_details.append(f"  [blue]{key}[/blue] = {value_str}")
                except:
                    error_details.append(f"  [blue]{key}[/blue] = <unprintable value>")
            tb = tb.tb_next
    
    # Print rich error panel
    console.print()
    console.print(Panel(
        "\n".join(error_details),
        title="[red]Error Details[/red]",
        border_style="red",
        padding=(1, 2)
    ))
    
    # Print traceback
    console.print()
    console.print(Panel(
        Syntax(
            "".join(format_exception(*exc_info)),
            "python",
            theme="monokai",
            line_numbers=True,
            word_wrap=True
        ),
        title="[red]Full Traceback[/red]",
        border_style="red"
    ))

def log_warning_with_context(msg: str, context: str = "", include_locals: bool = False):
    """Log warning with context and optionally local variables"""
    warning_msg = f"{context}: {msg}"
    
    # Log to loguru
    logger.opt(depth=1).warning(warning_msg)
    
    if include_locals:
        # Get caller's frame
        frame = sys._getframe(1)
        locals_msg = "\nLocal Variables:\n"
        for key, value in frame.f_locals.items():
            try:
                value_str = str(value)
                if len(value_str) > 200:
                    value_str = value_str[:200] + "..."
                locals_msg += f"  {key} = {value_str}\n"
            except:
                locals_msg += f"  {key} = <unprintable value>\n"
        logger.opt(depth=1).warning(locals_msg)

def log_info_with_context(msg: str, context: str = "", include_locals: bool = False):
    """Log info with context and optionally local variables"""
    info_msg = f"{context}: {msg}"
    
    # Log to loguru
    logger.opt(depth=1).info(info_msg)
    
    if include_locals:
        # Get caller's frame
        frame = sys._getframe(1)
        locals_msg = "\nLocal Variables:\n"
        for key, value in frame.f_locals.items():
            try:
                value_str = str(value)
                if len(value_str) > 200:
                    value_str = value_str[:200] + "..."
                locals_msg += f"  {key} = {value_str}\n"
            except:
                locals_msg += f"  {key} = <unprintable value>\n"
        logger.opt(depth=1).info(locals_msg)

def log_error_with_context(msg: str, context: str = "", include_locals: bool = False):
    """Log error with context and optionally local variables"""
    error_msg = f"{context}: {msg}"
    
    # Log to loguru
    logger.opt(depth=1).error(error_msg)
    
    if include_locals:
        # Get caller's frame
        frame = sys._getframe(1)
        locals_msg = "\nLocal Variables:\n"
        for key, value in frame.f_locals.items():
            try:
                value_str = str(value)
                if len(value_str) > 200:
                    value_str = value_str[:200] + "..."
                locals_msg += f"  {key} = {value_str}\n"
            except:
                locals_msg += f"  {key} = <unprintable value>\n"
        logger.opt(depth=1).error(locals_msg)

def log_extraction_results(knowledge: Any):
    """Log knowledge extraction results"""
    if not knowledge:
        return
        
    # Create tree view of results
    tree = Tree("Knowledge Extraction Results")
    
    # Add entities
    entities = tree.add("Entities")
    for entity in getattr(knowledge, "entities", []):
        entities.add(f"[cyan]{entity}[/cyan]")
        
    # Add relationships
    relationships = tree.add("Relationships")
    for rel in getattr(knowledge, "relationships", []):
        rel_str = f"[yellow]{rel.source}[/yellow] -> [green]{rel.relation}[/green] -> [yellow]{rel.target}[/yellow]"
        relationships.add(rel_str)
        
    # Add metadata
    if hasattr(knowledge, "metadata"):
        metadata = tree.add("Metadata")
        for k, v in knowledge.metadata.dict().items():
            metadata.add(f"[blue]{k}[/blue]: {v}")
            
    # Print tree
    console.print()
    console.print(Panel(tree, title="Extraction Results", border_style="cyan"))

def log_confidence_evaluation(result: Dict):
    """Log confidence evaluation results in a beautiful format"""
    # Create confidence table
    table = Table(
        title="[bold]Confidence Evaluation[/bold]",
        box=box.ROUNDED,
        show_header=True,
        header_style="bold"
    )
    
    table.add_column("Factor", style="cyan")
    table.add_column("Score", style="green")
    
    factors = result.get("factors", {})
    table.add_row("Content Quality", f"{factors.get('content_quality', 0.0):.2f}")
    table.add_row("Entity Confidence", f"{factors.get('entity_confidence', 0.0):.2f}")
    table.add_row("Relationship Validity", f"{factors.get('relationship_validity', 0.0):.2f}")
    table.add_row("Source Reliability", f"{factors.get('source_reliability', 0.0):.2f}")
    table.add_row("Context Relevance", f"{factors.get('context_relevance', 0.0):.2f}")
    table.add_row("Overall Confidence", f"{result.get('confidence', 0.0):.2f}")
    
    # Create reasoning panel
    reasoning_panel = Panel(
        result.get("reasoning", "No reasoning provided"),
        title="[bold]Confidence Reasoning[/bold]",
        border_style="blue"
    )
    
    # Print results
    console.print("\n")
    console.print(table)
    console.print(reasoning_panel)
    console.print("\n") 
```

.\scripts\lora_training.py
```python
from typing import List, Dict, Optional, Any, Literal, cast, Union, TYPE_CHECKING
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    PreTrainedModel,
    PreTrainedTokenizer,
    PreTrainedTokenizerBase
)
from datasets import Dataset
from peft.tuners.lora import LoraConfig
from peft.mapping import get_peft_model
from peft.utils.other import prepare_model_for_kbit_training
from peft.utils.peft_types import TaskType
from peft.peft_model import PeftModel
import torch
from loguru import logger
from .logging_config import log_error_with_traceback
from langchain_core.messages import SystemMessage, HumanMessage

from prompts.lora import (
    TrainingConfig,
    get_training_config_prompt
)

if TYPE_CHECKING:
    BasePreTrainedModel = PreTrainedModel
else:
    BasePreTrainedModel = object

class TrainingExample(BaseModel):
    """Schema for training examples"""
    input_text: str = Field(description="Input text for training")
    output_text: str = Field(description="Expected output text")
    metadata: Dict = Field(description="Additional metadata")
    quality_score: float = Field(description="Quality score of the example")

class TrainingMetrics(BaseModel):
    """Schema for training metrics"""
    loss: float = Field(description="Training loss")
    eval_loss: Optional[float] = Field(description="Evaluation loss")
    train_samples: int = Field(description="Number of training samples")
    eval_samples: Optional[int] = Field(description="Number of evaluation samples")
    training_time: float = Field(description="Training time in seconds")

class ModelAdaptationMetrics(BaseModel):
    """Schema for model adaptation metrics"""
    base_performance: Dict[str, float] = Field(description="Base model performance")
    adapted_performance: Dict[str, float] = Field(description="Adapted model performance")
    improvement: Dict[str, float] = Field(description="Performance improvement")

class LoRATrainingConfig(BaseModel):
    """Configuration for LoRA training"""
    model_name: str = Field(description="Base model name")
    lora_alpha: int = Field(description="LoRA alpha parameter")
    lora_dropout: float = Field(description="LoRA dropout rate")
    r: int = Field(description="LoRA rank")
    bias: Literal["none", "all", "lora_only"] = Field(description="LoRA bias type")
    task_type: str = Field(description="Task type for LoRA")
    target_modules: List[str] = Field(description="Target modules for LoRA")
    inference_mode: bool = Field(description="Whether to use inference mode")

class LoRATrainer:
    """LoRA training system."""

    def __init__(self, llm):
        """Initialize with language model."""
        self.llm = llm

    async def generate_config(self, task_description: str) -> TrainingConfig:
        """Generate LoRA training configuration."""
        try:
            prompt = get_training_config_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=TrainingConfig)
            result = await chain.ainvoke({
                "task": task_description
            })
            return result
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating training config")
            raise

    def _setup_model(self, config: LoRATrainingConfig) -> None:
        """Setup model and tokenizer"""
        try:
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)
            if self.tokenizer is None:
                raise ValueError("Failed to load tokenizer")
            
            # Load model
            base_model = AutoModelForCausalLM.from_pretrained(
                config.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            if base_model is None:
                raise ValueError("Failed to load model")
                
            # Prepare model for k-bit training
            prepared_model = prepare_model_for_kbit_training(base_model)
            
            # Create LoRA config
            lora_config = LoraConfig(
                r=config.r,
                lora_alpha=config.lora_alpha,
                target_modules=config.target_modules,
                lora_dropout=config.lora_dropout,
                bias=config.bias,
                task_type=TaskType.CAUSAL_LM,
            )
            
            # Get PEFT model
            peft_model = get_peft_model(prepared_model, lora_config)
            if peft_model is None:
                raise ValueError("Failed to create PEFT model")
            self.model = cast(PeftModel, peft_model)
            
            logger.info(f"Model setup complete with config: {config}")
            
        except Exception as e:
            logger.error(f"Error setting up model: {e}")
            raise
            
    def prepare_training_data(self, examples: List[TrainingExample]) -> Dataset:
        """Prepare training data from examples"""
        try:
            if self.tokenizer is None:
                raise ValueError("Tokenizer not initialized")
                
            # Convert examples to dataset format
            dataset_dict = {
                "input_text": [],
                "output_text": [],
                "quality_score": []
            }
            
            # Ensure we have at least one example
            if not examples:
                raise ValueError("No training examples provided")
            
            for example in examples:
                dataset_dict["input_text"].append(example.input_text)
                dataset_dict["output_text"].append(example.output_text)
                dataset_dict["quality_score"].append(example.quality_score)
                
            # Create dataset
            dataset = Dataset.from_dict(dataset_dict)
            
            # Tokenize dataset
            def tokenize_function(examples):
                if self.tokenizer is None:
                    raise ValueError("Tokenizer not initialized")
                    
                # Combine input and output with appropriate format
                full_texts = [
                    f"Input: {input_text}\nOutput: {output_text}"
                    for input_text, output_text in zip(examples["input_text"], examples["output_text"])
                ]
                tokenized = self.tokenizer(
                    full_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors=None  # Return a dictionary instead of tensors
                )
                return {
                    "input_ids": tokenized["input_ids"],
                    "attention_mask": tokenized["attention_mask"]
                }
                
            tokenized_dataset = dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=dataset.column_names
            )
            
            # Ensure we have at least one sample
            if len(tokenized_dataset) == 0:
                raise ValueError("No valid samples in dataset after tokenization")
            
            return tokenized_dataset
            
        except Exception as e:
            logger.error(f"Error preparing training data: {e}")
            raise
            
    def train(self, 
             train_dataset: Dataset,
             eval_dataset: Optional[Dataset] = None,
             num_train_epochs: int = 3,
             per_device_train_batch_size: int = 8,
             learning_rate: float = 2e-5,
             weight_decay: float = 0.01,
             max_grad_norm: float = 1.0) -> TrainingMetrics:
        """Train the model using LoRA"""
        try:
            if self.model is None:
                raise ValueError("Model not initialized")
            if self.tokenizer is None:
                raise ValueError("Tokenizer not initialized")
                
            # Setup training arguments
            training_args = TrainingArguments(
                output_dir=f"lora_adapters/{self.model.model_name}",
                num_train_epochs=num_train_epochs,
                per_device_train_batch_size=per_device_train_batch_size,
                learning_rate=learning_rate,
                weight_decay=weight_decay,
                max_grad_norm=max_grad_norm,
                logging_dir="logs",
                logging_steps=10,
                evaluation_strategy="epoch" if eval_dataset else "no",
                save_strategy="epoch",
                load_best_model_at_end=True if eval_dataset else False,
            )
            
            # Create data collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            # Initialize trainer
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
            )
            
            # Train model
            train_result = trainer.train()
            
            # Return metrics
            return TrainingMetrics(
                loss=train_result.training_loss,
                eval_loss=train_result.metrics.get("eval_loss"),
                train_samples=len(train_dataset),
                eval_samples=len(eval_dataset) if eval_dataset else None,
                training_time=train_result.metrics.get("training_time", 0.0)
            )
            
        except Exception as e:
            logger.error(f"Error during training: {e}")
            raise
            
    def evaluate_adaptation(self, eval_dataset: Dataset) -> ModelAdaptationMetrics:
        """Evaluate the model adaptation"""
        try:
            if self.model is None:
                raise ValueError("Model not initialized")
            if self.tokenizer is None:
                raise ValueError("Tokenizer not initialized")
                
            # Create a new trainer for evaluation
            eval_trainer = Trainer(
                model=self.model,
                args=TrainingArguments(
                    output_dir="eval_tmp",
                    per_device_eval_batch_size=8,
                    remove_unused_columns=False
                ),
                eval_dataset=eval_dataset,
                data_collator=DataCollatorForLanguageModeling(
                    tokenizer=cast(PreTrainedTokenizerBase, self.tokenizer),
                    mlm=False
                )
            )
            
            # Evaluate base model
            base_metrics = self._evaluate_model(self.model, eval_dataset)
            
            # Evaluate adapted model (using same model since we don't have a separate adapted model yet)
            adapted_metrics = base_metrics
            
            # Calculate improvements (will be 0 since we're using same model)
            improvements = {
                metric: 0.0
                for metric in base_metrics.keys()
            }
            
            return ModelAdaptationMetrics(
                base_performance=base_metrics,
                adapted_performance=adapted_metrics,
                improvement=improvements
            )
            
        except Exception as e:
            logger.error(f"Error evaluating adaptation: {e}")
            raise
            
    def _evaluate_model(self, model: Any, dataset: Dataset) -> Dict[str, float]:
        """Evaluate a model on a dataset"""
        try:
            if self.tokenizer is None:
                raise ValueError("Tokenizer not initialized")
                
            trainer = Trainer(
                model=model,
                eval_dataset=dataset,
                data_collator=DataCollatorForLanguageModeling(
                    tokenizer=self.tokenizer,
                    mlm=False
                )
            )
            
            metrics = trainer.evaluate()
            if metrics is None:
                raise ValueError("Model evaluation failed")
                
            return {
                "perplexity": torch.exp(torch.tensor(metrics["eval_loss"])).item(),
                "loss": metrics["eval_loss"]
            }
            
        except Exception as e:
            log_error_with_traceback(e, "Error in model evaluation")
            raise
            
    def save_adapter(self, path: str, adapter_name: str = "default"):
        """Save the LoRA adapter weights"""
        try:
            if self.model is None:
                raise ValueError("Model not initialized")
                
            self.model.save_pretrained(path)
            logger.info(f"Saved LoRA adapter to {path}")
        except Exception as e:
            logger.error(f"Error saving adapter: {e}")
            raise
            
    def load_adapter(self, path: str, adapter_name: str = "default"):
        """Load a saved LoRA adapter"""
        try:
            if self.model is None:
                raise ValueError("Model not initialized")
                
            self.model.load_adapter(path, adapter_name=adapter_name)
            logger.info(f"Loaded LoRA adapter from {path}")
        except Exception as e:
            logger.error(f"Error loading adapter: {e}")
            raise 
```

.\scripts\mdconvert.py
```python
# This is copied from Magentic-one's great repo: https://github.com/microsoft/autogen/blob/v0.4.4/python/packages/autogen-magentic-one/src/autogen_magentic_one/markdown_browser/mdconvert.py
# Thanks to Microsoft researchers for open-sourcing this!
# type: ignore
import base64
import copy
import html
import json
import mimetypes
import os
import re
import shutil
import subprocess
import sys
import tempfile
import traceback
from typing import Any, Dict, List, Optional, Union
from urllib.parse import parse_qs, quote, unquote, urlparse, urlunparse

import mammoth
import markdownify
import pandas as pd
import pdfminer
import pdfminer.high_level
import pptx

# File-format detection
import puremagic
# import pydub  # Commented out as we don't need audio processing for tests
import requests
# import speech_recognition as sr  # Commented out as we don't need speech recognition for tests
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import SRTFormatter
import urllib3


class _CustomMarkdownify(markdownify.MarkdownConverter):
    """
    A custom version of markdownify's MarkdownConverter. Changes include:

    - Altering the default heading style to use '#', '##', etc.
    - Removing javascript hyperlinks.
    - Truncating images with large data:uri sources.
    - Ensuring URIs are properly escaped, and do not conflict with Markdown syntax
    """

    def __init__(self, **options: Any):
        options["heading_style"] = options.get("heading_style", markdownify.ATX)
        # Explicitly cast options to the expected type if necessary
        super().__init__(**options)

    def convert_hn(self, n: int, el: Any, text: str, convert_as_inline: bool) -> str:
        """Same as usual, but be sure to start with a new line"""
        if not convert_as_inline:
            if not re.search(r"^\n", text):
                return "\n" + super().convert_hn(n, el, text, convert_as_inline)  # type: ignore

        return super().convert_hn(n, el, text, convert_as_inline)  # type: ignore

    def convert_a(self, el: Any, text: str, convert_as_inline: bool):
        """Same as usual converter, but removes Javascript links and escapes URIs."""
        prefix, suffix, text = markdownify.chomp(text)  # type: ignore
        if not text:
            return ""
        href = el.get("href")
        title = el.get("title")

        # Escape URIs and skip non-http or file schemes
        if href:
            try:
                parsed_url = urlparse(href)  # type: ignore
                if parsed_url.scheme and parsed_url.scheme.lower() not in ["http", "https", "file"]:  # type: ignore
                    return "%s%s%s" % (prefix, text, suffix)
                href = urlunparse(parsed_url._replace(path=quote(unquote(parsed_url.path))))  # type: ignore
            except ValueError:  # It's not clear if this ever gets thrown
                return "%s%s%s" % (prefix, text, suffix)

        # For the replacement see #29: text nodes underscores are escaped
        if (
            self.options["autolinks"]
            and text.replace(r"\_", "_") == href
            and not title
            and not self.options["default_title"]
        ):
            # Shortcut syntax
            return "<%s>" % href
        if self.options["default_title"] and not title:
            title = href
        title_part = ' "%s"' % title.replace('"', r"\"") if title else ""
        return "%s[%s](%s%s)%s" % (prefix, text, href, title_part, suffix) if href else text

    def convert_img(self, el: Any, text: str, convert_as_inline: bool) -> str:
        """Same as usual converter, but removes data URIs"""

        alt = el.attrs.get("alt", None) or ""
        src = el.attrs.get("src", None) or ""
        title = el.attrs.get("title", None) or ""
        title_part = ' "%s"' % title.replace('"', r"\"") if title else ""
        if convert_as_inline and el.parent.name not in self.options["keep_inline_images_in"]:
            return alt

        # Remove dataURIs
        if src.startswith("data:"):
            src = src.split(",")[0] + "..."

        return "![%s](%s%s)" % (alt, src, title_part)

    def convert_soup(self, soup: Any) -> str:
        return super().convert_soup(soup)  # type: ignore


class DocumentConverterResult:
    """The result of converting a document to text."""

    def __init__(self, title: Union[str, None] = None, text_content: str = ""):
        self.title: Union[str, None] = title
        self.text_content: str = text_content


class DocumentConverter:
    """Abstract superclass of all DocumentConverters."""

    def convert(self, local_path: str, **kwargs: Any) -> Union[None, DocumentConverterResult]:
        raise NotImplementedError()


class PlainTextConverter(DocumentConverter):
    """Anything with content type text/plain"""

    def convert(self, local_path: str, **kwargs: Any) -> Union[None, DocumentConverterResult]:
        # Guess the content type from any file extension that might be around
        content_type, _ = mimetypes.guess_type("__placeholder" + kwargs.get("file_extension", ""))

        # Only accept text files
        if content_type is None:
            return None
        # elif "text/" not in content_type.lower():
        #     return None

        text_content = ""
        with open(local_path, "rt", encoding="utf-8") as fh:
            text_content = fh.read()
        return DocumentConverterResult(
            title=None,
            text_content=text_content,
        )


class HtmlConverter(DocumentConverter):
    """Anything with content type text/html"""

    def convert(self, local_path: str, **kwargs: Any) -> Union[None, DocumentConverterResult]:
        # Bail if not html
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None

        result = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            result = self._convert(fh.read())

        return result

    def _convert(self, html_content: str) -> Union[None, DocumentConverterResult]:
        """Helper function that converts and HTML string."""

        # Parse the string
        soup = BeautifulSoup(html_content, "html.parser")

        # Remove javascript and style blocks
        for script in soup(["script", "style"]):
            script.extract()

        # Print only the main content
        body_elm = soup.find("body")
        webpage_text = ""
        if body_elm:
            webpage_text = _CustomMarkdownify().convert_soup(body_elm)
        else:
            webpage_text = _CustomMarkdownify().convert_soup(soup)

        assert isinstance(webpage_text, str)

        return DocumentConverterResult(
            title=None if soup.title is None else soup.title.string, text_content=webpage_text
        )


class WikipediaConverter(DocumentConverter):
    """Handle Wikipedia pages separately, focusing only on the main document content."""

    def convert(self, local_path: str, **kwargs: Any) -> Union[None, DocumentConverterResult]:
        # Bail if not Wikipedia
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None
        url = kwargs.get("url", "")
        if not re.search(r"^https?:\/\/[a-zA-Z]{2,3}\.wikipedia.org\/", url):
            return None

        # Parse the file
        soup = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            soup = BeautifulSoup(fh.read(), "html.parser")

        # Remove javascript and style blocks
        for script in soup(["script", "style"]):
            script.extract()

        # Print only the main content
        body_elm = soup.find("div", {"id": "mw-content-text"})
        title_elm = soup.find("span", {"class": "mw-page-title-main"})

        webpage_text = ""
        main_title = None if soup.title is None else soup.title.string

        if body_elm:
            # What's the title
            if title_elm and len(title_elm) > 0:
                main_title = title_elm.string  # type: ignore
                assert isinstance(main_title, str)

            # Convert the page
            webpage_text = f"# {main_title}\n\n" + _CustomMarkdownify().convert_soup(body_elm)
        else:
            webpage_text = _CustomMarkdownify().convert_soup(soup)

        return DocumentConverterResult(
            title=main_title,
            text_content=webpage_text,
        )


class YouTubeConverter(DocumentConverter):
    """Handle YouTube specially, focusing on the video title, description, and transcript."""

    def convert(self, local_path: str, **kwargs: Any) -> Union[None, DocumentConverterResult]:
        # Bail if not YouTube
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None
        url = kwargs.get("url", "")
        if not url.startswith("https://www.youtube.com/watch?"):
            return None

        # Parse the file
        soup = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            soup = BeautifulSoup(fh.read(), "html.parser")

        # Read the meta tags
        assert soup.title is not None and soup.title.string is not None
        metadata: Dict[str, str] = {"title": soup.title.string}
        for meta in soup(["meta"]):
            for a in meta.attrs:
                if a in ["itemprop", "property", "name"]:
                    metadata[meta[a]] = meta.get("content", "")
                    break

        # We can also try to read the full description. This is more prone to breaking, since it reaches into the page implementation
        try:
            for script in soup(["script"]):
                content = script.text
                if "ytInitialData" in content:
                    lines = re.split(r"\r?\n", content)
                    obj_start = lines[0].find("{")
                    obj_end = lines[0].rfind("}")
                    if obj_start >= 0 and obj_end >= 0:
                        data = json.loads(lines[0][obj_start : obj_end + 1])
                        attrdesc = self._findKey(data, "attributedDescriptionBodyText")  # type: ignore
                        if attrdesc:
                            metadata["description"] = str(attrdesc["content"])
                    break
        except Exception:
            pass

        # Start preparing the page
        webpage_text = "# YouTube\n"

        title = self._get(metadata, ["title", "og:title", "name"])  # type: ignore
        assert isinstance(title, str)

        if title:
            webpage_text += f"\n## {title}\n"

        stats = ""
        views = self._get(metadata, ["interactionCount"])  # type: ignore
        if views:
            stats += f"- **Views:** {views}\n"

        keywords = self._get(metadata, ["keywords"])  # type: ignore
        if keywords:
            stats += f"- **Keywords:** {keywords}\n"

        runtime = self._get(metadata, ["duration"])  # type: ignore
        if runtime:
            stats += f"- **Runtime:** {runtime}\n"

        if len(stats) > 0:
            webpage_text += f"\n### Video Metadata\n{stats}\n"

        description = self._get(metadata, ["description", "og:description"])  # type: ignore
        if description:
            webpage_text += f"\n### Description\n{description}\n"

        transcript_text = ""
        parsed_url = urlparse(url)  # type: ignore
        params = parse_qs(parsed_url.query)  # type: ignore
        if "v" in params:
            assert isinstance(params["v"][0], str)
            video_id = str(params["v"][0])
            try:
                # Must be a single transcript.
                transcript = YouTubeTranscriptApi.get_transcript(video_id)  # type: ignore
                # transcript_text = " ".join([part["text"] for part in transcript])  # type: ignore
                # Alternative formatting:
                transcript_text = SRTFormatter().format_transcript(transcript)
            except Exception:
                pass
        if transcript_text:
            webpage_text += f"\n### Transcript\n{transcript_text}\n"

        title = title if title else soup.title.string
        assert isinstance(title, str)

        return DocumentConverterResult(
            title=title,
            text_content=webpage_text,
        )

    def _get(self, metadata: Dict[str, str], keys: List[str], default: Union[str, None] = None) -> Union[str, None]:
        for k in keys:
            if k in metadata:
                return metadata[k]
        return default

    def _findKey(self, json: Any, key: str) -> Union[str, None]:  # TODO: Fix json type
        if isinstance(json, list):
            for elm in json:
                ret = self._findKey(elm, key)
                if ret is not None:
                    return ret
        elif isinstance(json, dict):
            for k in json:
                if k == key:
                    return json[k]
                else:
                    ret = self._findKey(json[k], key)
                    if ret is not None:
                        return ret
        return None


class PdfConverter(DocumentConverter):
    """
    Converts PDFs to Markdown. Most style information is ignored, so the results are essentially plain-text.
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a PDF
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".pdf":
            return None

        return DocumentConverterResult(
            title=None,
            text_content=pdfminer.high_level.extract_text(local_path),
        )


class DocxConverter(HtmlConverter):
    """
    Converts DOCX files to Markdown. Style information (e.g.m headings) and tables are preserved where possible.
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a DOCX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".docx":
            return None

        result = None
        with open(local_path, "rb") as docx_file:
            result = mammoth.convert_to_html(docx_file)
            html_content = result.value
            result = self._convert(html_content)

        return result


class XlsxConverter(HtmlConverter):
    """
    Converts XLSX files to Markdown, with each sheet presented as a separate Markdown table.
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".xlsx", ".xls"]:
            return None

        sheets = pd.read_excel(local_path, sheet_name=None)
        md_content = ""
        for s in sheets:
            md_content += f"## {s}\n"
            html_content = sheets[s].to_html(index=False)
            md_content += self._convert(html_content).text_content.strip() + "\n\n"

        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )


class PptxConverter(HtmlConverter):
    """
    Converts PPTX files to Markdown. Supports heading, tables and images with alt text.
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a PPTX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".pptx":
            return None

        md_content = ""

        presentation = pptx.Presentation(local_path)
        slide_num = 0
        for slide in presentation.slides:
            slide_num += 1

            md_content += f"\n\n<!-- Slide number: {slide_num} -->\n"

            title = slide.shapes.title
            for shape in slide.shapes:
                # Pictures
                if self._is_picture(shape):
                    # https://github.com/scanny/python-pptx/pull/512#issuecomment-1713100069
                    alt_text = ""
                    try:
                        alt_text = shape._element._nvXxPr.cNvPr.attrib.get("descr", "")
                    except Exception:
                        pass

                    # A placeholder name
                    filename = re.sub(r"\W", "", shape.name) + ".jpg"
                    md_content += "\n![" + (alt_text if alt_text else shape.name) + "](" + filename + ")\n"

                # Tables
                if self._is_table(shape):
                    html_table = "<html><body><table>"
                    first_row = True
                    for row in shape.table.rows:
                        html_table += "<tr>"
                        for cell in row.cells:
                            if first_row:
                                html_table += "<th>" + html.escape(cell.text) + "</th>"
                            else:
                                html_table += "<td>" + html.escape(cell.text) + "</td>"
                        html_table += "</tr>"
                        first_row = False
                    html_table += "</table></body></html>"
                    md_content += "\n" + self._convert(html_table).text_content.strip() + "\n"

                # Text areas
                elif shape.has_text_frame:
                    if shape == title:
                        md_content += "# " + shape.text.lstrip() + "\n"
                    else:
                        md_content += shape.text + "\n"

            md_content = md_content.strip()

            if slide.has_notes_slide:
                md_content += "\n\n### Notes:\n"
                notes_frame = slide.notes_slide.notes_text_frame
                if notes_frame is not None:
                    md_content += notes_frame.text
                md_content = md_content.strip()

        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )

    def _is_picture(self, shape):
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.PICTURE:
            return True
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.PLACEHOLDER:
            if hasattr(shape, "image"):
                return True
        return False

    def _is_table(self, shape):
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.TABLE:
            return True
        return False


class MediaConverter(DocumentConverter):
    """
    Abstract class for multi-modal media (e.g., images and audio)
    """

    def _get_metadata(self, local_path):
        exiftool = shutil.which("exiftool")
        if not exiftool:
            return None
        else:
            try:
                result = subprocess.run([exiftool, "-json", local_path], capture_output=True, text=True).stdout
                return json.loads(result)[0]
            except Exception:
                return None


class WavConverter(MediaConverter):
    """
    Converts WAV files to markdown via extraction of metadata (if `exiftool` is installed), and speech transcription (if `speech_recognition` is installed).
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".wav":
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "Title",
                "Artist",
                "Author",
                "Band",
                "Album",
                "Genre",
                "Track",
                "DateTimeOriginal",
                "CreateDate",
                "Duration",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Transcribe
        try:
            transcript = self._transcribe_audio(local_path)
            md_content += "\n\n### Audio Transcript:\n" + ("[No speech detected]" if transcript == "" else transcript)
        except Exception:
            md_content += "\n\n### Audio Transcript:\nError. Could not transcribe this audio."

        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )

    def _transcribe_audio(self, local_path) -> str:
        recognizer = sr.Recognizer()
        with sr.AudioFile(local_path) as source:
            audio = recognizer.record(source)
            return recognizer.recognize_google(audio).strip()


class Mp3Converter(WavConverter):
    """
    Converts MP3 files to markdown via extraction of metadata (if `exiftool` is installed), and speech transcription (if `speech_recognition` AND `pydub` are installed).
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a MP3
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".mp3":
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "Title",
                "Artist",
                "Author",
                "Band",
                "Album",
                "Genre",
                "Track",
                "DateTimeOriginal",
                "CreateDate",
                "Duration",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Transcribe
        handle, temp_path = tempfile.mkstemp(suffix=".wav")
        os.close(handle)
        try:
            sound = pydub.AudioSegment.from_mp3(local_path)
            sound.export(temp_path, format="wav")

            _args = dict()
            _args.update(kwargs)
            _args["file_extension"] = ".wav"

            try:
                transcript = super()._transcribe_audio(temp_path).strip()
                md_content += "\n\n### Audio Transcript:\n" + (
                    "[No speech detected]" if transcript == "" else transcript
                )
            except Exception:
                md_content += "\n\n### Audio Transcript:\nError. Could not transcribe this audio."

        finally:
            os.unlink(temp_path)

        # Return the result
        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )


class ImageConverter(MediaConverter):
    """
    Converts images to markdown via extraction of metadata (if `exiftool` is installed), OCR (if `easyocr` is installed), and description via a multimodal LLM (if an mlm_client is configured).
    """

    def convert(self, local_path, **kwargs) -> Union[None, DocumentConverterResult]:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".jpg", ".jpeg", ".png"]:
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "ImageSize",
                "Title",
                "Caption",
                "Description",
                "Keywords",
                "Artist",
                "Author",
                "DateTimeOriginal",
                "CreateDate",
                "GPSPosition",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Try describing the image with GPTV
        mlm_client = kwargs.get("mlm_client")
        mlm_model = kwargs.get("mlm_model")
        if mlm_client is not None and mlm_model is not None:
            md_content += (
                "\n# Description:\n"
                + self._get_mlm_description(
                    local_path, extension, mlm_client, mlm_model, prompt=kwargs.get("mlm_prompt")
                ).strip()
                + "\n"
            )

        return DocumentConverterResult(
            title=None,
            text_content=md_content,
        )

    def _get_mlm_description(self, local_path, extension, client, model, prompt=None):
        if prompt is None or prompt.strip() == "":
            prompt = "Write a detailed caption for this image."

        sys.stderr.write(f"MLM Prompt:\n{prompt}\n")

        data_uri = ""
        with open(local_path, "rb") as image_file:
            content_type, encoding = mimetypes.guess_type("_dummy" + extension)
            if content_type is None:
                content_type = "image/jpeg"
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
            data_uri = f"data:{content_type};base64,{image_base64}"

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": data_uri,
                        },
                    },
                ],
            }
        ]

        response = client.chat.completions.create(model=model, messages=messages)
        return response.choices[0].message.content


class FileConversionException(BaseException):
    pass


class UnsupportedFormatException(BaseException):
    pass


class MarkdownConverter:
    """(In preview) An extremely simple text-based document reader, suitable for LLM use.
    This reader will convert common file-types or webpages to Markdown."""

    def __init__(
        self,
        requests_session: Optional[requests.Session] = None,
        mlm_client: Optional[Any] = None,
        mlm_model: Optional[Any] = None,
    ):
        if requests_session is None:
            self._requests_session = requests.Session()
        else:
            self._requests_session = requests_session

        self._mlm_client = mlm_client
        self._mlm_model = mlm_model

        self._page_converters: List[DocumentConverter] = []

        # Register converters for successful browsing operations
        # Later registrations are tried first / take higher priority than earlier registrations
        # To this end, the most specific converters should appear below the most generic converters
        self.register_page_converter(PlainTextConverter())
        self.register_page_converter(HtmlConverter())
        self.register_page_converter(WikipediaConverter())
        self.register_page_converter(YouTubeConverter())
        self.register_page_converter(DocxConverter())
        self.register_page_converter(XlsxConverter())
        self.register_page_converter(PptxConverter())
        self.register_page_converter(WavConverter())
        self.register_page_converter(Mp3Converter())
        self.register_page_converter(ImageConverter())
        self.register_page_converter(PdfConverter())

    def convert(
        self, source: Union[str, requests.Response], **kwargs: Any
    ) -> DocumentConverterResult:  # TODO: deal with kwargs
        """
        Args:
            - source: can be a string representing a path or url, or a requests.response object
            - extension: specifies the file extension to use when interpreting the file. If None, infer from source (path, uri, content-type, etc.)
        """

        # Local path or url
        if isinstance(source, str):
            if source.startswith("http://") or source.startswith("https://") or source.startswith("file://"):
                return self.convert_url(source, **kwargs)
            else:
                return self.convert_local(source, **kwargs)
        # Request response
        elif isinstance(source, requests.Response):
            return self.convert_response(source, **kwargs)

    def convert_local(self, path: str, **kwargs: Any) -> DocumentConverterResult:  # TODO: deal with kwargs
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Get extension alternatives from the path and puremagic
        base, ext = os.path.splitext(path)
        self._append_ext(extensions, ext)
        self._append_ext(extensions, self._guess_ext_magic(path))

        # Convert
        return self._convert(path, extensions, **kwargs)

    # TODO what should stream's type be?
    def convert_stream(self, stream: Any, **kwargs: Any) -> DocumentConverterResult:  # TODO: deal with kwargs
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Save the file locally to a temporary file. It will be deleted before this method exits
        handle, temp_path = tempfile.mkstemp()
        fh = os.fdopen(handle, "wb")
        result = None
        try:
            # Write to the temporary file
            content = stream.read()
            if isinstance(content, str):
                fh.write(content.encode("utf-8"))
            else:
                fh.write(content)
            fh.close()

            # Use puremagic to check for more extension options
            self._append_ext(extensions, self._guess_ext_magic(temp_path))

            # Convert
            result = self._convert(temp_path, extensions, **kwargs)
        # Clean up
        finally:
            try:
                fh.close()
            except Exception:
                pass
            os.unlink(temp_path)

        return result

    def convert_url(self, url: str, **kwargs: Any) -> DocumentConverterResult:  # TODO: fix kwargs type
        # Configure robust request handling
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Cache-Control": "max-age=0",
            "Sec-Ch-Ua": '"Chromium";v="122", "Not(A:Brand";v="24", "Microsoft Edge";v="122"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"Windows"',
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1"
        }
        
        # Configure session
        if not self._requests_session:
            self._requests_session = requests.Session()
        
        # Configure retries
        retries = urllib3.util.Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"]
        )
        adapter = requests.adapters.HTTPAdapter(max_retries=retries)
        self._requests_session.mount("http://", adapter)
        self._requests_session.mount("https://", adapter)
        
        try:
            # Make request with configured session
            response = self._requests_session.get(
                url,
                stream=True,
                headers=headers,
                timeout=30,
                verify=False
            )
            response.raise_for_status()
            return self.convert_response(response, **kwargs)
            
        except requests.exceptions.RequestException as e:
            print(f"Error requesting {url}: {str(e)}")
            return DocumentConverterResult(text_content=f"Error accessing content: {str(e)}")
            
        except Exception as e:
            print(f"Error converting {url}: {str(e)}")
            return DocumentConverterResult(text_content=f"Error converting content: {str(e)}")

    def convert_response(
        self, response: requests.Response, **kwargs: Any
    ) -> DocumentConverterResult:  # TODO fix kwargs type
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Guess from the mimetype
        content_type = response.headers.get("content-type", "").split(";")[0]
        self._append_ext(extensions, mimetypes.guess_extension(content_type))

        # Read the content disposition if there is one
        content_disposition = response.headers.get("content-disposition", "")
        m = re.search(r"filename=([^;]+)", content_disposition)
        if m:
            base, ext = os.path.splitext(m.group(1).strip("\"'"))
            self._append_ext(extensions, ext)

        # Read from the extension from the path
        base, ext = os.path.splitext(urlparse(response.url).path)
        self._append_ext(extensions, ext)

        # Save the file locally to a temporary file. It will be deleted before this method exits
        handle, temp_path = tempfile.mkstemp()
        fh = os.fdopen(handle, "wb")
        result = None
        try:
            # Download the file
            for chunk in response.iter_content(chunk_size=512):
                fh.write(chunk)
            fh.close()

            # Use puremagic to check for more extension options
            self._append_ext(extensions, self._guess_ext_magic(temp_path))

            # Convert
            result = self._convert(temp_path, extensions, url=response.url)
        except Exception as e:
            print(f"Error in converting: {e}")

        # Clean up
        finally:
            try:
                fh.close()
            except Exception:
                pass
            os.unlink(temp_path)

        return result

    def _convert(self, local_path: str, extensions: List[Union[str, None]], **kwargs) -> DocumentConverterResult:
        error_trace = ""
        for ext in extensions + [None]:  # Try last with no extension
            for converter in self._page_converters:
                _kwargs = copy.deepcopy(kwargs)

                # Overwrite file_extension appropriately
                if ext is None:
                    if "file_extension" in _kwargs:
                        del _kwargs["file_extension"]
                else:
                    _kwargs.update({"file_extension": ext})

                # Copy any additional global options
                if "mlm_client" not in _kwargs and self._mlm_client is not None:
                    _kwargs["mlm_client"] = self._mlm_client

                if "mlm_model" not in _kwargs and self._mlm_model is not None:
                    _kwargs["mlm_model"] = self._mlm_model

                # If we hit an error log it and keep trying
                try:
                    res = converter.convert(local_path, **_kwargs)
                except Exception:
                    error_trace = ("\n\n" + traceback.format_exc()).strip()

                if res is not None:
                    # Normalize the content
                    res.text_content = "\n".join([line.rstrip() for line in re.split(r"\r?\n", res.text_content)])
                    res.text_content = re.sub(r"\n{3,}", "\n\n", res.text_content)

                    # Todo
                    return res

        # If we got this far without success, report any exceptions
        if len(error_trace) > 0:
            raise FileConversionException(
                f"Could not convert '{local_path}' to Markdown. File type was recognized as {extensions}. While converting the file, the following error was encountered:\n\n{error_trace}"
            )

        # Nothing can handle it!
        raise UnsupportedFormatException(
            f"Could not convert '{local_path}' to Markdown. The formats {extensions} are not supported."
        )

    def _append_ext(self, extensions, ext):
        """Append a unique non-None, non-empty extension to a list of extensions."""
        if ext is None:
            return
        ext = ext.strip()
        if ext == "":
            return
        # if ext not in extensions:
        if True:
            extensions.append(ext)

    def _guess_ext_magic(self, path):
        """Use puremagic (a Python implementation of libmagic) to guess a file's extension based on the first few bytes."""
        # Use puremagic to guess
        try:
            guesses = puremagic.magic_file(path)
            if len(guesses) > 0:
                ext = guesses[0].extension.strip()
                if len(ext) > 0:
                    return ext
        except FileNotFoundError:
            pass
        except IsADirectoryError:
            pass
        except PermissionError:
            pass
        return None

    def register_page_converter(self, converter: DocumentConverter) -> None:
        """Register a page text converter."""
        self._page_converters.insert(0, converter)

```

.\scripts\models.py
```python
from typing import List, Dict, Literal, Optional, Any
from pydantic import BaseModel, Field, validator
from datetime import datetime

class OntologyRelation(BaseModel):
    """Model for ontology relationships"""
    name: str = Field(description="Name of the relationship type")
    description: str = Field(description="Description of what this relationship means")
    symmetric: bool = Field(default=False, description="Whether the relationship is symmetric (a->b means b->a)")
    transitive: bool = Field(default=False, description="Whether the relationship is transitive (a->b->c means a->c)")
    inverse_of: Optional[str] = Field(default=None, description="Name of the inverse relationship if any")

class OntologyClass(BaseModel):
    """Model for ontology classes"""
    name: str = Field(description="Name of the class")
    description: str = Field(description="Description of what this class represents")
    parent_classes: List[str] = Field(default_factory=list, description="Parent classes in the hierarchy")
    properties: Dict[str, str] = Field(default_factory=dict, description="Properties and their types")
    allowed_relations: List[str] = Field(default_factory=list, description="Allowed relationship types for this class")

class DomainConfig(BaseModel):
    """Configuration for a specific knowledge domain"""
    name: str = Field(description="Name of the domain")
    description: str = Field(description="Description of the domain")
    classes: List[OntologyClass] = Field(description="Classes in this domain's ontology")
    relations: List[OntologyRelation] = Field(description="Relationship types in this domain")
    validation_rules: Dict[str, Dict] = Field(description="Validation rules for the domain")
    confidence_thresholds: Dict[str, float] = Field(description="Confidence thresholds for different operations")
    search_strategies: List[str] = Field(description="Search strategies for this domain")

class KnowledgeAcquisitionConfig(BaseModel):
    """Configuration for knowledge acquisition"""
    domains: List[DomainConfig] = Field(description="Configured knowledge domains")
    default_domain: str = Field(description="Default domain to use")
    source_types: List[str] = Field(description="Allowed source types")
    validation_rules: Dict = Field(description="Global validation rules")
    confidence_thresholds: Dict[str, float] = Field(description="Global confidence thresholds")
    chunk_size: int = Field(default=2000, description="Size of text chunks for processing")
    chunk_overlap: int = Field(default=400, description="Overlap between text chunks")
    max_tokens: int = Field(default=32000, description="Maximum tokens for model context")
    enable_web_search: bool = Field(default=False, description="Whether to enable web search enrichment")
    collection_name: str = Field(default="knowledge_base", description="Name of the vector store collection")
    persist_directory: str = Field(default="./data/vector_store", description="Directory to persist vector store")
    neo4j_uri: str = Field(default="bolt://localhost:7687", description="Neo4j database URI")
    neo4j_username: str = Field(default="neo4j", description="Neo4j username")
    neo4j_password: str = Field(default="password", description="Neo4j password")

    @validator("domains")
    def validate_domains(cls, v):
        domain_names = [d.name for d in v]
        if len(domain_names) != len(set(domain_names)):
            raise ValueError("Domain names must be unique")
        return v

    @validator("default_domain")
    def validate_default_domain(cls, v, values):
        if "domains" in values and v not in [d.name for d in values["domains"]]:
            raise ValueError("Default domain must be one of the configured domains")
        return v

class Relationship(BaseModel):
    """Schema for knowledge relationships"""
    source: str = Field(description="Source entity")
    relation: Literal[
        # Methodology relationships
        "uses", "applies", "implements",
        # Performance relationships
        "improves", "outperforms", "achieves",
        # Component relationships
        "contains", "consists_of", "part_of",
        # Comparison relationships
        "better_than", "similar_to", "different_from",
        # Causal relationships
        "leads_to", "causes", "affects",
        # Temporal relationships
        "precedes", "follows", "concurrent_with",
        # Legacy relationships
        "is_a", "has_part", "related_to"
    ] = Field(description="Type of relationship")
    target: str = Field(description="Target entity")
    domain: str = Field(default="knowledge", description="Domain this relationship belongs to")
    confidence: float = Field(default=1.0, description="Confidence in this relationship", ge=0.0, le=1.0)

    @validator("relation")
    def validate_relation(cls, v):
        valid_relations = [
            # Methodology relationships
            "uses", "applies", "implements",
            # Performance relationships
            "improves", "outperforms", "achieves",
            # Component relationships
            "contains", "consists_of", "part_of",
            # Comparison relationships
            "better_than", "similar_to", "different_from",
            # Causal relationships
            "leads_to", "causes", "affects",
            # Temporal relationships
            "precedes", "follows", "concurrent_with",
            # Legacy relationships
            "is_a", "has_part", "related_to"
        ]
        if v not in valid_relations:
            raise ValueError(f"relation must be one of: {valid_relations}")
        return v

class SourceMetadata(BaseModel):
    """Metadata for a knowledge source."""
    source_type: str = Field(description="Type of source (text, pdf, web, etc)")
    confidence_score: float = Field(description="Confidence score between 0.0 and 1.0", ge=0.0, le=1.0)
    domain_relevance: float = Field(description="Domain relevance score between 0.0 and 1.0", ge=0.0, le=1.0)
    timestamp: str = Field(description="ISO format timestamp")
    validation_status: Literal["pending", "processed", "failed"] = Field(default="pending", description="Validation status")
    domain: str = Field(description="Domain this source belongs to")
    
    # Add metrics tracking
    qa_metrics: Dict[str, Any] = Field(
        default_factory=lambda: {
            "questions_generated": 0,
            "questions_answered": 0,
            "average_confidence": 0.0
        },
        description="Metrics from QA processing"
    )
    
    graph_metrics: Dict[str, Any] = Field(
        default_factory=lambda: {
            "entities_added": 0,
            "relationships_added": 0,
            "failed_operations": 0
        },
        description="Metrics from graph operations"
    )
    
    embedding_metrics: Dict[str, Any] = Field(
        default_factory=lambda: {
            "embeddings_generated": 0,
            "total_tokens": 0,
            "average_vector_length": 0.0
        },
        description="Metrics from embedding operations"
    )

    @validator("timestamp")
    def validate_timestamp(cls, v):
        try:
            datetime.fromisoformat(v)
            return v
        except ValueError:
            raise ValueError("timestamp must be in ISO format")

    @validator("validation_status")
    def validate_status(cls, v):
        valid_statuses = ["pending", "processed", "failed"]
        if v not in valid_statuses:
            raise ValueError(f"validation_status must be one of: {valid_statuses}")
        return v

    @validator("source_type")
    def validate_source_type(cls, v):
        valid_types = ["text", "pdf", "web"]
        if v not in valid_types:
            raise ValueError(f"source_type must be one of: {valid_types}")
        return v

class ExtractedKnowledge(BaseModel):
    """Model for extracted knowledge"""
    content: str = Field(description="The content or summary of the source")
    entities: List[str] = Field(description="List of extracted entities")
    relationships: List[Relationship] = Field(default_factory=list, description="List of relationships between entities")
    confidence: float = Field(default=1.0, description="Overall confidence score for the extraction", ge=0.0, le=1.0)
    metadata: Optional[SourceMetadata] = Field(None, description="Additional metadata about the extraction")
    domain: str = Field(default="knowledge", description="Domain this knowledge belongs to")
    qa_pairs: List[Dict[str, Any]] = Field(default_factory=list, description="Question-answer pairs generated from content")

    @validator("content")
    def validate_content(cls, v):
        if not v or not v.strip():
            raise ValueError("content cannot be empty")
        return v.strip()

    @validator("entities")
    def validate_entities(cls, v):
        if not v:
            raise ValueError("entities list cannot be empty")
        return [e.strip() for e in v if e.strip()]

class LLMResponse(BaseModel):
    """Model for LLM responses"""
    content: str = Field(description="Response content")
    model: Optional[str] = Field(None, description="Model used")
    usage: Optional[Dict] = Field(None, description="Token usage info")

class EntityExtractionInput(BaseModel):
    """Input for entity extraction"""
    text: str = Field(description="Text to extract entities from")
    domain: str = Field(default="knowledge", description="Domain for extraction")

class RelationshipExtractionInput(BaseModel):
    """Input for relationship extraction"""
    text: str = Field(description="Text to extract relationships from")
    entities: List[str] = Field(description="Known entities")
    domain: str = Field(default="knowledge", description="Domain for extraction")

class MetadataGenerationInput(BaseModel):
    """Input for metadata generation"""
    text: str = Field(description="Text to generate metadata for")
    domain: str = Field(default="knowledge", description="Domain for metadata")

class DocumentProcessingInput(BaseModel):
    """Input for document processing"""
    source_path: str = Field(description="Path to source document")
    source_type: str = Field(description="Type of source")
    domain: str = Field(default="knowledge", description="Domain for processing")

class ConfidenceFactors(BaseModel):
    """Model for confidence evaluation factors"""
    content_quality: float = Field(default=0.5, description="Quality of the content", ge=0.0, le=1.0)
    entity_confidence: float = Field(default=0.5, description="Confidence in entity extraction", ge=0.0, le=1.0)
    relationship_validity: float = Field(default=0.5, description="Validity of relationships", ge=0.0, le=1.0)
    source_reliability: float = Field(default=0.5, description="Reliability of the source", ge=0.0, le=1.0)
    context_relevance: float = Field(default=0.5, description="Relevance to context", ge=0.0, le=1.0)
    overall: float = Field(default=0.5, description="Overall confidence score", ge=0.0, le=1.0)

    def get(self, key: str, default: float = 0.5) -> float:
        """Get factor value with default"""
        return getattr(self, key, default)

class ConfidenceEvaluation(BaseModel):
    """Model for confidence evaluation"""
    confidence: float = Field(description="Overall confidence score", ge=0.0, le=1.0)
    factors: ConfidenceFactors = Field(description="Detailed confidence factors")
    reasoning: str = Field(description="Reasoning behind confidence evaluation") 
```

.\scripts\qa_system.py
```python
from typing import List, Dict, Optional, Any
from langchain_core.output_parsers import PydanticOutputParser
from langchain_neo4j import Neo4jGraph
from loguru import logger
from langchain_ollama import ChatOllama
import json
import os
from rich.console import Console
from scripts.logging_config import log_error_with_traceback, log_warning_with_context
from scripts.llm_compiler import LLMCompiler, Task, Plan, TaskResult, JoinDecision, CompilerState
from scripts.chat_langchain import ChatLangChain
from pydantic import SecretStr
from langchain_core.language_models.chat_models import BaseChatModel
from prompts.qa import (
    Question,
    QuestionList,
    get_question_generation_prompt,
    Answer,
    get_answer_generation_prompt,
    KnowledgeGap,
    get_knowledge_gap_prompt,
    QAResponse,
    get_qa_prompt
)
from prompts.qa.qa_planning import get_qa_plan_prompt
from prompts.qa.join_decision import get_join_decision_prompt

# Initialize console
console = Console()

class QASystem(LLMCompiler):
    """Question answering system."""

    def __init__(self, graph: Neo4jGraph, llm: Optional[BaseChatModel] = None, model: str = "deepscaler", temperature: float = 0.7):
        """Initialize with graph database and language model."""
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise EnvironmentError("GOOGLE_API_KEY environment variable must be set")
            
        llm = llm if llm is not None else ChatLangChain(
            api_key=SecretStr(api_key),
            model="gemini-1.5-flash",
            temperature=temperature,
            pydantic_schema=QAResponse
        )
        super().__init__(llm)
        self.graph = graph
        self.current_topic = ""
        self.current_context = ""
        self.num_questions = 5

    async def generate_questions(self, topic: str, num_questions: int = 5) -> List[Question]:
        """Generate questions about a topic."""
        try:
            # Get prompt and parser
            prompt = get_question_generation_prompt()
            parser = PydanticOutputParser(pydantic_object=QuestionList)
            format_instructions = parser.get_format_instructions()
            
            chain = prompt | self.llm | parser
            
            # Get context from graph
            context = await self._get_topic_context(topic)
            
            # Generate questions
            result = await chain.ainvoke({
                "topic": topic,
                "context": context,
                "format_instructions": format_instructions,
                "num_questions": num_questions
            })
            
            questions = result.questions if hasattr(result, 'questions') else []
            
            # Validate and filter questions
            validated_questions = []
            for question in questions:
                # Validate question quality
                if await self._validate_question(question, context):
                    validated_questions.append(question)
            
            return validated_questions
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating questions")
            return []

    async def _validate_question(self, question: Question, context: str) -> bool:
        """Validate a generated question."""
        try:
            # Check if question is answerable from context
            answer = await self._generate_answer_task(question.question, context)
            if not answer or not answer.get("answer"):
                log_warning_with_context(f"Question not answerable: {question.question}", "QA")
                return False
            
            # Check confidence with lower threshold
            if answer.get("confidence", 0.0) < 0.5:  # Reduced from 0.6
                log_warning_with_context(f"Low confidence answer: {question.question}", "QA")
                return False
            
            # Check if question is too simple/generic
            if len(question.question.split()) < 3:  # Reduced from 4
                log_warning_with_context(f"Question too short: {question.question}", "QA")
                return False
            
            # Check if question is relevant to topic
            if not any(term.lower() in question.question.lower() for term in question.topic.lower().split()):
                log_warning_with_context(f"Question not relevant to topic: {question.question}", "QA")
                return False
            
            # Check question type distribution with more permissive limits
            if not hasattr(self, '_question_types'):
                self._question_types = {
                    'knowledge_recall': 0,
                    'concept_application': 0,
                    'analysis': 0,
                    'problem_solving': 0,
                    'critical_thinking': 0
                }
            
            # Ensure balanced distribution of question types with more permissive limits
            current_count = self._question_types.get(question.type, 0)
            max_per_type = max(5, self.num_questions // 2)  # More permissive limit
            if current_count >= max_per_type:
                log_warning_with_context(f"Too many questions of type {question.type}", "QA")
                return False
            
            self._question_types[question.type] = current_count + 1
            return True
            
        except Exception as e:
            log_error_with_traceback(e, "Error validating question")
            return False

    def _map_question_type(self, qa_type: str) -> str:
        """Map QA system question type to example type."""
        mapping = {
            'knowledge_recall': 'knowledge_recall',
            'concept_application': 'concept_application',
            'analysis': 'analysis',
            'problem_solving': 'problem_solving',
            'critical_thinking': 'critical_thinking'
        }
        return mapping.get(qa_type, "knowledge_recall")

    async def _get_topic_context(self, topic: str) -> str:
        """Get context for a topic from the graph."""
        try:
            results = self.graph.query(f"""
                MATCH (d:Document)
                WHERE d.content CONTAINS $topic
                RETURN d.content as content
                LIMIT 5
            """, {"topic": topic})
            
            return "\n".join([r["content"] for r in results])
            
        except Exception as e:
            log_error_with_traceback(e, "Error getting topic context")
            return ""

    async def process_qa_chain(self, question: str) -> QAResponse:
        """Process a question through the QA chain."""
        try:
            # Get prompt and parser
            prompt = get_qa_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=QAResponse)
            
            # Get context
            context = await self._retrieve_context_task(question)
            
            # Run QA chain
            result = await chain.ainvoke({
                "question": question,
                "context": context.get("context", ""),
                "sources": context.get("sources", [])
            })
            
            return result
            
        except Exception as e:
            log_error_with_traceback(e, "Error in QA chain")
            return QAResponse(
                answer="Error processing question",
                sources=[],
                confidence=0.0,
                reasoning="QA chain failed"
            )

    async def _generate_plan(self, state: CompilerState) -> Plan:
        """Generate QA plan"""
        try:
            prompt, parser = get_qa_plan_prompt()
            chain = prompt | self.llm | parser
            plan = await chain.ainvoke({"question": state.get('question', '')})
            return plan

        except Exception as e:
            log_error_with_traceback(e, "Error generating QA plan")
            raise

    async def _execute_tasks(self, tasks: List[Task]) -> List[TaskResult]:
        """Execute QA tasks"""
        try:
            results = []
            for task in tasks:
                try:
                    # Check dependencies
                    deps_met = all(
                        any(r.task_id == dep and not r.error for r in results)
                        for dep in task.dependencies
                    )
                    if not deps_met:
                        continue

                    # Execute task
                    result = None
                    if task.tool == "retrieve_context":
                        result = await self._retrieve_context_task(task.args["question"])
                    elif task.tool == "generate_answer":
                        result = await self._generate_answer_task(task.args["question"], task.args["context"])
                    elif task.tool == "validate_answer":
                        result = await self._validate_answer_task(task.args["answer"])
                    elif task.tool == "identify_gaps":
                        result = await self._identify_gaps_task(task.args["question"], task.args["answer"])

                    results.append(TaskResult(
                        task_id=task.idx,
                        result=result,
                        error=None
                    ))

                except Exception as e:
                    results.append(TaskResult(
                        task_id=task.idx,
                        result=None,
                        error=str(e)
                    ))

            return results

        except Exception as e:
            log_error_with_traceback(e, "Error executing QA tasks")
            raise

    async def _make_join_decision(self, state: CompilerState) -> JoinDecision:
        """Decide whether to complete or replan"""
        try:
            # Create join prompt
            plan_json = "{}"
            plan = state.get('plan')
            if plan is not None:
                plan_json = json.dumps(plan.dict() if hasattr(plan, 'dict') else plan, indent=2)

            results_json = "[]"
            results = state.get('results')
            if results:
                results_json = json.dumps([r.dict() if hasattr(r, 'dict') else r for r in results], indent=2)

            prompt, parser = get_join_decision_prompt()
            chain = prompt | self.llm | parser
            decision = await chain.ainvoke({
                "plan": plan_json,
                "results": results_json
            })
            return decision

        except Exception as e:
            log_error_with_traceback(e, "Error making join decision")
            raise

    async def _generate_final_result(self, state: CompilerState) -> Answer:
        """Generate final answer"""
        try:
            # Combine results into Answer
            answer = ""
            sources = []
            confidence = 0.0
            reasoning = ""
            validation_status = "pending"
            count = 0

            # Extract results from tasks
            for result in state.get('results', []):
                if result and result.result:
                    if isinstance(result.result, dict):
                        if 'answer' in result.result:
                            answer = result.result['answer']
                        if 'sources' in result.result:
                            sources.extend(result.result['sources'])
                        if 'confidence' in result.result:
                            confidence += float(result.result['confidence'])
                            count += 1
                        if 'reasoning' in result.result:
                            reasoning = result.result['reasoning']
                        if 'validation_status' in result.result:
                            validation_status = result.result['validation_status']

            # Average confidence scores
            if count > 0:
                confidence = confidence / count

            return Answer(
                answer=answer,
                sources=sources,
                confidence=confidence,
                reasoning=reasoning,
                validation_status=validation_status
            )

        except Exception as e:
            log_error_with_traceback(e, "Error generating final result")
            raise

    async def _retrieve_context_task(self, question: str) -> Dict[str, Any]:
        """Retrieve context from knowledge graph"""
        try:
            results = self.graph.query(f"""
                MATCH (d:Document)
                WHERE d.content CONTAINS $question
                RETURN d.content as content
                LIMIT 5
            """, {"question": question})
            
            context = "\n".join([r["content"] for r in results])
            return {
                "context": context,
                "sources": [r["content"][:100] + "..." for r in results]
            }
            
        except Exception as e:
            log_error_with_traceback(e, "Error retrieving context")
            raise

    async def _generate_answer_task(self, question: str, context: str) -> Dict[str, Any]:
        """Generate answer for a question."""
        try:
            # Get prompt and parser
            prompt = get_answer_generation_prompt()
            parser = PydanticOutputParser(pydantic_object=Answer)
            format_instructions = parser.get_format_instructions()
            
            chain = prompt | self.llm | parser
            
            # Generate answer
            result = await chain.ainvoke({
                "question": question,
                "context": context,
                "format_instructions": format_instructions
            })
            
            # Validate answer quality
            if result:
                answer_quality = await self._assess_answer_quality(result)
                if answer_quality < 0.5:  # Reduced from 0.6
                    log_warning_with_context(f"Low quality answer for: {question}", "QA")
                    return {
                        "answer": None,
                        "confidence": 0.0,
                        "sources": []
                    }
                
                # Update confidence based on quality
                result_dict = result.dict() if hasattr(result, 'dict') else result
                result_dict["confidence"] = answer_quality
                return result_dict
            
            return {
                "answer": None,
                "confidence": 0.0,
                "sources": []
            }
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating answer")
            return {
                "answer": "Error generating answer",
                "confidence": 0.0,
                "sources": []
            }

    async def _assess_answer_quality(self, answer: Answer) -> float:
        """Assess the quality of a generated answer."""
        try:
            # Initialize quality factors
            factors = {
                "completeness": 0.0,  # How complete is the answer
                "relevance": 0.0,     # How relevant to the question
                "clarity": 0.0,       # How clear and well-structured
                "support": 0.0        # How well supported by sources/reasoning
            }
            
            # Check completeness
            if answer.answer and len(answer.answer.split()) >= 15:  # Reduced from 25
                factors["completeness"] = 0.8
            elif answer.answer and len(answer.answer.split()) >= 8:  # Reduced from 10
                factors["completeness"] = 0.5
            
            # Check relevance via reasoning
            if answer.reasoning and len(answer.reasoning.split()) >= 15:  # Reduced from 20
                factors["relevance"] = 0.8
            elif answer.reasoning:
                factors["relevance"] = 0.5
            
            # Check clarity
            if answer.answer:
                has_structure = "." in answer.answer and "," in answer.answer
                has_connectors = any(word in answer.answer.lower() for word in ["because", "therefore", "however", "additionally"])
                if has_structure and has_connectors:
                    factors["clarity"] = 0.8
                elif has_structure:
                    factors["clarity"] = 0.5
            
            # Check support
            if answer.sources and len(answer.sources) >= 1:  # Reduced from 2
                factors["support"] = 0.8
            elif answer.sources:
                factors["support"] = 0.5
            
            # Calculate overall quality
            weights = {
                "completeness": 0.3,
                "relevance": 0.3,
                "clarity": 0.2,
                "support": 0.2
            }
            
            quality = sum(score * weights[factor] for factor, score in factors.items())
            return quality
            
        except Exception as e:
            log_error_with_traceback(e, "Error assessing answer quality")
            return 0.5  # Default quality on error

    async def _validate_answer_task(self, answer: Dict[str, Any]) -> Dict[str, Any]:
        """Validate generated answer"""
        try:
            # Extract key components
            answer_text = answer.get("answer", "")
            reasoning = answer.get("reasoning", "")
            sources = answer.get("sources", [])
            
            # Initialize confidence factors
            completeness = 0.0  # How complete is the answer
            relevance = 0.0    # How relevant is it to the question
            support = 0.0      # How well is it supported by sources
            coherence = 0.0    # How coherent/well-structured is it
            
            # Check answer completeness
            if answer_text and len(answer_text.split()) >= 25:
                completeness = 0.8
            elif answer_text and len(answer_text.split()) >= 10:
                completeness = 0.5
            
            # Check relevance via reasoning
            if reasoning and len(reasoning.split()) >= 20:
                relevance = 0.8
            elif reasoning:
                relevance = 0.5
            
            # Check source support
            if sources and len(sources) >= 2:
                support = 0.9
            elif sources:
                support = 0.6
            
            # Check coherence
            if answer_text and "." in answer_text and "," in answer_text:
                coherence = 0.8
            elif answer_text and "." in answer_text:
                coherence = 0.5
            
            # Calculate overall confidence
            confidence = (completeness + relevance + support + coherence) / 4
            
            # Determine validation status
            if confidence >= 0.7:
                validation_status = "validated"
            else:
                validation_status = "failed"
                confidence = max(0.0, confidence - 0.2)  # Penalize confidence
            
            return {
                **answer,
                "validation_status": validation_status,
                "confidence": confidence,
                "validation_metrics": {
                    "completeness": completeness,
                    "relevance": relevance,
                    "support": support,
                    "coherence": coherence
                }
            }
            
        except Exception as e:
            log_error_with_traceback(e, "Error validating answer")
            raise

    async def _identify_gaps_task(self, question: str, answer: Dict[str, Any]) -> Dict[str, Any]:
        """Identify knowledge gaps"""
        try:
            prompt = get_knowledge_gap_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=KnowledgeGap)
            result = await chain.ainvoke({
                "question": question,
                "answer": answer.get('answer', ''),
                "confidence": answer.get('confidence', 0.0),
                "context": answer.get('context', '')
            })
            return result.model_dump()
            
        except Exception as e:
            log_error_with_traceback(e, "Error identifying gaps")
            raise

    async def process_question(self, question: str) -> QAResponse:
        """Process a question through the workflow."""
        try:
            # Create initial state
            state = {
                "question": question,
                "plan": None,
                "results": [],
                "join_decision": None,
                "final_result": None
            }

            # Run LLM compiler workflow
            result = await self.run(state)
            return result if isinstance(result, QAResponse) else QAResponse(
                answer=f"Error processing question: No valid result",
                sources=[],
                confidence=0.0,
                reasoning="Processing failed"
            )

        except Exception as e:
            log_error_with_traceback(e, "Error processing question")
            raise 
```

.\scripts\reformulator.py
```python
"""Reformulator for converting conversation outputs into clear, concise answers."""

from typing import List, Dict, Any, Optional
from langchain_core.messages import BaseMessage
from langchain_core.language_models.base import BaseLanguageModel
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import PydanticOutputParser

from prompts.reformulation.reformulation_prompts import (
    ReformulatedAnswer,
    get_reformulation_prompt
)

from scripts.logging_config import log_error_with_traceback
from scripts.text_web_browser import SimpleTextBrowser, web_search

async def prepare_response(
    original_task: str,
    inner_messages: List[BaseMessage],
    reformulation_model: BaseLanguageModel,
    include_web_search: bool = False
) -> str:
    """Prepare a reformulated response from conversation messages."""
    try:
        # Get prompt and parser
        prompt, parser = get_reformulation_prompt()
        
        # Convert messages to string format
        conversation = ""
        for msg in inner_messages:
            if not msg.content:
                continue
            role = msg.type if hasattr(msg, 'type') else 'unknown'
            content = msg.content if isinstance(msg.content, str) else str(msg.content)
            conversation += f"{role}: {content}\n\n"
        
        # Optionally perform web search for additional context
        web_context = ""
        if include_web_search:
            try:
                search_results = await web_search(original_task)
                if search_results and not search_results.startswith("Error"):
                    web_context = f"\nRelevant web search results:\n{search_results}\n"
            except Exception as e:
                log_error_with_traceback(e, "Error performing web search")
        
        # Run reformulation chain
        chain = (
            {"original_task": RunnablePassthrough(), 
             "conversation": RunnableLambda(lambda x: x["conversation"] + x.get("web_context", ""))} 
            | prompt 
            | reformulation_model 
            | parser
        )
        
        result = await chain.ainvoke({
            "original_task": original_task,
            "conversation": conversation,
            "web_context": web_context
        })
        
        # Return just the answer part
        return result.answer
        
    except Exception as e:
        log_error_with_traceback(e, "Error reformulating response")
        return f"Error reformulating response: {str(e)}"

```

.\scripts\research_agent.py
```python
import logging
from typing import List, Dict, Any
from prompts.example_generation import get_example_generation_prompt

logger = logging.getLogger(__name__)

class ResearchAgent:
    def __init__(self, config, llm, example_parser):
        self.config = config
        self.llm = llm
        self.example_parser = example_parser
        self.state = None  # Will be set during initialization
        
    async def _generate_examples(self, knowledge: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate examples from knowledge entries."""
        examples = []
        
        for entry in knowledge:
            if "id" not in entry:
                logger.warning(f"Knowledge entry missing ID: {entry}")
                continue
            
            if entry["id"] not in self.state.synthetic_knowledge:
                logger.warning(f"Knowledge entry {entry['id']} not found in synthetic knowledge")
                continue
            
            full_entry = self.state.synthetic_knowledge[entry["id"]]
            
            # Extract content and metadata from full entry
            content = full_entry.get("content", "")
            metadata = full_entry.get("metadata", {})
            
            # Generate examples using content
            prompt = get_example_generation_prompt()
            chain = prompt | self.llm | self.example_parser
            result = await chain.ainvoke({"content": content})
            
            if not result or not hasattr(result, "examples"):
                logger.warning(f"No examples generated for knowledge {entry['id']}")
                continue
            
            # Add metadata to examples
            for example in result.examples:
                if hasattr(example, "metadata") and isinstance(example.metadata, dict):
                    example.metadata.update(metadata)
                examples.append(example)
            
        return examples 
```

.\scripts\synthetic_knowledge.py
```python
from typing import List, Dict, Optional, Any
from datetime import datetime
import json
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.documents import Document
from langchain_neo4j import Neo4jGraph
from rich.console import Console


from prompts.synthetic_knowledge import (
    Pattern,
    Hypothesis,
    Relationship,
    get_pattern_recognition_prompt,
    get_hypothesis_generation_prompt,
    get_relationship_inference_prompt
)
from prompts.synthetic_knowledge.knowledge_synthesis import (
    SyntheticKnowledge,
    SourceMetadata,
    get_knowledge_synthesis_prompt
)
from prompts.synthetic_knowledge.join_decision import get_join_decision_prompt

from scripts.logging_config import (
    log_error_with_traceback,
)
from scripts.llm_compiler import LLMCompiler, Task, Plan, TaskResult, JoinDecision, CompilerState
from prompts.knowledge_acquisition.confidence_evaluation import (
    get_confidence_evaluation_prompt,
    ConfidenceEvaluation,
    ConfidenceFactors
)

console = Console()

class SynthesisState(BaseModel):
    """Schema for synthesis workflow state"""
    input_documents: List[Document] = Field(description="Input documents")
    identified_patterns: List[Pattern] = Field(default_factory=list, description="Identified patterns")
    generated_hypotheses: List[Hypothesis] = Field(default_factory=list, description="Generated hypotheses")
    inferred_relationships: List[Relationship] = Field(default_factory=list, description="Inferred relationships")
    synthetic_knowledge: Optional[Dict[str, Any]] = Field(None, description="Generated synthetic knowledge")

class SyntheticKnowledgeGenerator(LLMCompiler):
    """Synthetic knowledge generation system."""

    def __init__(self, graph: Neo4jGraph, llm):
        """Initialize with graph database and language model."""
        super().__init__(llm)
        self.graph = graph

    async def generate_knowledge(self, documents: List[Document]) -> Dict[str, Any]:
        """Generate synthetic knowledge from documents."""
        try:
            # Create initial state
            state = {
                "documents": documents,
                "plan": None,
                "results": [],
                "join_decision": None,
                "final_result": None
            }

            # Run LLM compiler workflow
            result = await self.run(state)
            if isinstance(result, BaseModel):
                return result.model_dump()
            return result

        except Exception as e:
            log_error_with_traceback(e, "Error generating knowledge")
            raise

    async def _generate_plan(self, state: Dict[str, Any]) -> Plan:
        """Generate execution plan."""
        try:
            tasks = []
            task_idx = 0
            
            # Add pattern recognition tasks
            for doc in state.get("input_documents", []):
                tasks.append(Task(
                    idx=task_idx,
                    tool="recognize_patterns",
                    args={"content": doc.page_content},
                    dependencies=[]
                ))
                task_idx += 1
                
            # Add hypothesis generation task
            tasks.append(Task(
                idx=task_idx,
                tool="generate_hypotheses",
                args={"patterns": []},  # Will be filled from pattern recognition results
                dependencies=[t.idx for t in tasks]  # Depends on all pattern recognition tasks
            ))
            task_idx += 1
            
            # Add relationship inference task
            tasks.append(Task(
                idx=task_idx,
                tool="infer_relationships",
                args={"hypotheses": []},  # Will be filled from hypothesis generation results
                dependencies=[task_idx - 1]  # Depends on hypothesis generation task
            ))
            task_idx += 1
            
            # Add knowledge synthesis task
            tasks.append(Task(
                idx=task_idx,
                tool="synthesize_knowledge",
                args={
                    "patterns": [],  # Will be filled from pattern recognition results
                    "hypotheses": [],  # Will be filled from hypothesis generation results
                    "relationships": []  # Will be filled from relationship inference results
                },
                dependencies=[task_idx - 1]  # Depends on relationship inference task
            ))
            
            return Plan(
                tasks=tasks,
                thought="Generated plan to process documents through pattern recognition, hypothesis generation, relationship inference, and knowledge synthesis"
            )
            
        except Exception as e:
            log_error_with_traceback(e, "Error generating plan")
            raise

    async def _execute_tasks(self, tasks: List[Task]) -> List[TaskResult]:
        """Execute knowledge synthesis tasks"""
        try:
            results = []
            for task in tasks:
                try:
                    # Check dependencies
                    deps_met = all(
                        any(r.task_id == dep and not r.error for r in results)
                        for dep in task.dependencies
                    )
                    if not deps_met:
                        continue

                    # Execute task with formatted args
                    result = None
                    if task.tool == "recognize_patterns":
                        result = await self._recognize_patterns(task.args["content"])
                    elif task.tool == "generate_hypotheses":
                        result = await self._generate_hypotheses(task.args["patterns"])
                    elif task.tool == "infer_relationships":
                        result = await self._infer_relationships(task.args["hypotheses"])
                    elif task.tool == "synthesize_knowledge":
                        result = await self._synthesize_knowledge(
                            task.args["patterns"],
                            task.args["hypotheses"],
                            task.args["relationships"]
                        )

                    # Convert result to dict if needed
                    if result is not None:
                        if hasattr(result, 'model_dump'):
                            result_dict = result.model_dump()
                        elif hasattr(result, 'dict'):
                            result_dict = result.dict()
                        else:
                            result_dict = result if isinstance(result, dict) else {"value": result}

                        # Add thought field if missing
                        if "thought" not in result_dict:
                            result_dict["thought"] = f"Successfully executed {task.tool} task"
                    else:
                        result_dict = None

                    results.append(TaskResult(
                        task_id=task.idx,
                        result=result_dict,
                        error=None
                    ))

                except Exception as e:
                    results.append(TaskResult(
                        task_id=task.idx,
                        result=None,
                        error=str(e)
                    ))

            return results

        except Exception as e:
            log_error_with_traceback(e, "Error executing synthesis tasks")
            raise

    async def _make_join_decision(self, state: CompilerState) -> JoinDecision:
        """Decide whether to complete or replan"""
        try:
            # Create join prompt
            plan_json = "{}"
            plan = state.get('plan')
            if plan is not None:
                plan_json = json.dumps(plan.dict() if hasattr(plan, 'dict') else plan, indent=2)

            results_json = "[]"
            results = state.get('results')
            if results:
                results_json = json.dumps([r.dict() if hasattr(r, 'dict') else r for r in results], indent=2)

            prompt = get_join_decision_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=JoinDecision)
            decision = await chain.ainvoke({
                "plan": plan_json,
                "results": results_json
            })
            return decision

        except Exception as e:
            log_error_with_traceback(e, "Error making join decision")
            raise

    async def _generate_final_result(self, state: CompilerState) -> SyntheticKnowledge:
        """Generate final synthetic knowledge"""
        try:
            # Combine results into SyntheticKnowledge
            content = ""
            patterns = []
            hypotheses = []
            relationships = []
            confidence = 0.0
            validation_status = "pending"
            count = 0

            # Extract results from tasks
            for result in state.get('results', []):
                if result and result.result:
                    if isinstance(result.result, dict):
                        if 'content' in result.result:
                            content = result.result['content']
                        if 'patterns' in result.result:
                            patterns.extend([Pattern(**p) for p in result.result['patterns']])
                        if 'hypotheses' in result.result:
                            hypotheses.extend([Hypothesis(**h) for h in result.result['hypotheses']])
                        if 'relationships' in result.result:
                            relationships.extend([Relationship(**r) for r in result.result['relationships']])
                        if 'confidence' in result.result:
                            confidence += float(result.result['confidence'])
                            count += 1

            # Average confidence scores
            if count > 0:
                confidence = confidence / count

            # Create source metadata
            metadata = SourceMetadata(
                source_type="text",
                confidence_score=confidence,
                domain_relevance=confidence,
                timestamp=datetime.now().isoformat(),
                validation_status=validation_status
            )

            return SyntheticKnowledge(
                content=content if content else "No content generated",
                patterns=patterns,
                hypotheses=hypotheses,
                relationships=relationships,
                confidence=confidence,
                validation_status=validation_status,
                metadata=metadata
            )

        except Exception as e:
            log_error_with_traceback(e, "Error generating final result")
            raise

    async def _recognize_patterns(self, content: str) -> List[Dict[str, Any]]:
        """Recognize patterns in content."""
        try:
            prompt = get_pattern_recognition_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=Pattern)
            result = await chain.ainvoke({"content": content})
            return result.model_dump()
        except Exception as e:
            log_error_with_traceback(e, "Error recognizing patterns")
            raise

    async def _generate_hypotheses(self, patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate hypotheses from patterns."""
        try:
            prompt = get_hypothesis_generation_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=Hypothesis)
            result = await chain.ainvoke({"patterns": json.dumps(patterns, indent=2)})
            return result.model_dump()
        except Exception as e:
            log_error_with_traceback(e, "Error generating hypotheses")
            raise

    async def _infer_relationships(self, hypotheses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Infer relationships from hypotheses."""
        try:
            prompt = get_relationship_inference_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=Relationship)
            result = await chain.ainvoke({"hypotheses": json.dumps(hypotheses, indent=2)})
            return result.model_dump()
        except Exception as e:
            log_error_with_traceback(e, "Error inferring relationships")
            raise

    async def _synthesize_knowledge(
        self,
        patterns: List[Dict[str, Any]],
        hypotheses: List[Dict[str, Any]],
        relationships: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Synthesize knowledge from patterns, hypotheses, and relationships."""
        try:
            prompt = get_knowledge_synthesis_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=SyntheticKnowledge)
            result = await chain.ainvoke({
                "patterns": json.dumps(patterns, indent=2),
                "hypotheses": json.dumps(hypotheses, indent=2),
                "relationships": json.dumps(relationships, indent=2)
            })
            # Convert SourceMetadata to dict before returning
            result_dict = result.model_dump()
            if result_dict.get("metadata"):
                result_dict["metadata"] = result_dict["metadata"].model_dump()
            return result_dict
        except Exception as e:
            log_error_with_traceback(e, "Error synthesizing knowledge")
            raise

    async def _evaluate_confidence(self, content: str, entities: List[str], relationships: List[Dict]) -> ConfidenceEvaluation:
        """Evaluate confidence using proper evaluation prompts."""
        try:
            prompt = get_confidence_evaluation_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=ConfidenceEvaluation)
            result = await chain.ainvoke({
                "content": content,
                "entities": entities,
                "relationships": relationships,
                "source_type": "text"
            })
            return result
        except Exception as e:
            log_error_with_traceback(e, "Error evaluating confidence")
            raise

    async def _process_document(self, document: Document) -> Dict[str, Any]:
        """Process a single document."""
        try:
            # Process content
            patterns = await self._recognize_patterns(document.page_content)
            hypotheses = await self._generate_hypotheses(patterns)
            relationships = await self._infer_relationships(hypotheses)
            
            # Evaluate confidence
            confidence_eval = await self._evaluate_confidence(
                document.page_content,
                [p["pattern_type"] for p in patterns],
                relationships
            )
            
            # Create metadata with evaluated confidence
            metadata = {
                "source_type": "text",
                "confidence_score": confidence_eval.confidence,
                "domain_relevance": confidence_eval.factors.context_relevance,
                "timestamp": datetime.now().isoformat(),
                "validation_status": "pending"
            }
            
            # Synthesize knowledge
            knowledge = await self._synthesize_knowledge(patterns, hypotheses, relationships)
            if knowledge:
                knowledge["metadata"] = metadata
                
            return knowledge

        except Exception as e:
            log_error_with_traceback(e, "Error processing document")
            raise

    async def _execute_task(self, task: Task, state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a single task."""
        try:
            # Process task
            result = None
            if task.tool == "recognize_patterns":
                result = await self._recognize_patterns(task.args["content"])
            elif task.tool == "generate_hypotheses":
                result = await self._generate_hypotheses(task.args["patterns"])
            elif task.tool == "infer_relationships":
                result = await self._infer_relationships(task.args["hypotheses"])
            elif task.tool == "synthesize_knowledge":
                result = await self._synthesize_knowledge(
                    task.args["patterns"],
                    task.args["hypotheses"],
                    task.args["relationships"]
                )
                if result:
                    # Evaluate confidence
                    confidence_eval = await self._evaluate_confidence(
                        result.get("content", ""),
                        [p["pattern_type"] for p in result.get("patterns", [])],
                        result.get("relationships", [])
                    )
                    result["metadata"] = {
                        "source_type": "text",
                        "confidence_score": confidence_eval.confidence,
                        "domain_relevance": confidence_eval.factors.context_relevance,
                        "timestamp": datetime.now().isoformat(),
                        "validation_status": "pending"
                    }

            return {"task_id": task.idx, "result": result}

        except Exception as e:
            log_error_with_traceback(e, f"Error executing task {task.tool}")
            return {
                "task_id": task.idx,
                "result": None,
                "error": str(e)
            }

    async def _handle_error(self, error: Exception, state: Dict[str, Any]) -> Dict[str, Any]:
        """Handle errors during execution."""
        try:
            # Create error state with zero confidence
            error_state = state.copy()
            error_state["error"] = str(error)
            error_state["error_metadata"] = {
                "source_type": "text",
                "confidence_score": 0.0,
                "domain_relevance": 0.0,
                "timestamp": datetime.now().isoformat(),
                "validation_status": "failed"
            }
            error_state["status"] = "failed"

            return error_state

        except Exception as e:
            log_error_with_traceback(e, "Error handling error")
            raise

    async def _join_task_results(self, task_results: List[TaskResult], state: Dict[str, Any]) -> JoinDecision:
        """Join task results and decide next steps."""
        try:
            # Process results
            complete = True
            thought = "All tasks completed successfully"
            replan = False
            feedback = None

            for result in task_results:
                if result.error:
                    complete = False
                    thought = f"Task {result.task_id} failed: {result.error}"
                    replan = True
                    feedback = f"Task {result.task_id} needs to be retried"
                    break

                if result.result is None:
                    complete = False
                    thought = f"Task {result.task_id} returned no result"
                    replan = True
                    feedback = f"Task {result.task_id} needs to be retried"
                    break

                # Evaluate confidence for knowledge synthesis results
                if result.result.get("synthetic_knowledge"):
                    confidence_eval = await self._evaluate_confidence(
                        result.result["synthetic_knowledge"].get("content", ""),
                        [p["pattern_type"] for p in result.result["synthetic_knowledge"].get("patterns", [])],
                        result.result["synthetic_knowledge"].get("relationships", [])
                    )
                    result.result["synthetic_knowledge"]["metadata"] = {
                        "source_type": "text",
                        "confidence_score": confidence_eval.confidence,
                        "domain_relevance": confidence_eval.factors.context_relevance,
                        "timestamp": datetime.now().isoformat(),
                        "validation_status": "pending"
                    }

            return JoinDecision(
                complete=complete,
                thought=thought,
                replan=replan,
                feedback=feedback
            )

        except Exception as e:
            log_error_with_traceback(e, "Error joining task results")
            return JoinDecision(
                complete=False,
                thought=f"Error joining results: {str(e)}",
                replan=True,
                feedback="Error occurred during join, retry tasks"
            )

    async def _update_state(self, task_results: List[TaskResult], state: Dict[str, Any]) -> Dict[str, Any]:
        """Update state with task results."""
        try:
            # Process results
            new_state = state.copy()
            
            for result in task_results:
                if result.error or result.result is None:
                    continue
                    
                if "patterns" in result.result:
                    new_state["identified_patterns"] = result.result["patterns"]
                elif "hypotheses" in result.result:
                    new_state["generated_hypotheses"] = result.result["hypotheses"]
                elif "relationships" in result.result:
                    new_state["inferred_relationships"] = result.result["relationships"]
                elif "synthetic_knowledge" in result.result:
                    # Evaluate confidence
                    confidence_eval = await self._evaluate_confidence(
                        result.result["synthetic_knowledge"].get("content", ""),
                        [p["pattern_type"] for p in result.result["synthetic_knowledge"].get("patterns", [])],
                        result.result["synthetic_knowledge"].get("relationships", [])
                    )
                    result.result["synthetic_knowledge"]["metadata"] = {
                        "source_type": "text",
                        "confidence_score": confidence_eval.confidence,
                        "domain_relevance": confidence_eval.factors.context_relevance,
                        "timestamp": datetime.now().isoformat(),
                        "validation_status": "pending"
                    }
                    new_state["synthetic_knowledge"] = result.result["synthetic_knowledge"]
            
            return new_state

        except Exception as e:
            log_error_with_traceback(e, "Error updating state")
            raise

    async def _process_results(self, task_results: List[TaskResult]) -> Dict[str, Any]:
        """Process task results."""
        try:
            # Process results
            patterns = []
            hypotheses = []
            relationships = []
            synthetic_knowledge = None

            for result in task_results:
                if result.error or result.result is None:
                    continue

                if "patterns" in result.result:
                    patterns.extend(result.result["patterns"])
                elif "hypotheses" in result.result:
                    hypotheses.extend(result.result["hypotheses"])
                elif "relationships" in result.result:
                    relationships.extend(result.result["relationships"])
                elif "synthetic_knowledge" in result.result:
                    # Evaluate confidence
                    confidence_eval = await self._evaluate_confidence(
                        result.result["synthetic_knowledge"].get("content", ""),
                        [p["pattern_type"] for p in result.result["synthetic_knowledge"].get("patterns", [])],
                        result.result["synthetic_knowledge"].get("relationships", [])
                    )
                    result.result["synthetic_knowledge"]["metadata"] = {
                        "source_type": "text",
                        "confidence_score": confidence_eval.confidence,
                        "domain_relevance": confidence_eval.factors.context_relevance,
                        "timestamp": datetime.now().isoformat(),
                        "validation_status": "pending"
                    }
                    synthetic_knowledge = result.result["synthetic_knowledge"]

            return {
                "patterns": patterns,
                "hypotheses": hypotheses,
                "relationships": relationships,
                "synthetic_knowledge": synthetic_knowledge
            }

        except Exception as e:
            log_error_with_traceback(e, "Error processing results")
            raise

    async def _validate_results(self, task_results: List[TaskResult]) -> Dict[str, Any]:
        """Validate task results."""
        try:
            validation_errors = []
            validation_warnings = []

            for result in task_results:
                if result.error:
                    validation_errors.append(f"Task {result.task_id} failed: {result.error}")
                    continue

                if result.result is None:
                    validation_warnings.append(f"Task {result.task_id} returned no result")
                    continue

                # Validate patterns
                if "patterns" in result.result:
                    if not isinstance(result.result["patterns"], list):
                        validation_errors.append(f"Task {result.task_id}: patterns must be a list")
                    else:
                        for pattern in result.result["patterns"]:
                            if not all(k in pattern for k in ["pattern_type", "description", "supporting_evidence", "confidence"]):
                                validation_errors.append(f"Task {result.task_id}: invalid pattern format")

                # Validate hypotheses
                elif "hypotheses" in result.result:
                    if not isinstance(result.result["hypotheses"], list):
                        validation_errors.append(f"Task {result.task_id}: hypotheses must be a list")
                    else:
                        for hypothesis in result.result["hypotheses"]:
                            if not all(k in hypothesis for k in ["statement", "reasoning", "evidence", "confidence", "validation_status"]):
                                validation_errors.append(f"Task {result.task_id}: invalid hypothesis format")

                # Validate relationships
                elif "relationships" in result.result:
                    if not isinstance(result.result["relationships"], list):
                        validation_errors.append(f"Task {result.task_id}: relationships must be a list")
                    else:
                        for relationship in result.result["relationships"]:
                            if not all(k in relationship for k in ["source", "relation", "target"]):
                                validation_errors.append(f"Task {result.task_id}: invalid relationship format")

                # Validate synthetic knowledge
                elif "synthetic_knowledge" in result.result:
                    knowledge = result.result["synthetic_knowledge"]
                    if not all(k in knowledge for k in ["content", "patterns", "hypotheses", "relationships", "confidence", "validation_status"]):
                        validation_errors.append(f"Task {result.task_id}: invalid synthetic knowledge format")
                    else:
                        # Evaluate confidence for synthetic knowledge
                        confidence_eval = await self._evaluate_confidence(
                            knowledge.get("content", ""),
                            [p["pattern_type"] for p in knowledge.get("patterns", [])],
                            knowledge.get("relationships", [])
                        )
                        knowledge["metadata"] = {
                            "source_type": "text",
                            "confidence_score": confidence_eval.confidence,
                            "domain_relevance": confidence_eval.factors.context_relevance,
                            "timestamp": datetime.now().isoformat(),
                            "validation_status": "pending"
                        }

            return {
                "is_valid": len(validation_errors) == 0,
                "errors": validation_errors,
                "warnings": validation_warnings
            }

        except Exception as e:
            log_error_with_traceback(e, "Error validating results")
            raise 
```

.\scripts\text_inspector_tool.py
```python
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, validator


from langchain.text_splitter import RecursiveCharacterTextSplitter

from loguru import logger
import json

from .mdconvert import MarkdownConverter
from scripts.logging_config import log_error_with_traceback
from langchain_core.language_models.chat_models import BaseChatModel
from scripts.llm_compiler import LLMCompiler, Task, Plan, TaskResult, JoinDecision, CompilerState

# Import only the prompt functions and use our own model definitions
from prompts.text_inspection.text_analysis import get_text_analysis_prompt
from prompts.text_inspection.text_inspector_prompts import (
    get_plan_generation_prompt,
    get_join_decision_prompt
)

# Local model definitions that match the schema but are independent
class TextSegment(BaseModel):
    """Schema for text segments"""
    content: str = Field(description="Segment text")
    start_char: int = Field(description="Start character position")
    end_char: int = Field(description="End character position")
    metadata: Dict = Field(description="Segment metadata")

    @validator("content")
    def validate_content(cls, v: Any) -> str:
        if isinstance(v, str):
            return v
        if hasattr(v, "content"):
            return str(v.content)
        return str(v)

class Entity(BaseModel):
    """Schema for named entities"""
    text: str = Field(description="Entity text")
    title: str = Field(description="Entity title")

class TextAnalysis(BaseModel):
    """Schema for text analysis results"""
    content: str = Field(description="Original text content")
    segments: List[TextSegment] = Field(description="Text segments")
    key_points: List[str] = Field(description="Key points from text")
    entities: List[Entity] = Field(description="Extracted entities")
    relationships: List[str] = Field(description="Identified relationships")
    summary: str = Field(description="Text summary")

class InspectionState(BaseModel):
    """State for text inspection workflow"""
    text: str = Field(description="Input text")
    segments: List[TextSegment] = Field(default_factory=list)
    analysis: Optional[TextAnalysis] = None
    metadata: Dict = Field(default_factory=dict)

class TextInspector(LLMCompiler):
    """Text inspection system using LLMCompiler pattern"""
    def __init__(self, llm: Optional[BaseChatModel] = None):
        super().__init__(llm)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

    async def _generate_plan(self, state: CompilerState) -> Plan:
        """Generate text inspection plan"""
        try:
            prompt, parser = get_plan_generation_prompt()
            chain = prompt | self.llm | parser
            plan = await chain.ainvoke({"content": state.get('content', '')})
            return plan

        except Exception as e:
            log_error_with_traceback(e, "Error generating inspection plan")
            raise

    async def _execute_tasks(self, tasks: List[Task]) -> List[TaskResult]:
        """Execute text inspection tasks"""
        try:
            results = []
            for task in tasks:
                try:
                    # Check dependencies
                    deps_met = all(
                        any(r.task_id == dep and not r.error for r in results)
                        for dep in task.dependencies
                    )
                    if not deps_met:
                        continue

                    # Execute task
                    result = None
                    if task.tool == "analyze_text":
                        result = await self._analyze_text(task.args["content"])
                    elif task.tool == "identify_segments":
                        result = await self._identify_segments(task.args["content"])
                    elif task.tool == "extract_entities":
                        result = await self._extract_entities(task.args["content"])
                    elif task.tool == "identify_relationships":
                        result = await self._identify_relationships(task.args["content"])

                    results.append(TaskResult(
                        task_id=task.idx,
                        result=result,
                        error=None
                    ))

                except Exception as e:
                    results.append(TaskResult(
                        task_id=task.idx,
                        result=None,
                        error=str(e)
                    ))

            return results

        except Exception as e:
            log_error_with_traceback(e, "Error executing inspection tasks")
            raise

    async def _make_join_decision(self, state: CompilerState) -> JoinDecision:
        """Decide whether to complete or replan"""
        try:
            # Create join prompt
            plan_json = "{}"
            plan = state.get('plan')
            if plan is not None:
                plan_json = json.dumps(plan.dict() if hasattr(plan, 'dict') else plan, indent=2)

            results_json = "[]"
            results = state.get('results')
            if results:
                results_json = json.dumps([r.dict() if hasattr(r, 'dict') else r for r in results], indent=2)

            prompt, parser = get_join_decision_prompt()
            chain = prompt | self.llm | parser
            decision = await chain.ainvoke({
                "plan": plan_json,
                "results": results_json
            })
            return decision

        except Exception as e:
            log_error_with_traceback(e, "Error making join decision")
            raise

    async def _generate_final_result(self, state: CompilerState) -> TextAnalysis:
        """Generate final text analysis result"""
        try:
            # Combine results into TextAnalysis
            analysis = TextAnalysis(
                content=state.get('content', ''),
                segments=[],
                key_points=[],
                entities=[],
                relationships=[],
                summary=""
            )

            # Extract results from tasks
            for result in state.get('results', []):
                if result and result.result:
                    if isinstance(result.result, dict):
                        if 'segments' in result.result:
                            analysis.segments.extend(result.result['segments'])
                        if 'key_points' in result.result:
                            analysis.key_points.extend(result.result['key_points'])
                        if 'entities' in result.result:
                            analysis.entities.extend(result.result['entities'])
                        if 'relationships' in result.result:
                            analysis.relationships.extend(result.result['relationships'])
                        if 'summary' in result.result:
                            analysis.summary = result.result['summary']

            return analysis

        except Exception as e:
            log_error_with_traceback(e, "Error generating final result")
            raise

    async def inspect_text(self, text: str) -> TextAnalysis:
        """Inspect text content and extract structured information"""
        try:
            # Create initial state
            state = {
                "content": text,
                "plan": None,
                "results": [],
                "join_decision": None,
                "final_result": None
            }

            # Run LLMCompiler workflow
            result = await self.run(state)
            return result if isinstance(result, TextAnalysis) else TextAnalysis(
                content=text,
                segments=[TextSegment(
                    content=text,
                    start_char=0,
                    end_char=len(text),
                    metadata={}
                )],
                key_points=[],
                entities=[],
                relationships=[],
                summary=""
            )

        except Exception as e:
            log_error_with_traceback(e, "Error inspecting text")
            raise

    async def inspect_file(self, file_path: str) -> TextAnalysis:
        """Inspect a file and extract structured information"""
        try:
            # Read file content
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # Extract metadata from file path
            file_metadata = {
                "file_path": file_path,
                "file_type": file_path.split('.')[-1] if '.' in file_path else 'unknown'
            }
            
            return await self.inspect_text(text)
            
        except Exception as e:
            log_error_with_traceback(e, f"Error inspecting file: {file_path}")
            return TextAnalysis(
                content="",
                segments=[TextSegment(
                    content="",
                    start_char=0,
                    end_char=0,
                    metadata={}
                )],
                key_points=[],
                entities=[],
                relationships=[],
                summary=f"Failed to analyze file: {file_path}"
            )

    async def _analyze_text(self, content: str) -> Dict[str, Any]:
        """Analyze text content"""
        try:
            prompt, parser = get_text_analysis_prompt()
            chain = prompt | self.llm | parser
            result = await chain.ainvoke({"text": content})
            return result.model_dump()
            
        except Exception as e:
            log_error_with_traceback(e, "Error analyzing text")
            raise

    async def _identify_segments(self, content: str) -> Dict[str, Any]:
        """Identify logical segments in text"""
        try:
            # Split text into segments
            segments = []
            current_pos = 0
            
            # Use text splitter to get chunks
            chunks = self.text_splitter.split_text(content)
            
            # Convert chunks to segments
            for chunk in chunks:
                start_pos = content.find(chunk, current_pos)
                if start_pos != -1:
                    segments.append(TextSegment(
                        content=chunk,
                        start_char=start_pos,
                        end_char=start_pos + len(chunk),
                        metadata={
                            "type": "chunk",
                            "length": len(chunk)
                        }
                    ))
                    current_pos = start_pos + len(chunk)
            
            return {"segments": [s.model_dump() for s in segments]}
            
        except Exception as e:
            log_error_with_traceback(e, "Error identifying segments")
            raise

    async def _extract_entities(self, content: str) -> Dict[str, Any]:
        """Extract named entities from text"""
        try:
            prompt, parser = get_text_analysis_prompt()
            chain = prompt | self.llm | parser
            result = await chain.ainvoke({"text": content})
            return {"entities": [e.model_dump() for e in result.entities]}
            
        except Exception as e:
            log_error_with_traceback(e, "Error extracting entities")
            raise

    async def _identify_relationships(self, content: str) -> Dict[str, Any]:
        """Identify relationships between concepts"""
        try:
            prompt, parser = get_text_analysis_prompt()
            chain = prompt | self.llm | parser
            result = await chain.ainvoke({"text": content})
            return {"relationships": result.relationships}
            
        except Exception as e:
            log_error_with_traceback(e, "Error identifying relationships")
            raise

async def inspect_file_as_text(file_path: str, llm) -> Dict:
    """Convenience function for file inspection"""
    inspector = TextInspector(llm)
    result = await inspector.inspect_file(file_path)
    return result.model_dump()

```

.\scripts\text_web_browser_fixed.py
```python
"""Web browser utilities."""
import re
import os
import asyncio
from typing import Dict, Any, Optional, List, Set, Tuple
from bs4 import BeautifulSoup, Comment
from scripts.mdconvert import MarkdownConverter
from scripts.logging_config import log_error_with_traceback, log_warning_with_context
import aiohttp
from rich.table import Table
from rich import box
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_community.utilities import SearxSearchWrapper
from urllib.parse import urlparse
import hashlib
from tqdm.rich import tqdm
import numpy as np
from collections import defaultdict
from scripts.chat_langchain import ChatLangChain
from prompts.knowledge_acquisition.query_generation import (
    QueryGenerationResponse,
    get_query_generation_prompt
)
from pydantic import SecretStr
from langchain.output_parsers import PydanticOutputParser
import xml.etree.ElementTree as ET
import json
import ssl
from rich.progress import Progress, track
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

class SearchMetadata(BaseModel):
    """Metadata for a search result"""
    query: str = Field(description="Original search query")
    url: str = Field(description="Source URL")
    title: str = Field(description="Page title")
    snippet: str = Field(description="Search result snippet")
    source: str = Field(description="Source type (e.g., pubmed, arxiv)")
    published_date: Optional[str] = Field(None, description="Publication date if available")
    score: float = Field(default=0.0, description="Search result score")
    timestamp: str = Field(description="When the result was processed")
    hash: str = Field(description="Content hash for deduplication")
    query_group: str = Field(description="Group of related queries this came from")

class WebContent(BaseModel):
    """Model for processed web content"""
    title: str = Field(description="Title of the webpage")
    url: str = Field(description="URL of the webpage")
    content: str = Field(description="Cleaned and processed content")
    summary: str = Field(description="Brief summary of the content")
    domain: str = Field(description="Domain/topic this content belongs to")
    timestamp: str = Field(description="When the content was processed")
    metadata: SearchMetadata = Field(description="Search and source metadata")
    chunks: List[Dict[str, Any]] = Field(default_factory=list, description="Chunked content with embeddings")

class URLTracker:
    """Track and manage URLs to avoid duplicates"""
    def __init__(self):
        self.urls: Set[str] = set()
        self.content_hashes: Set[str] = set()
        self.domain_counts: Dict[str, int] = {}
        self.query_groups: Dict[str, Set[str]] = defaultdict(set)
        
    def add_url(self, url: str, query_group: str) -> bool:
        """Add URL if not seen before. Returns True if URL is new."""
        if url in self.urls:
            return False
            
        # Check domain limits
        domain = urlparse(url).netloc
        if self.domain_counts.get(domain, 0) >= 3:  # Max 3 URLs per domain
            return False
            
        # Check query group limits
        if len(self.query_groups[query_group]) >= 5:  # Max 5 URLs per query group
            return False
            
        self.urls.add(url)
        self.domain_counts[domain] = self.domain_counts.get(domain, 0) + 1
        self.query_groups[query_group].add(url)
        return True
        
    def add_content(self, content: str) -> bool:
        """Add content if not seen before. Returns True if content is new."""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        if content_hash in self.content_hashes:
            return False
        self.content_hashes.add(content_hash)
        return True

class SimpleTextBrowser:
    """Simple browser for extracting text from web pages."""
    
    def __init__(self):
        """Initialize browser."""
        self.browserless_url = "https://browserless.anuna.dev"
        self.session = aiohttp.ClientSession()
        
    async def __aenter__(self):
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
        
    async def visit(self, url: str) -> str:
        """Visit URL and extract text content using MarkdownConverter"""
        retries = 3
        retry_delay = 1.0
        
        for attempt in range(retries):
            try:
                # Use MarkdownConverter for better content handling
                converter = MarkdownConverter()
                result = converter.convert(url)
                
                if not result or not result.text_content:
                    log_warning_with_context("No text content found", "Browser")
                    if attempt < retries - 1:
                        await asyncio.sleep(retry_delay * (attempt + 1))
                        continue
                    return "Error: No content found"
                
                # Clean up the text
                text = result.text_content
                text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces
                text = re.sub(r'\n\s*\n', '\n\n', text)  # Replace multiple newlines
                text = text.strip()
                return text
            except Exception as e:
                log_error_with_traceback(e, f"Error visiting {url}")
                if attempt < retries - 1:
                    await asyncio.sleep(retry_delay * (attempt + 1))
                    continue
                return f"Error accessing content: {str(e)}"
        
        return "Error: Maximum retries exceeded"

async def generate_diverse_queries(base_query: str, domain: str) -> List[Tuple[str, str]]:
    """Generate diverse search queries with group labels using LLM."""
    try:
        # Initialize LLM
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise EnvironmentError("GOOGLE_API_KEY environment variable must be set")
            
        llm = ChatLangChain(
            model="gemini-1.5-flash",
            temperature=0.7,
            api_key=SecretStr(api_key),
            pydantic_schema=QueryGenerationResponse,
            format='json',
            response_format={"type": "json_object"}
        )
        
        # Extract key terms if queries are too long
        if len(domain) > 50:
            key_terms = [term.strip() for term in domain.split() if len(term) > 3][:3]
            domain = " ".join(key_terms)
            
        if len(base_query) > 50:
            key_terms = [term.strip() for term in base_query.split() if len(term) > 3][:3]
            base_query = " ".join(key_terms)
            
        # Get prompt template
        prompt = get_query_generation_prompt()
        
        # Generate queries using LLM
        chain = prompt | llm | PydanticOutputParser(pydantic_object=QueryGenerationResponse)
        response = await chain.ainvoke({
            "base_query": base_query,
            "domain": domain,
            "format_instructions": PydanticOutputParser(pydantic_object=QueryGenerationResponse).get_format_instructions()
        })
        
        # Convert to list of (query, group) tuples
        queries = []
        seen_queries = set()  # Track unique queries
        
        for group in response.query_groups:
            for query in group.queries:
                # Skip duplicates and ensure reasonable length
                if query in seen_queries or len(query) > 100:
                    continue
                    
                # Clean and format query
                query = query.strip()
                if not query:
                    continue
                    
                # Add to results
                group_id = f"{group.group_name}_{len(queries)}"
                queries.append((query, group_id))
                seen_queries.add(query)
                
        # Shuffle while maintaining group associations
        indices = np.random.permutation(len(queries))
        return [queries[i] for i in indices]
        
    except Exception as e:
        log_error_with_traceback(e, "Error generating queries")
        # Fallback to basic queries if LLM fails
        return [
            (f"{domain} {base_query} overview", "overview_0"),
            (f"{domain} {base_query} research", "research_0"),
            (f"{domain} {base_query} treatment", "treatment_0")
        ]

async def clean_content(content: str) -> str:
    """Clean and normalize content."""
    # Remove HTML
    soup = BeautifulSoup(content, "html.parser")
    
    # Remove unwanted elements
    for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
        element.decompose()
        
    # Remove comments
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()
        
    # Get main content
    main_content = ""
    main_tags = soup.find_all(["article", "main", "div[role='main']"])
    if main_tags:
        main_content = " ".join(tag.get_text(strip=True, separator=" ") for tag in main_tags)
    else:
        body = soup.find("body")
        if body:
            main_content = body.get_text(strip=True, separator=" ")
        else:
            main_content = soup.get_text(strip=True, separator=" ")
            
    # Clean up text
    main_content = re.sub(r'\s+', ' ', main_content)  # Normalize whitespace
    main_content = re.sub(r'\[.*?\]', '', main_content)  # Remove square bracket content
    main_content = re.sub(r'[^\x00-\x7F]+', '', main_content)  # Remove non-ASCII
    main_content = re.sub(r'(\w)\1{3,}', r'\1\1', main_content)  # Normalize repeated chars
    main_content = main_content.strip()
    
    return main_content

async def fetch_content(browser: SimpleTextBrowser, result: Dict[str, Any], query: str, query_group: str) -> Optional[WebContent]:
    """Fetch and process content from a search result."""
    try:
        url = result.get("link", "").strip()
        if not url:
            return None
            
        # Get content
        content = await browser.visit(url)
        if not content or "Error accessing content:" in content:
            return None
            
        # Clean content
        cleaned_content = await clean_content(content)
        if not cleaned_content:
            return None
            
        # Create metadata
        metadata = SearchMetadata(
            query=query,
            url=url,
            title=result.get("title", ""),
            snippet=result.get("snippet", ""),
            source=result.get("source", ""),
            published_date=result.get("published_date"),
            score=float(result.get("score", 0)),
            timestamp=datetime.now().isoformat(),
            hash=hashlib.md5(cleaned_content.encode()).hexdigest(),
            query_group=query_group
        )
        
        # Create web content object
        return WebContent(
            title=result.get("title", ""),
            url=url,
            content=cleaned_content,
            summary=result.get("snippet", ""),
            domain=result.get("domain", ""),
            timestamp=datetime.now().isoformat(),
            metadata=metadata,
            chunks=[]  # Will be populated later
        )
        
    except Exception as e:
        log_error_with_traceback(e, f"Error processing result {url}")
        return None

async def web_search(query: str, config: Dict[str, Any]) -> str:
    """Perform web search using multiple fallback strategies."""
    try:
        if not query or not query.strip():
            return "Error: Empty search query"
            
        # Get domain from config and extract key terms
        domain = config.get("domain_name", "general")
        disable_progress = config.get("disable_progress", False)
        
        # Generate diverse search queries using LLM
        search_queries = await generate_diverse_queries(query, domain)
        
        # Initialize URL tracker
        url_tracker = URLTracker()
        
        # Initialize search with multiple fallback engines
        search_engines = [
            {
                "name": "pubmed",
                "url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
                "params": {"db": "pubmed", "term": "{query}", "retmax": "10", "format": "json"},
                "rate_limit": 3.0  # Requests per second
            },
            {
                "name": "arxiv",
                "url": "http://export.arxiv.org/api/query",
                "params": {"search_query": "{query}", "max_results": "10"},
                "rate_limit": 1.0  # Requests per second
            }
        ]
        
        formatted_results = []
        web_contents = []
        
        # Process queries in parallel with rate limiting
        async with SimpleTextBrowser() as browser:
            semaphore = asyncio.Semaphore(2)  # Limit concurrent requests
            
            async def process_query(query: str, query_group: str) -> List[WebContent]:
                async with semaphore:
                    try:
                        results = []
                        # Try each search engine with rate limiting
                        for engine in search_engines:
                            try:
                                await asyncio.sleep(1.0 / engine["rate_limit"])  # Rate limiting
                                engine_results = await _search_with_engine(engine, query)
                                if engine_results:
                                    results.extend(engine_results)
                            except Exception as e:
                                log_error_with_traceback(e, f"Error with {engine['name']}")
                                continue
                        
                        # Process results in parallel with rate limiting
                        tasks = []
                        for result in results[:10]:
                            # Check URL uniqueness
                            url = result.get("link", "").strip()
                            if not url or not url_tracker.add_url(url, query_group):
                                continue
                                
                            # Add delay between requests
                            await asyncio.sleep(0.5)  # Rate limiting for content fetching
                            tasks.append(fetch_content(browser, result, query, query_group))
                            
                        # Wait for all content fetching to complete
                        contents = await asyncio.gather(*tasks)
                        return [c for c in contents if c is not None]
                        
                    except Exception as e:
                        log_error_with_traceback(e, f"Error processing query: {query}")
                        return []
            
            # Process queries with tqdm progress
            tasks = []
            if disable_progress:
                for query, group in search_queries:
                    tasks.append(process_query(query, group))
            else:
                for query, group in tqdm(search_queries, desc="Processing search queries"):
                    tasks.append(process_query(query, group))
                
            # Wait for all queries to complete
            all_results = await asyncio.gather(*tasks)
            
            # Flatten results and filter duplicates
            for results in all_results:
                for content in results:
                    if url_tracker.add_content(content.content):
                        web_contents.append(content)
            
            # Save results to files with progress bar
            if disable_progress:
                for i, content in enumerate(web_contents, 1):
                    filename = f"web/source_{i}.txt"
                    os.makedirs("web", exist_ok=True)  # Ensure directory exists
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(f"title: {content.title}\n")
                        f.write(f"url: {content.url}\n")
                        f.write(f"query: {content.metadata.query}\n")
                        f.write(f"query_group: {content.metadata.query_group}\n")
                        f.write(f"source: {content.metadata.source}\n")
                        f.write(f"published_date: {content.metadata.published_date or ''}\n")
                        f.write(f"timestamp: {content.timestamp}\n")
                        f.write("---\n")
                        f.write(content.content)
                    
                    formatted_results.append(str(content.model_dump()))
                    formatted_results.append("---")
            else:
                for i, content in enumerate(tqdm(web_contents, desc="Saving results"), 1):
                    filename = f"web/source_{i}.txt"
                    os.makedirs("web", exist_ok=True)  # Ensure directory exists
                    with open(filename, "w", encoding="utf-8") as f:
                        f.write(f"title: {content.title}\n")
                        f.write(f"url: {content.url}\n")
                        f.write(f"query: {content.metadata.query}\n")
                        f.write(f"query_group: {content.metadata.query_group}\n")
                        f.write(f"source: {content.metadata.source}\n")
                        f.write(f"published_date: {content.metadata.published_date or ''}\n")
                        f.write(f"timestamp: {content.timestamp}\n")
                        f.write("---\n")
                        f.write(content.content)
                    
                    formatted_results.append(str(content.model_dump()))
                    formatted_results.append("---")
                
        return "\n".join(formatted_results) if formatted_results else "No results found"
            
    except Exception as e:
        log_error_with_traceback(e, "Error in web search")
        return f"Search failed: {str(e)}"

async def _search_with_engine(engine: Dict[str, Any], query: str) -> List[Dict[str, Any]]:
    """Search using a specific engine with retries and error handling."""
    try:
        # Format query for engine
        params = engine["params"].copy()
        for k, v in params.items():
            if isinstance(v, str):
                params[k] = v.format(query=query)
                
        # Configure SSL verification
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = True
        ssl_context.verify_mode = ssl.CERT_REQUIRED
        
        # Configure connection pooling and retries
        connector = aiohttp.TCPConnector(
            ssl=ssl_context,
            limit=10,  # Connection pool limit
            force_close=False,  # Keep connections alive
            enable_cleanup_closed=True  # Clean up closed connections
        )
        
        # Configure timeout and retry settings
        timeout = aiohttp.ClientTimeout(total=30, connect=10, sock_read=10)
        
        # Make request with retries
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            raise_for_status=True
        ) as session:
            for attempt in range(3):
                try:
                    async with session.get(engine["url"], params=params) as response:
                        if response.status == 200:
                            data = await response.text()
                            # Parse response based on engine
                            if engine["name"] == "pubmed":
                                return _parse_pubmed_response(data)
                            elif engine["name"] == "arxiv":
                                return _parse_arxiv_response(data)
                        # Rate limiting
                        await asyncio.sleep(1.0 / engine.get("rate_limit", 1.0))
                except Exception as e:
                    if attempt == 2:  # Last attempt
                        log_error_with_traceback(e, f"Error searching {engine['name']}")
                    await asyncio.sleep(1 * (attempt + 1))  # Exponential backoff
                    continue
        return []
    except Exception as e:
        log_error_with_traceback(e, f"Error in {engine['name']} search")
        return []

def _parse_pubmed_response(data: str) -> List[Dict[str, Any]]:
    """Parse PubMed API response."""
    try:
        results = []
        # Parse JSON response
        response = json.loads(data)
        
        # Extract article IDs
        if "esearchresult" in response and "idlist" in response["esearchresult"]:
            for pmid in response["esearchresult"]["idlist"]:
                results.append({
                    "link": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                    "title": f"PubMed Article {pmid}",
                    "snippet": "",
                    "source": "pubmed"
                })
        return results
    except Exception as e:
        log_error_with_traceback(e, "Error parsing PubMed response")
        return []

def _parse_arxiv_response(data: str) -> List[Dict[str, Any]]:
    """Parse arXiv API response."""
    try:
        results = []
        root = ET.fromstring(data)
        for entry in root.findall(".//{http://www.w3.org/2005/Atom}entry"):
            title = entry.find(".//{http://www.w3.org/2005/Atom}title")
            link = entry.find(".//{http://www.w3.org/2005/Atom}id")
            summary = entry.find(".//{http://www.w3.org/2005/Atom}summary")
            if title is not None and link is not None:
                results.append({
                    "link": link.text,
                    "title": title.text,
                    "snippet": summary.text if summary is not None else "",
                    "source": "arxiv"
                })
        return results
    except Exception as e:
        log_error_with_traceback(e, "Error parsing arXiv response")
        return []

async def _fetch_url_content(session: aiohttp.ClientSession, url: str) -> str:
    """Base function to fetch content from a URL."""
    try:
        async with session.get(url) as response:
            return await response.text()
    except Exception as e:
        print(f"Error fetching {url}: {str(e)}")
        return ""

async def _fetch_url_content_with_ssl(session: aiohttp.ClientSession, url: str) -> str:
    """Fetch content from a URL with SSL verification disabled."""
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    connector = aiohttp.TCPConnector(ssl=ssl_context)
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(connector=connector) as ssl_session:
        try:
            async with ssl_session.get(url, timeout=timeout) as response:
                return await response.text()
        except Exception as e:
            print(f"Error fetching {url}: {str(e)}")
            return ""

async def fetch_url_content(session: aiohttp.ClientSession, url: str) -> str:
    """Fetch content from a URL, handling both regular and SSL requests."""
    if "pubmed.ncbi.nlm.nih.gov" in url:
        return await _fetch_url_content_with_ssl(session, url)
    return await _fetch_url_content(session, url)

async def process_search_query(session: aiohttp.ClientSession, query: str, engine: str = "google") -> List[Dict[str, str]]:
    """Process a search query and return results."""
    try:
        if engine == "google":
            results = await google_search(session, query)
        elif engine == "arxiv":
            results = await arxiv_search(session, query)
        else:
            results = []
        
        web_contents = []
        for result in results:
            content = await fetch_url_content(session, result["url"])
            if content:
                web_contents.append({
                    "title": result.get("title", ""),
                    "url": result["url"],
                    "content": content
                })
        return web_contents
    except Exception as e:
        print(f"Error processing query {query}: {str(e)}")
        return []

async def google_search(session: aiohttp.ClientSession, query: str, num_results: int = 10) -> List[Dict[str, str]]:
    """Perform a Google search and return results."""
    try:
        service = build("customsearch", "v1", developerKey=os.getenv("GOOGLE_API_KEY"))
        result = service.cse().list(q=query, cx=os.getenv("GOOGLE_CSE_ID"), num=num_results).execute()
        
        search_results = []
        for item in result.get("items", []):
            search_results.append({
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "snippet": item.get("snippet", "")
            })
        return search_results
    except HttpError as e:
        if e.resp.status == 429:  # Rate limit exceeded
            print("Google API rate limit exceeded. Waiting before retrying...")
            await asyncio.sleep(60)  # Wait for 60 seconds
            return await google_search(session, query, num_results)
        else:
            print(f"Google search error: {str(e)}")
            return []
    except Exception as e:
        print(f"Error in Google search: {str(e)}")
        return []

async def arxiv_search(session: aiohttp.ClientSession, query: str, max_results: int = 10) -> List[Dict[str, str]]:
    """Perform an arXiv search and return results."""
    try:
        base_url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": query,
            "max_results": max_results,
            "sortBy": "relevance",
            "sortOrder": "descending"
        }
        
        async with session.get(base_url, params=params) as response:
            content = await response.text()
            
            # Parse XML response
            results = []
            # Add XML parsing logic here if needed
            return results
    except Exception as e:
        print(f"Error in arXiv search: {str(e)}")
        return [] 
```

.\scripts\training.py
```python
from typing import List, Dict, Optional

from loguru import logger

class Training:
    def train(self, examples: List[Dict], num_samples: Optional[int] = None) -> None:
        """Train the model on examples"""
        if not examples:
            logger.warning("No training examples provided")
            return
            
        # Ensure num_samples is valid
        if num_samples is None:
            num_samples = len(examples)
        else:
            num_samples = max(1, min(num_samples, len(examples)))  # Ensure between 1 and len(examples)
            
        logger.info(f"Training on {num_samples} examples")
        
        # Convert examples to training format
        training_data = []
        for example in examples[:num_samples]:
            try:
                formatted = self._format_example(example)
                if formatted:
                    training_data.append(formatted)
            except Exception as e:
                logger.warning(f"Failed to format example: {e}")
                
        if not training_data:
            logger.warning("No valid training examples after formatting")
            return
            
        try:
            # Train model
            self.model.train(training_data)
            logger.info("Training complete")
        except Exception as e:
            logger.error(f"Training failed: {e}")
            raise 
```

.\scripts\types.py
```python
from typing import Dict, List, Optional, TypedDict, Any, Union
from pydantic import BaseModel
from langchain_core.documents import Document
from prompts.compiler.compiler_prompts import Plan, TaskResult, JoinDecision

class CompilerState(TypedDict, total=False):
    """Compiler state with optional fields"""
    # Required fields
    content: str
    error: Optional[str]
    feedback: Optional[str]
    
    # Optional fields
    domain_name: str
    plan: Optional[Plan]  # Use Plan type directly
    results: List[TaskResult]  # Use TaskResult type directly
    join_decision: Optional[JoinDecision]
    final_result: Optional[Dict[str, Any]]
    knowledge_sources: List[Union[Document, Dict[str, Any]]]
    synthetic_knowledge: List[Dict[str, Any]]
    serializable_knowledge: List[Dict[str, Any]]
    training_examples: List[Dict[str, Any]]
    model_metrics: Dict[str, Any]

class SystemState(BaseModel):
    """System state"""
    domain_name: str
    knowledge_sources: List[Dict[str, Any]] = []
    generated_questions: List[Dict[str, Any]] = []
    synthetic_knowledge: List[Dict[str, Any]] = []
    training_examples: List[Dict[str, Any]] = []
    model_metrics: Dict[str, Any] = {} 
```

.\scripts\visual_qa.py
```python
"""Visual QA system."""

from typing import List, Dict, Optional, Any
from pathlib import Path
from io import BytesIO
import base64
import json
import requests
from dotenv import load_dotenv

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnableLambda
# Image processing
from PIL import Image

# Logging
from loguru import logger

# Local imports
from prompts.visual_qa.element_detection import (
    Region,
    VisualElement,
    get_element_detection_prompt
)
from prompts.visual_qa.scene_analysis import (
    VisualAttributes,
    SceneAnalysis,
    get_scene_analysis_prompt
)
from prompts.visual_qa.visual_qa_prompts import (
    VisualAnswer,
    get_visual_qa_prompt
)
from prompts.visual_qa.plan_generation import (
    Plan,
    get_plan_generation_prompt
)
from prompts.visual_qa.join_decision import (
    JoinDecision,
    get_join_decision_prompt
)

from scripts.logging_config import (
    log_error_with_traceback,
)

from scripts.llm_compiler import LLMCompiler, Task, TaskResult, CompilerState

load_dotenv(override=True)

def encode_image(image_path: str) -> str:
    """Convert image to Base64 encoded string."""
    try:
        with Image.open(image_path) as img:
            # Convert to RGB if needed
            if img.mode != "RGB":
                img = img.convert("RGB")
            
            # Save to BytesIO in JPEG format
            buffered = BytesIO()
            img.save(buffered, format="JPEG")
            img_str = base64.b64encode(buffered.getvalue()).decode()
            return img_str
    except Exception as e:
        logger.error(f"Error encoding image {image_path}: {e}")
        raise

def resize_image(image_path: str) -> str:
    """Resize an image to half its dimensions."""
    try:
        # Convert to Path object for proper path handling
        path = Path(image_path)
        img = Image.open(path)
        width, height = img.size
        img = img.resize((int(width / 2), int(height / 2)))
        
        # Create resized directory if it doesn't exist
        resized_dir = path.parent / "resized"
        resized_dir.mkdir(exist_ok=True)
        
        # Create new path for resized image
        new_image_path = resized_dir / f"resized_{path.name}"
        img.save(new_image_path)
        return str(new_image_path)
    except Exception as e:
        logger.error(f"Error resizing image {image_path}: {e}")
        raise

class VisualAnalyzer(LLMCompiler):
    """Visual analysis system."""

    def __init__(self, llm):
        """Initialize with language model."""
        super().__init__(llm)

    async def _detect_elements(self, image_path: str) -> List[VisualElement]:
        """Detect visual elements in the image."""
        try:
            # Encode image
            encoded_image = encode_image(image_path)
            
            # Get detection prompt
            prompt = get_element_detection_prompt()
            
            # Run detection
            chain = prompt | self.llm | RunnableLambda(lambda x: [VisualElement(**e) for e in json.loads(x.content if hasattr(x, 'content') else x).get('elements', [])])
            elements = await chain.ainvoke({"image": encoded_image})
            return elements
            
        except Exception as e:
            log_error_with_traceback(e, "Error detecting elements")
            return []

    async def _analyze_scene(self, image_path: str, elements: List[VisualElement]) -> SceneAnalysis:
        """Analyze the scene composition."""
        try:
            # Encode image
            encoded_image = encode_image(image_path)
            
            # Get scene analysis prompt
            prompt = get_scene_analysis_prompt()
            
            # Run analysis
            chain = prompt | self.llm | RunnableLambda(lambda x: SceneAnalysis(**json.loads(x.content if hasattr(x, 'content') else x)))
            scene = await chain.ainvoke({
                "image": encoded_image,
                "elements": [e.model_dump() for e in elements]
            })
            return scene
            
        except Exception as e:
            log_error_with_traceback(e, "Error analyzing scene")
            return SceneAnalysis(
                scene_description="Failed to analyze scene",
                key_objects=[],
                spatial_relationships=[],
                visual_attributes=VisualAttributes(
                    lighting="unknown",
                    composition="unknown",
                    style="unknown"
                ),
                confidence=0.0
            )

    async def _answer_question(self, image_path: str, question: str, scene: SceneAnalysis) -> VisualAnswer:
        """Answer a question about the image."""
        try:
            # Encode image
            encoded_image = encode_image(image_path)
            
            # Get QA prompt
            prompt = get_visual_qa_prompt()
            
            # Run QA
            chain = prompt | self.llm | RunnableLambda(lambda x: VisualAnswer(**json.loads(x.content if hasattr(x, 'content') else x)))
            answer = await chain.ainvoke({
                "image": encoded_image,
                "question": question,
                "scene_description": scene.scene_description,
                "key_objects": scene.key_objects,
                "spatial_relationships": scene.spatial_relationships
            })
            return answer
            
        except Exception as e:
            log_error_with_traceback(e, "Error answering question")
            return VisualAnswer(
                answer="Failed to answer question",
                visual_evidence=[],
                context="Analysis failed",
                confidence=0.0
            )

    async def _generate_plan(self, state: CompilerState) -> Plan:
        """Generate visual analysis plan."""
        try:
            prompt = get_plan_generation_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=Plan)
            plan = await chain.ainvoke({
                "image_path": state.get('image_path', ''),
                "question": state.get('question', '')
            })
            return plan

        except Exception as e:
            log_error_with_traceback(e, "Error generating visual analysis plan")
            raise

    async def _execute_tasks(self, tasks: List[Task]) -> List[TaskResult]:
        """Execute visual analysis tasks."""
        try:
            results = []
            elements = []
            scene = None
            
            for task in tasks:
                try:
                    # Check dependencies
                    deps_met = all(
                        any(r.task_id == dep and not r.error for r in results)
                        for dep in task.dependencies
                    )
                    if not deps_met:
                        continue

                    # Execute task
                    result = None
                    if task.tool == "detect_elements":
                        elements = await self._detect_elements(task.args["image_path"])
                        result = {"elements": [e.model_dump() for e in elements]}
                        
                    elif task.tool == "analyze_scene":
                        scene = await self._analyze_scene(task.args["image_path"], elements)
                        result = scene.model_dump()
                        
                    elif task.tool == "answer_question":
                        if scene:
                            answer = await self._answer_question(
                                task.args["image_path"],
                                task.args["question"],
                                scene
                            )
                            result = answer.model_dump()

                    results.append(TaskResult(
                        task_id=task.idx,
                        result=result,
                        error=None
                    ))

                except Exception as e:
                    results.append(TaskResult(
                        task_id=task.idx,
                        result=None,
                        error=str(e)
                    ))

            return results

        except Exception as e:
            log_error_with_traceback(e, "Error executing visual analysis tasks")
            raise

    async def _make_join_decision(self, state: CompilerState) -> JoinDecision:
        """Decide whether to complete or replan"""
        try:
            # Create join prompt
            plan_json = "{}"
            plan = state.get('plan')
            if plan is not None:
                plan_json = json.dumps(plan.dict() if hasattr(plan, 'dict') else plan, indent=2)

            results_json = "[]"
            results = state.get('results')
            if results:
                results_json = json.dumps([r.dict() if hasattr(r, 'dict') else r for r in results], indent=2)

            prompt = get_join_decision_prompt()
            chain = prompt | self.llm | PydanticOutputParser(pydantic_object=JoinDecision)
            decision = await chain.ainvoke({
                "plan": plan_json,
                "results": results_json
            })
            return decision

        except Exception as e:
            log_error_with_traceback(e, "Error making join decision")
            raise

    async def _generate_final_result(self, state: CompilerState) -> VisualAnswer:
        """Generate final visual analysis result."""
        try:
            # Extract result from tasks
            for result in state.get('results', []):
                if result and result.result:
                    if isinstance(result.result, dict):
                        return VisualAnswer(
                            answer=result.result.get('answer', 'Failed to analyze image'),
                            visual_evidence=result.result.get('visual_evidence', []),
                            context=result.result.get('context', 'Analysis failed'),
                            confidence=result.result.get('confidence', 0.0)
                        )

            # Return default result if no valid results found
            return VisualAnswer(
                answer="Failed to analyze image",
                visual_evidence=[],
                context="Analysis failed",
                confidence=0.0
            )

        except Exception as e:
            log_error_with_traceback(e, "Error generating final result")
            raise

    async def analyze_image(self, image_path: str, question: str) -> VisualAnswer:
        """Analyze an image and answer questions about it."""
        try:
            # Create initial state
            state = {
                "image_path": image_path,
                "question": question,
                "plan": None,
                "results": [],
                "join_decision": None,
                "final_result": None
            }

            # Run LLM compiler workflow
            result = await self.run(state)
            return result if isinstance(result, VisualAnswer) else VisualAnswer(
                answer="Failed to analyze image",
                visual_evidence=[],
                context="Analysis failed",
                confidence=0.0
            )

        except Exception as e:
            log_error_with_traceback(e, "Error analyzing image")
            raise 
```

.\tests\test_knowledge_graph_integration.py
```python
import asyncio
import json
import os
from datetime import datetime, timedelta

import numpy as np
import pytest
from neo4j import GraphDatabase
from langchain_community.embeddings import HuggingFaceEmbeddings

from knowledge_graph_runner import (
    DomainConfig,
    KnowledgeGraphSystem,
    KnowledgeNode,
    KnowledgeRelation,
    GraphRAGProcessor
)

# Test configuration
TEST_NEO4J_URI = os.getenv("TEST_NEO4J_URI", "bolt://localhost:7687")
TEST_NEO4J_USER = os.getenv("TEST_NEO4J_USER", "neo4j")
TEST_NEO4J_PASSWORD = os.getenv("TEST_NEO4J_PASSWORD", "password")

@pytest.fixture(scope="session")
def domain_config():
    return DomainConfig(
        name="test_medical",
        entity_types=["Disease", "Drug", "Symptom"],
        relation_types=["TREATS", "CAUSES"],
        validation_rules={
            "confidence_threshold": 0.7,
            "required_sources": 1,
            "max_relation_distance": 2,
            "temporal_validity": {
                "publication_max_age_years": 5,
                "clinical_trial_status": ["ACTIVE", "COMPLETED"]
            }
        },
        search_strategies={
            "primary_sources": ["pubmed"],
            "search_depth": 2,
            "max_results_per_query": 10,
            "prioritize_recent": True,
            "include_preprints": False,
            "citation_threshold": 3
        }
    )

@pytest.fixture(scope="session")
def neo4j_driver():
    driver = GraphDatabase.driver(
        TEST_NEO4J_URI,
        auth=(TEST_NEO4J_USER, TEST_NEO4J_PASSWORD)
    )
    yield driver
    driver.close()

@pytest.fixture(autouse=True)
def clean_neo4j(neo4j_driver):
    # Clean up before and after each test
    with neo4j_driver.session() as session:
        session.run("MATCH (n) DETACH DELETE n")
        session.run("DROP INDEX node_embeddings IF EXISTS")
    yield
    with neo4j_driver.session() as session:
        session.run("MATCH (n) DETACH DELETE n")
        session.run("DROP INDEX node_embeddings IF EXISTS")

@pytest.fixture
def knowledge_graph_system(domain_config):
    system = KnowledgeGraphSystem(domain_config)
    return system

@pytest.fixture
def embeddings():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

@pytest.mark.asyncio
async def test_knowledge_acquisition_and_graph_rag(knowledge_graph_system, neo4j_driver):
    # Test basic knowledge acquisition with GraphRAG
    queries = [
        "What are the primary treatments for Type 2 Diabetes?",
        "What are the main side effects of Metformin?"
    ]
    
    results = await knowledge_graph_system.run(queries)
    
    # Verify results structure
    assert isinstance(results, dict)
    assert results["processed_documents"] > 0
    assert results["extracted_nodes"] > 0
    assert results["extracted_relationships"] > 0
    
    # Verify Neo4j graph population with embeddings
    with neo4j_driver.session() as session:
        # Check nodes have embeddings
        result = session.run("""
            MATCH (n)
            WHERE exists(n.embedding)
            RETURN count(n) as count
        """)
        assert result.single()["count"] > 0
        
        # Check relationships have descriptions
        result = session.run("""
            MATCH ()-[r]->()
            WHERE exists(r.description)
            RETURN count(r) as count
        """)
        assert result.single()["count"] > 0

@pytest.mark.asyncio
async def test_vector_search(knowledge_graph_system, neo4j_driver, embeddings):
    # Add test nodes with embeddings
    test_text = "Metformin is used to treat type 2 diabetes"
    test_embedding = embeddings.embed_query(test_text)
    
    node = KnowledgeNode(
        id="test_node",
        type="Drug",
        properties={"name": "Metformin", "description": test_text},
        confidence=0.9,
        sources=["test"],
        timestamp=datetime.now(),
        embedding=test_embedding
    )
    
    knowledge_graph_system.graph_manager.create_node(node)
    knowledge_graph_system.graph_manager.setup_vector_index()
    
    # Test vector similarity search
    query_text = "diabetes treatment medication"
    query_embedding = embeddings.embed_query(query_text)
    
    similar_nodes = knowledge_graph_system.graph_manager.find_similar_entities(
        embedding=query_embedding,
        k=1,
        similarity_cutoff=0.7
    )
    
    assert len(similar_nodes) > 0

@pytest.mark.asyncio
async def test_graph_rag_processing(knowledge_graph_system):
    # Test GraphRAG document processing
    test_text = """
    Metformin is a medication used to treat type 2 diabetes. It works by reducing glucose production
    in the liver and improving insulin sensitivity. Common side effects include nausea and diarrhea.
    """
    
    processor = knowledge_graph_system.graph_rag
    graph_docs = processor.process_text(test_text)
    
    # Verify document structure
    assert isinstance(graph_docs, list)
    assert len(graph_docs) > 0
    
    doc = graph_docs[0]
    assert "nodes" in doc
    assert "relationships" in doc
    
    # Verify node structure
    for node in doc["nodes"]:
        assert "id" in node
        assert "description" in node
        assert "embedding" in node
        assert isinstance(node["embedding"], list)
        assert len(node["embedding"]) > 0
        
    # Verify relationship structure
    for rel in doc["relationships"]:
        assert "source" in rel
        assert "target" in rel
        assert "type" in rel
        assert "description" in rel

@pytest.mark.asyncio
async def test_entity_merging(knowledge_graph_system, neo4j_driver):
    # Add similar entities
    text1 = "Metformin treats diabetes by reducing blood sugar"
    text2 = "Metformin is a medication that helps control blood glucose levels"
    
    processor = knowledge_graph_system.graph_rag
    docs1 = processor.process_text(text1)
    docs2 = processor.process_text(text2)
    
    # Populate graph
    knowledge_graph_system._populate_graph(docs1)
    knowledge_graph_system._populate_graph(docs2)
    knowledge_graph_system.graph_manager.setup_vector_index()
    
    # Run entity merging
    knowledge_graph_system._merge_similar_entities(similarity_cutoff=0.8)
    
    # Verify merged entities
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (n)
            WHERE n.properties.description CONTAINS 'Metformin'
            RETURN count(n) as count
        """)
        # Should have merged similar Metformin nodes
        assert result.single()["count"] == 1

@pytest.mark.asyncio
async def test_temporal_knowledge_tracking(knowledge_graph_system, neo4j_driver):
    # Add knowledge from different time periods
    old_text = "Old study shows metformin reduces blood sugar"
    new_text = "Recent study confirms metformin's effectiveness in diabetes treatment"
    
    processor = knowledge_graph_system.graph_rag
    old_docs = processor.process_text(old_text)
    new_docs = processor.process_text(new_text)
    
    # Modify timestamps
    for doc in old_docs:
        for node in doc["nodes"]:
            node["timestamp"] = (datetime.now() - timedelta(days=365*6)).isoformat()
            
    for doc in new_docs:
        for node in doc["nodes"]:
            node["timestamp"] = datetime.now().isoformat()
    
    # Populate graph
    knowledge_graph_system._populate_graph(old_docs)
    knowledge_graph_system._populate_graph(new_docs)
    
    # Verify temporal filtering
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (n)
            WHERE datetime(n.timestamp) > datetime() - duration('P5Y')
            RETURN count(n) as count
        """)
        recent_count = result.single()["count"]
        assert recent_count == len(new_docs[0]["nodes"])  # Only new nodes should be counted

@pytest.mark.asyncio
async def test_cross_domain_inference(knowledge_graph_system, neo4j_driver):
    # Add knowledge about diseases and drugs
    queries = [
        "What are the mechanisms of action for insulin in treating diabetes?",
        "How does metformin affect glucose metabolism?"
    ]
    
    await knowledge_graph_system.run(queries)
    
    # Verify cross-domain relationships
    with neo4j_driver.session() as session:
        result = session.run("""
            MATCH (d:Disease)-[r]-(m:Drug)
            WHERE d.properties.description CONTAINS 'diabetes'
            AND m.properties.description CONTAINS 'metformin'
            RETURN count(r) as count
        """)
        cross_domain_count = result.single()["count"]
        assert cross_domain_count > 0

@pytest.mark.asyncio
async def test_knowledge_validation(knowledge_graph_system, neo4j_driver):
    # Add potentially contradictory information
    queries = [
        "What is the recommended daily dose of metformin for type 2 diabetes?",
        "What are the contraindications for metformin use?"
    ]
    
    await knowledge_graph_system.run(queries)
    
    # Verify source attribution and confidence scores
    with neo4j_driver.session() as session:
        # Check source attribution
        result = session.run("""
            MATCH (n)
            WHERE size(n.sources) >= 1
            RETURN count(n) as count
        """)
        sourced_count = result.single()["count"]
        assert sourced_count > 0
        
        # Check confidence scores
        result = session.run("""
            MATCH (n)
            WHERE exists(n.confidence)
            RETURN count(n) as count
        """)
        scored_count = result.single()["count"]
        assert scored_count > 0
        
        # Verify high-confidence relationships
        result = session.run("""
            MATCH ()-[r]->()
            WHERE r.confidence >= 0.7
            RETURN count(r) as count
        """)
        high_confidence_count = result.single()["count"]
        assert high_confidence_count > 0 
```

